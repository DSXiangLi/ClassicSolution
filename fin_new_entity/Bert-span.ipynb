{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import numpy as np# data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T08:55:11.412321Z",
     "iopub.status.busy": "2022-09-27T08:55:11.411777Z",
     "iopub.status.idle": "2022-09-27T08:55:20.729212Z",
     "shell.execute_reply": "2022-09-27T08:55:20.727741Z",
     "shell.execute_reply.started": "2022-09-27T08:55:11.412268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import collections \n",
    "from itertools import chain\n",
    "import torch \n",
    "import time \n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "## 项目folder\n",
    "from evaluation import islegal, aggregate_f1\n",
    "from models import BertSpan\n",
    "from dataset import SpanDataset\n",
    "\n",
    "from src.dataset.converter import data_loader \n",
    "from src.train_utils import set_seed, ModelSave, get_torch_device, EarlyStop, TrainParams\n",
    "from src.evaluation import span_inference\n",
    "from src.metric import  seq_span_metrics, tag_cls_log\n",
    "from src.seqlabel_utils import extract_entity, get_entity_span\n",
    "\n",
    "\n",
    "import transformers \n",
    "transformers.logging.set_verbosity_error()\n",
    "from transformers import BertTokenizer,AdamW, get_linear_schedule_with_warmup\n",
    "device = get_torch_device()\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T08:55:21.528207Z",
     "iopub.status.busy": "2022-09-27T08:55:21.527031Z",
     "iopub.status.idle": "2022-09-27T08:55:21.537053Z",
     "shell.execute_reply": "2022-09-27T08:55:21.535611Z",
     "shell.execute_reply.started": "2022-09-27T08:55:21.528155Z"
    }
   },
   "outputs": [],
   "source": [
    "tp = TrainParams(\n",
    "    log_steps = 10,\n",
    "    save_steps = 1000,\n",
    "    epoch_size=20,\n",
    "    max_seq_len=512,\n",
    "    batch_size=20,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    lr=5e-5,\n",
    "    weight_decay=0.0,\n",
    "    epsilon=1e-6,\n",
    "    warmup_steps=100,\n",
    "    dropout_rate=0.2,\n",
    "    gradient_clip=5.0,\n",
    "    hidden_size=200,\n",
    "    early_stop_params = {\n",
    "        'monitor':'f1_micro',\n",
    "        'mode':'max',\n",
    "        'min_delta': 0,\n",
    "        'patience':3,\n",
    "        'verbose':False\n",
    "    },\n",
    "    pretrain_model = 'bert-base-chinese',\n",
    "    continue_train=False,\n",
    "    label2idx ={'FIN':1},\n",
    "    idx2label = {1:'FIN'},\n",
    "    label_size=2,\n",
    "    schema='span'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(tp.pretrain_model, do_lower_case=True)\n",
    "train_dataset = SpanDataset(data_loader('/kaggle/input/finent/train_span.txt'), tokenizer, tp.max_seq_len)\n",
    "valid_dataset = SpanDataset(data_loader('/kaggle/input/finent/valid_span.txt'), tokenizer, tp.max_seq_len)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=tp.batch_size)\n",
    "valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=tp.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.update({'num_train_steps': int(len(train_loader)*tp.epoch_size)})\n",
    "\n",
    "CKPT = './checkpoint/span_tag'\n",
    "saver = ModelSave(CKPT, continue_train=False)\n",
    "saver.init()\n",
    "es = EarlyStop(**tp.early_stop_params)\n",
    "global_step = 0\n",
    "tb = SummaryWriter(CKPT)\n",
    "\n",
    "model = BertSpan(tp)\n",
    "model.to(device)\n",
    "optimizer, scheduler = model.get_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T08:56:40.534523Z",
     "iopub.status.busy": "2022-09-27T08:56:40.533987Z",
     "iopub.status.idle": "2022-09-27T09:48:59.084500Z",
     "shell.execute_reply": "2022-09-27T09:48:59.083011Z",
     "shell.execute_reply.started": "2022-09-27T08:56:40.534483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   10    |   1.267005   |     -     |   12.54  \n",
      "   1    |   20    |   0.370860   |     -     |   10.56  \n",
      "   1    |   30    |   0.124825   |     -     |   10.52  \n",
      "   1    |   40    |   0.118028   |     -     |   10.44  \n",
      "   1    |   50    |   0.102047   |     -     |   10.36  \n",
      "   1    |   60    |   0.058101   |     -     |   10.36  \n",
      "   1    |   70    |   0.038279   |     -     |   10.37  \n",
      "   1    |   80    |   0.035708   |     -     |   10.30  \n",
      "   1    |   90    |   0.028355   |     -     |   10.38  \n",
      "   1    |   100   |   0.031106   |     -     |   10.31  \n",
      "   1    |   110   |   0.028496   |     -     |   10.47  \n",
      "   1    |   120   |   0.029313   |     -     |   10.38  \n",
      "   1    |   130   |   0.029044   |     -     |   10.39  \n",
      "   1    |   140   |   0.031391   |     -     |   10.36  \n",
      "   1    |   150   |   0.027777   |     -     |   10.40  \n",
      "   1    |   160   |   0.026703   |     -     |   10.34  \n",
      "   1    |   170   |   0.024650   |     -     |   10.38  \n",
      "   1    |   180   |   0.024665   |     -     |   10.50  \n",
      "   1    |   190   |   0.022990   |     -     |   10.34  \n",
      "   1    |   200   |   0.025234   |     -     |   10.34  \n",
      "   1    |   210   |   0.024816   |     -     |   10.40  \n",
      "   1    |   220   |   0.021516   |     -     |   10.34  \n",
      "   1    |   230   |   0.025890   |     -     |   10.40  \n",
      "   1    |   240   |   0.022281   |     -     |   10.45  \n",
      "   1    |   250   |   0.022267   |     -     |   10.31  \n",
      "   1    |   260   |   0.019422   |     -     |   10.31  \n",
      "   1    |   270   |   0.021000   |     -     |   10.35  \n",
      "   1    |   280   |   0.020132   |     -     |   10.35  \n",
      "   1    |   290   |   0.029799   |     -     |   10.38  \n",
      "   1    |   300   |   0.022085   |     -     |   10.31  \n",
      "   1    |   310   |   0.025592   |     -     |   10.34  \n",
      "   1    |   320   |   0.019722   |     -     |   10.41  \n",
      "   1    |   330   |   0.015901   |     -     |   10.32  \n",
      "   1    |   340   |   0.023457   |     -     |   10.39  \n",
      "   1    |   350   |   0.019422   |     -     |   10.33  \n",
      "   1    |   360   |   0.018512   |     -     |   10.32  \n",
      "   1    |   362   |   0.027903   |     -     |   1.48   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.080903   |  0.029440  |  419.59  \n",
      "\n",
      "\n",
      " Epoch  | Macro Precision | Macro Recall | Macro F1 \n",
      "----------------------------------------------------------------------\n",
      "   1     |     69.095%     |   74.245%    |  71.577%  \n",
      " Epoch   | Micro Precision | Micro Recall | Micro F1 \n",
      "----------------------------------------------------------------------\n",
      "   1    |     69.095%     |   74.245%    |  71.577%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   2    |   10    |   0.017214   |     -     |   11.41  \n",
      "   2    |   20    |   0.015071   |     -     |   10.36  \n",
      "   2    |   30    |   0.012124   |     -     |   10.31  \n",
      "   2    |   40    |   0.013020   |     -     |   10.34  \n",
      "   2    |   50    |   0.013923   |     -     |   10.44  \n",
      "   2    |   60    |   0.011099   |     -     |   10.39  \n",
      "   2    |   70    |   0.016362   |     -     |   10.34  \n",
      "   2    |   80    |   0.012356   |     -     |   10.41  \n",
      "   2    |   90    |   0.011299   |     -     |   10.36  \n",
      "   2    |   100   |   0.014980   |     -     |   10.32  \n",
      "   2    |   110   |   0.014092   |     -     |   10.41  \n",
      "   2    |   120   |   0.013971   |     -     |   10.41  \n",
      "   2    |   130   |   0.013137   |     -     |   10.35  \n",
      "   2    |   140   |   0.013258   |     -     |   10.37  \n",
      "   2    |   150   |   0.015458   |     -     |   10.36  \n",
      "   2    |   160   |   0.013072   |     -     |   10.38  \n",
      "   2    |   170   |   0.014513   |     -     |   10.41  \n",
      "   2    |   180   |   0.014309   |     -     |   10.41  \n",
      "   2    |   190   |   0.013167   |     -     |   10.30  \n",
      "   2    |   200   |   0.016440   |     -     |   10.34  \n",
      "   2    |   210   |   0.015114   |     -     |   10.40  \n",
      "   2    |   220   |   0.015107   |     -     |   10.36  \n",
      "   2    |   230   |   0.015831   |     -     |   10.37  \n",
      "   2    |   240   |   0.013565   |     -     |   10.38  \n",
      "   2    |   250   |   0.013046   |     -     |   10.35  \n",
      "   2    |   260   |   0.011940   |     -     |   10.33  \n",
      "   2    |   270   |   0.014444   |     -     |   10.35  \n",
      "   2    |   280   |   0.016010   |     -     |   10.28  \n",
      "   2    |   290   |   0.015804   |     -     |   10.34  \n",
      "   2    |   300   |   0.015125   |     -     |   10.36  \n",
      "   2    |   310   |   0.019141   |     -     |   10.32  \n",
      "   2    |   320   |   0.013517   |     -     |   10.35  \n",
      "   2    |   330   |   0.019873   |     -     |   10.36  \n",
      "   2    |   340   |   0.016770   |     -     |   10.40  \n",
      "   2    |   350   |   0.013085   |     -     |   10.34  \n",
      "   2    |   360   |   0.013255   |     -     |   10.38  \n",
      "   2    |   362   |   0.013295   |     -     |   1.49   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.014499   |  0.030017  |  412.79  \n",
      "\n",
      "\n",
      " Epoch  | Macro Precision | Macro Recall | Macro F1 \n",
      "----------------------------------------------------------------------\n",
      "   2     |     73.344%     |   73.281%    |  73.312%  \n",
      " Epoch   | Micro Precision | Micro Recall | Micro F1 \n",
      "----------------------------------------------------------------------\n",
      "   2    |     73.344%     |   73.281%    |  73.312%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   3    |   10    |   0.010541   |     -     |   11.37  \n",
      "   3    |   20    |   0.011408   |     -     |   10.36  \n",
      "   3    |   30    |   0.009897   |     -     |   10.34  \n",
      "   3    |   40    |   0.009456   |     -     |   10.30  \n",
      "   3    |   50    |   0.010215   |     -     |   10.40  \n",
      "   3    |   60    |   0.007391   |     -     |   10.41  \n",
      "   3    |   70    |   0.007996   |     -     |   10.35  \n",
      "   3    |   80    |   0.008765   |     -     |   10.32  \n",
      "   3    |   90    |   0.010646   |     -     |   10.36  \n",
      "   3    |   100   |   0.008950   |     -     |   10.34  \n",
      "   3    |   110   |   0.010604   |     -     |   10.35  \n",
      "   3    |   120   |   0.011471   |     -     |   10.36  \n",
      "   3    |   130   |   0.007936   |     -     |   10.35  \n",
      "   3    |   140   |   0.010240   |     -     |   10.41  \n",
      "   3    |   150   |   0.006653   |     -     |   10.36  \n",
      "   3    |   160   |   0.009180   |     -     |   10.32  \n",
      "   3    |   170   |   0.011051   |     -     |   10.36  \n",
      "   3    |   180   |   0.008473   |     -     |   10.40  \n",
      "   3    |   190   |   0.010516   |     -     |   10.31  \n",
      "   3    |   200   |   0.010129   |     -     |   10.34  \n",
      "   3    |   210   |   0.009989   |     -     |   10.37  \n",
      "   3    |   220   |   0.007090   |     -     |   10.31  \n",
      "   3    |   230   |   0.012353   |     -     |   10.39  \n",
      "   3    |   240   |   0.010584   |     -     |   10.46  \n",
      "   3    |   250   |   0.008029   |     -     |   10.31  \n",
      "   3    |   260   |   0.007518   |     -     |   10.32  \n",
      "   3    |   270   |   0.011536   |     -     |   10.39  \n",
      "   3    |   280   |   0.009125   |     -     |   10.42  \n",
      "   3    |   290   |   0.011183   |     -     |   10.33  \n",
      "   3    |   300   |   0.007642   |     -     |   10.33  \n",
      "   3    |   310   |   0.008267   |     -     |   10.33  \n",
      "   3    |   320   |   0.010647   |     -     |   10.33  \n",
      "   3    |   330   |   0.008259   |     -     |   10.33  \n",
      "   3    |   340   |   0.008030   |     -     |   10.34  \n",
      "   3    |   350   |   0.011625   |     -     |   10.38  \n",
      "   3    |   360   |   0.012116   |     -     |   10.45  \n",
      "   3    |   362   |   0.009167   |     -     |   1.48   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.009624   |  0.034387  |  412.85  \n",
      "\n",
      "\n",
      " Epoch  | Macro Precision | Macro Recall | Macro F1 \n",
      "----------------------------------------------------------------------\n",
      "   3     |     68.426%     |   77.424%    |  72.647%  \n",
      " Epoch   | Micro Precision | Micro Recall | Micro F1 \n",
      "----------------------------------------------------------------------\n",
      "   3    |     68.426%     |   77.424%    |  72.647%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   4    |   10    |   0.006779   |     -     |   11.43  \n",
      "   4    |   20    |   0.007162   |     -     |   10.38  \n",
      "   4    |   30    |   0.006296   |     -     |   10.29  \n",
      "   4    |   40    |   0.005514   |     -     |   10.32  \n",
      "   4    |   50    |   0.005704   |     -     |   10.38  \n",
      "   4    |   60    |   0.006206   |     -     |   10.37  \n",
      "   4    |   70    |   0.008886   |     -     |   10.35  \n",
      "   4    |   80    |   0.006659   |     -     |   10.37  \n",
      "   4    |   90    |   0.005800   |     -     |   10.37  \n",
      "   4    |   100   |   0.007405   |     -     |   10.34  \n",
      "   4    |   110   |   0.006852   |     -     |   10.42  \n",
      "   4    |   120   |   0.006741   |     -     |   10.35  \n",
      "   4    |   130   |   0.006643   |     -     |   10.37  \n",
      "   4    |   140   |   0.010494   |     -     |   10.43  \n",
      "   4    |   150   |   0.008081   |     -     |   10.32  \n",
      "   4    |   160   |   0.006943   |     -     |   10.32  \n",
      "   4    |   170   |   0.006223   |     -     |   10.40  \n",
      "   4    |   180   |   0.005613   |     -     |   10.34  \n",
      "   4    |   190   |   0.007180   |     -     |   10.38  \n",
      "   4    |   200   |   0.007531   |     -     |   10.42  \n",
      "   4    |   210   |   0.005255   |     -     |   10.35  \n",
      "   4    |   220   |   0.006840   |     -     |   10.32  \n",
      "   4    |   230   |   0.010578   |     -     |   10.39  \n",
      "   4    |   240   |   0.005875   |     -     |   10.37  \n",
      "   4    |   250   |   0.005075   |     -     |   10.34  \n",
      "   4    |   260   |   0.006565   |     -     |   10.34  \n",
      "   4    |   270   |   0.007196   |     -     |   10.35  \n",
      "   4    |   280   |   0.005834   |     -     |   10.42  \n",
      "   4    |   290   |   0.006532   |     -     |   10.32  \n",
      "   4    |   300   |   0.005344   |     -     |   10.37  \n",
      "   4    |   310   |   0.006132   |     -     |   10.35  \n",
      "   4    |   320   |   0.005968   |     -     |   10.31  \n",
      "   4    |   330   |   0.004027   |     -     |   10.36  \n",
      "   4    |   340   |   0.007250   |     -     |   10.31  \n",
      "   4    |   350   |   0.005204   |     -     |   10.35  \n",
      "   4    |   360   |   0.005076   |     -     |   10.41  \n",
      "   4    |   362   |   0.005145   |     -     |   1.49   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.006607   |  0.039043  |  412.78  \n",
      "\n",
      "\n",
      " Epoch  | Macro Precision | Macro Recall | Macro F1 \n",
      "----------------------------------------------------------------------\n",
      "   4     |     72.170%     |   75.784%    |  73.933%  \n",
      " Epoch   | Micro Precision | Micro Recall | Micro F1 \n",
      "----------------------------------------------------------------------\n",
      "   4    |     72.170%     |   75.784%    |  73.933%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   5    |   10    |   0.003982   |     -     |   11.35  \n",
      "   5    |   20    |   0.003955   |     -     |   10.36  \n",
      "   5    |   30    |   0.004609   |     -     |   10.31  \n",
      "   5    |   40    |   0.004529   |     -     |   10.45  \n",
      "   5    |   50    |   0.003653   |     -     |   10.39  \n",
      "   5    |   60    |   0.005590   |     -     |   10.30  \n",
      "   5    |   70    |   0.003739   |     -     |   10.34  \n",
      "   5    |   80    |   0.004666   |     -     |   10.39  \n",
      "   5    |   90    |   0.003422   |     -     |   10.41  \n",
      "   5    |   100   |   0.004621   |     -     |   10.33  \n",
      "   5    |   110   |   0.003928   |     -     |   10.34  \n",
      "   5    |   120   |   0.004431   |     -     |   10.32  \n",
      "   5    |   130   |   0.003180   |     -     |   10.37  \n",
      "   5    |   140   |   0.003627   |     -     |   10.39  \n",
      "   5    |   150   |   0.003609   |     -     |   10.32  \n",
      "   5    |   160   |   0.003721   |     -     |   10.32  \n",
      "   5    |   170   |   0.006884   |     -     |   10.38  \n",
      "   5    |   180   |   0.005346   |     -     |   10.38  \n",
      "   5    |   190   |   0.002869   |     -     |   10.30  \n",
      "   5    |   200   |   0.004328   |     -     |   10.37  \n",
      "   5    |   210   |   0.003108   |     -     |   10.39  \n",
      "   5    |   220   |   0.002995   |     -     |   10.32  \n",
      "   5    |   230   |   0.003988   |     -     |   10.45  \n",
      "   5    |   240   |   0.004551   |     -     |   10.33  \n",
      "   5    |   250   |   0.005251   |     -     |   10.33  \n",
      "   5    |   260   |   0.003987   |     -     |   10.44  \n",
      "   5    |   270   |   0.004959   |     -     |   10.34  \n",
      "   5    |   280   |   0.004216   |     -     |   10.32  \n",
      "   5    |   290   |   0.005409   |     -     |   10.39  \n",
      "   5    |   300   |   0.004270   |     -     |   10.32  \n",
      "   5    |   310   |   0.003268   |     -     |   10.37  \n",
      "   5    |   320   |   0.004238   |     -     |   10.34  \n",
      "   5    |   330   |   0.004291   |     -     |   10.33  \n",
      "   5    |   340   |   0.005230   |     -     |   10.30  \n",
      "   5    |   350   |   0.005790   |     -     |   10.43  \n",
      "   5    |   360   |   0.004007   |     -     |   10.35  \n",
      "   5    |   362   |   0.006250   |     -     |   1.48   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.004307   |  0.041469  |  412.72  \n",
      "\n",
      "\n",
      " Epoch  | Macro Precision | Macro Recall | Macro F1 \n",
      "----------------------------------------------------------------------\n",
      "   5     |     72.604%     |   74.245%    |  73.415%  \n",
      " Epoch   | Micro Precision | Micro Recall | Micro F1 \n",
      "----------------------------------------------------------------------\n",
      "   5    |     72.604%     |   74.245%    |  73.415%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   6    |   10    |   0.003992   |     -     |   11.42  \n",
      "   6    |   20    |   0.003515   |     -     |   10.40  \n",
      "   6    |   30    |   0.003347   |     -     |   10.41  \n",
      "   6    |   40    |   0.004167   |     -     |   10.34  \n",
      "   6    |   50    |   0.004209   |     -     |   10.36  \n",
      "   6    |   60    |   0.004086   |     -     |   10.33  \n",
      "   6    |   70    |   0.003946   |     -     |   10.34  \n",
      "   6    |   80    |   0.004504   |     -     |   10.42  \n",
      "   6    |   90    |   0.002428   |     -     |   10.32  \n",
      "   6    |   100   |   0.002356   |     -     |   10.36  \n",
      "   6    |   110   |   0.002382   |     -     |   10.37  \n",
      "   6    |   120   |   0.004849   |     -     |   10.37  \n",
      "   6    |   130   |   0.003678   |     -     |   10.33  \n",
      "   6    |   140   |   0.003087   |     -     |   10.33  \n",
      "   6    |   150   |   0.002922   |     -     |   10.38  \n",
      "   6    |   160   |   0.002321   |     -     |   10.34  \n",
      "   6    |   170   |   0.003007   |     -     |   10.34  \n",
      "   6    |   180   |   0.003975   |     -     |   10.35  \n",
      "   6    |   190   |   0.005436   |     -     |   10.37  \n",
      "   6    |   200   |   0.006795   |     -     |   10.43  \n",
      "   6    |   210   |   0.004211   |     -     |   10.33  \n",
      "   6    |   220   |   0.003442   |     -     |   10.35  \n",
      "   6    |   230   |   0.003246   |     -     |   10.42  \n",
      "   6    |   240   |   0.002319   |     -     |   10.35  \n",
      "   6    |   250   |   0.003969   |     -     |   10.32  \n",
      "   6    |   260   |   0.003578   |     -     |   10.43  \n",
      "   6    |   270   |   0.003202   |     -     |   10.32  \n",
      "   6    |   280   |   0.003641   |     -     |   10.31  \n",
      "   6    |   290   |   0.002483   |     -     |   10.42  \n",
      "   6    |   300   |   0.003054   |     -     |   10.36  \n",
      "   6    |   310   |   0.004245   |     -     |   10.35  \n",
      "   6    |   320   |   0.003294   |     -     |   10.40  \n",
      "   6    |   330   |   0.003295   |     -     |   10.33  \n",
      "   6    |   340   |   0.002771   |     -     |   10.37  \n",
      "   6    |   350   |   0.003182   |     -     |   10.48  \n",
      "   6    |   360   |   0.002430   |     -     |   10.42  \n",
      "   6    |   362   |   0.001616   |     -     |   1.49   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   0.003538   |  0.050092  |  412.95  \n",
      "\n",
      "\n",
      " Epoch  | Macro Precision | Macro Recall | Macro F1 \n",
      "----------------------------------------------------------------------\n",
      "   6     |     71.896%     |   74.317%    |  73.086%  \n",
      " Epoch   | Micro Precision | Micro Recall | Micro F1 \n",
      "----------------------------------------------------------------------\n",
      "   6    |     71.896%     |   74.317%    |  73.086%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   7    |   10    |   0.002699   |     -     |   11.40  \n",
      "   7    |   20    |   0.002252   |     -     |   10.40  \n",
      "   7    |   30    |   0.003862   |     -     |   10.36  \n",
      "   7    |   40    |   0.003582   |     -     |   10.36  \n",
      "   7    |   50    |   0.002047   |     -     |   10.34  \n",
      "   7    |   60    |   0.001997   |     -     |   10.34  \n",
      "   7    |   70    |   0.003065   |     -     |   10.41  \n",
      "   7    |   80    |   0.002259   |     -     |   10.38  \n",
      "   7    |   90    |   0.003524   |     -     |   10.31  \n",
      "   7    |   100   |   0.002330   |     -     |   10.38  \n",
      "   7    |   110   |   0.002512   |     -     |   10.42  \n",
      "   7    |   120   |   0.002013   |     -     |   10.41  \n",
      "   7    |   130   |   0.002961   |     -     |   10.34  \n",
      "   7    |   140   |   0.002897   |     -     |   10.34  \n",
      "   7    |   150   |   0.002020   |     -     |   10.37  \n",
      "   7    |   160   |   0.002079   |     -     |   10.33  \n",
      "   7    |   170   |   0.002420   |     -     |   10.40  \n",
      "   7    |   180   |   0.002378   |     -     |   10.36  \n",
      "   7    |   190   |   0.002103   |     -     |   10.36  \n",
      "   7    |   200   |   0.002553   |     -     |   10.44  \n",
      "   7    |   210   |   0.002853   |     -     |   10.34  \n",
      "   7    |   220   |   0.001610   |     -     |   10.34  \n",
      "   7    |   230   |   0.003080   |     -     |   10.40  \n",
      "   7    |   240   |   0.002582   |     -     |   10.36  \n",
      "   7    |   250   |   0.003190   |     -     |   10.32  \n",
      "   7    |   260   |   0.004921   |     -     |   10.42  \n",
      "   7    |   270   |   0.003223   |     -     |   10.34  \n",
      "   7    |   280   |   0.002163   |     -     |   10.33  \n",
      "   7    |   290   |   0.003731   |     -     |   10.43  \n",
      "   7    |   300   |   0.002722   |     -     |   10.34  \n",
      "   7    |   310   |   0.002149   |     -     |   10.43  \n",
      "   7    |   320   |   0.002600   |     -     |   10.34  \n",
      "   7    |   330   |   0.002925   |     -     |   10.35  \n",
      "   7    |   340   |   0.001881   |     -     |   10.34  \n",
      "   7    |   350   |   0.003770   |     -     |   10.37  \n",
      "   7    |   360   |   0.003619   |     -     |   10.32  \n",
      "   7    |   362   |   0.002134   |     -     |   1.49   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   0.002742   |  0.046192  |  413.72  \n",
      "\n",
      "\n",
      " Epoch  | Macro Precision | Macro Recall | Macro F1 \n",
      "----------------------------------------------------------------------\n",
      "   7     |     73.560%     |   73.698%    |  73.629%  \n",
      " Epoch   | Micro Precision | Micro Recall | Micro F1 \n",
      "----------------------------------------------------------------------\n",
      "   7    |     73.560%     |   73.698%    |  73.629%  \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(tp['epoch_size']):\n",
    "    print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10}  | {'Elapsed':^9}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    # Measure the elapsed time of each epoch\n",
    "    t0_epoch, t0_batch = time.time(), time.time()\n",
    "    total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        global_step +=1\n",
    "        batch_counts +=1\n",
    "\n",
    "        #Forward propogate\n",
    "        model.zero_grad()\n",
    "        feature = {k:v.to(device) for k, v in batch.items()}\n",
    "        logits = model(feature)\n",
    "        loss = model.compute_loss(feature, logits)\n",
    "        batch_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), tp.gradient_clip)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # Log steps for train loss logging\n",
    "        if (step % tp.log_steps == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "            time_elapsed = time.time() - t0_batch\n",
    "            print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "            tb.add_scalar('loss/batch_train', batch_loss / batch_counts, global_step=global_step)\n",
    "            batch_loss, batch_counts = 0, 0\n",
    "            t0_batch = time.time()\n",
    "\n",
    "        # Save steps for ckpt saving and dev evaluation\n",
    "        if (step % tp.save_steps == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "            val_metrics = seq_span_metrics(model, valid_loader, tp.idx2label, device)\n",
    "            for key, val in val_metrics.items():\n",
    "                tb.add_scalar(f'metric/{key}', val, global_step=global_step)\n",
    "            avg_train_loss = total_loss / step\n",
    "            tb.add_scalars('loss/train_valid',{'train': avg_train_loss,\n",
    "                                                'valid': val_metrics['val_loss']}, global_step=global_step)\n",
    "            saver(total_loss / step, val_metrics['val_loss'], epoch_i, global_step, model, optimizer, scheduler)\n",
    "\n",
    "    # On Epoch End: calcualte train & valid loss and log overall metrics\n",
    "    time_elapsed = time.time() - t0_epoch\n",
    "    val_metrics = seq_span_metrics(model, valid_loader, tp.idx2label, device)\n",
    "    avg_train_loss = total_loss / step\n",
    "\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_metrics['val_loss']:^10.6f} | {time_elapsed:^9.2f}\")\n",
    "    tag_cls_log(epoch_i, val_metrics)\n",
    "    print(\"\\n\")\n",
    "    if es.check(val_metrics):\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T09:59:29.054539Z",
     "iopub.status.busy": "2022-09-27T09:59:29.053861Z",
     "iopub.status.idle": "2022-09-27T09:59:29.737060Z",
     "shell.execute_reply": "2022-09-27T09:59:29.735724Z",
     "shell.execute_reply.started": "2022-09-27T09:59:29.054483Z"
    }
   },
   "outputs": [],
   "source": [
    "valid = pd.read_csv('./trainsample/valid.csv')\n",
    "train = pd.read_csv('./trainsample/train.csv')\n",
    "train.fillna({'entities':''},inplace=True)\n",
    "valid.fillna({'entities':''},inplace=True)\n",
    "known_entity = set(chain(*train['entities'].map(lambda x: x.split(';')).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T10:32:03.054016Z",
     "iopub.status.busy": "2022-09-27T10:32:03.053475Z",
     "iopub.status.idle": "2022-09-27T10:32:03.364960Z",
     "shell.execute_reply": "2022-09-27T10:32:03.363688Z",
     "shell.execute_reply.started": "2022-09-27T10:32:03.053972Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = span_inference(model, valid_loader, device)\n",
    "valid['pred_pos'] = [get_entity_span(i, tp.idx2label) for i in pred]\n",
    "valid['pred_entity'] = valid.apply(lambda x: extract_entity(x.corpus, x.pred_pos)['FIN'], axis=1)\n",
    "valid['pred_entity'] = valid['pred_entity'].map(lambda x: [i for i in x if islegal(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T10:32:06.368319Z",
     "iopub.status.busy": "2022-09-27T10:32:06.367157Z",
     "iopub.status.idle": "2022-09-27T10:32:06.415925Z",
     "shell.execute_reply": "2022-09-27T10:32:06.414590Z",
     "shell.execute_reply.started": "2022-09-27T10:32:06.368259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Entity Evaluation\n",
      "   precision    recall        f1\n",
      "0   0.628162  0.642241  0.635124\n",
      "Unknown Entity Evalutation\n",
      "   precision    recall        f1\n",
      "0   0.440208  0.488462  0.463081\n"
     ]
    }
   ],
   "source": [
    "stat = aggregate_f1(valid['id'].values, \n",
    "                  valid['entities'].values,\n",
    "                  valid['pred_entity'].values, \n",
    "                  known_entity)\n",
    "stat.to_csv('bert_span.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
