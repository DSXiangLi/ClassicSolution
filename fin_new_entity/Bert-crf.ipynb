{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T11:43:00.853204Z",
     "iopub.status.busy": "2022-09-26T11:43:00.851803Z",
     "iopub.status.idle": "2022-09-26T11:43:00.866678Z",
     "shell.execute_reply": "2022-09-26T11:43:00.865382Z",
     "shell.execute_reply.started": "2022-09-26T11:43:00.853160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import collections \n",
    "from itertools import chain\n",
    "import torch \n",
    "import time \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "## 项目folder\n",
    "from dataset import SeqLabelDataset\n",
    "from models import BertCrf\n",
    "from evaluation import islegal, aggregate_f1\n",
    "\n",
    "from src.dataset.converter import data_loader \n",
    "from src.train_utils import set_seed, ModelSave, get_torch_device, EarlyStop, TrainParams\n",
    "from src.evaluation import seqlabel_inference, seqlabel_report\n",
    "from src.metric import  seq_tag_metrics, tag_cls_log\n",
    "from src.seqlabel_utils import extract_entity, get_entity_bio\n",
    "\n",
    "\n",
    "import transformers \n",
    "from transformers import BertTokenizer\n",
    "transformers.logging.set_verbosity_error()\n",
    "device = get_torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T11:43:08.196150Z",
     "iopub.status.busy": "2022-09-26T11:43:08.195373Z",
     "iopub.status.idle": "2022-09-26T11:43:08.203199Z",
     "shell.execute_reply": "2022-09-26T11:43:08.202050Z",
     "shell.execute_reply.started": "2022-09-26T11:43:08.196115Z"
    }
   },
   "outputs": [],
   "source": [
    "tp = TrainParams(\n",
    "    log_steps = 10,\n",
    "    save_steps = 10000,\n",
    "    epoch_size=20,\n",
    "    max_seq_len=512,\n",
    "    batch_size=20,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    lr=5e-5,\n",
    "    crf_lr = 5e-5 * 10,\n",
    "    weight_decay=0.0,\n",
    "    epsilon=1e-6,\n",
    "    warmup_steps=100,\n",
    "    dropout_rate=0.2,\n",
    "    gradient_clip=5.0,\n",
    "    early_stop_params = {\n",
    "        'monitor':'f1_micro',\n",
    "        'mode':'max',\n",
    "        'min_delta': 0,\n",
    "        'patience':3,\n",
    "        'verbose':False\n",
    "    },\n",
    "    pretrain_model = 'bert-base-chinese',\n",
    "    continue_train=False,\n",
    "    label2idx ={\n",
    "        'O':0, 'B-FIN':1,'I-FIN':2\n",
    "    },\n",
    "    idx2label = {0:'O',1:'B-FIN',2:'I-FIN'},\n",
    "    label_size=3,\n",
    "    schema='BIO'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T11:56:37.363316Z",
     "iopub.status.busy": "2022-09-26T11:56:37.362942Z",
     "iopub.status.idle": "2022-09-26T11:57:19.221210Z",
     "shell.execute_reply": "2022-09-26T11:57:19.220259Z",
     "shell.execute_reply.started": "2022-09-26T11:56:37.363286Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(tp.pretrain_model, do_lower_case=True)\n",
    "train_dataset = SeqLabelDataset(data_loader('/kaggle/input/finent/train_bio.txt'), tokenizer, tp.max_seq_len, tp.label2idx)\n",
    "valid_dataset = SeqLabelDataset(data_loader('/kaggle/input/finent/valid_bio.txt'), tokenizer, tp.max_seq_len,  tp.label2idx)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=tp.batch_size)\n",
    "valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=tp.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:01:22.719245Z",
     "iopub.status.busy": "2022-09-26T12:01:22.718567Z",
     "iopub.status.idle": "2022-09-26T12:01:38.716653Z",
     "shell.execute_reply": "2022-09-26T12:01:38.715655Z",
     "shell.execute_reply.started": "2022-09-26T12:01:22.719210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: [Errno 2] No such file or directory: './checkpoint/bio_tag' not exists\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd38d06922b4cf5b9c77255a0f8bcfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/393M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tp.update({'num_train_steps': int(len(train_loader)*tp.epoch_size)})\n",
    "\n",
    "CKPT = './checkpoint/bio_tag'\n",
    "saver = ModelSave(CKPT, continue_train=False)\n",
    "saver.init()\n",
    "es = EarlyStop(**tp.early_stop_params)\n",
    "global_step = 0\n",
    "tb = SummaryWriter(CKPT)\n",
    "\n",
    "model = BertCrf(tp)\n",
    "model.to(device)\n",
    "optimizer, scheduler = model.get_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:04:05.869980Z",
     "iopub.status.busy": "2022-09-26T12:04:05.869394Z",
     "iopub.status.idle": "2022-09-26T13:37:43.037081Z",
     "shell.execute_reply": "2022-09-26T13:37:43.036103Z",
     "shell.execute_reply.started": "2022-09-26T12:04:05.869943Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   10    |  509.616402  |     -     |   16.03  \n",
      "   1    |   20    |  169.999390  |     -     |   14.22  \n",
      "   1    |   30    |  56.893949   |     -     |   14.08  \n",
      "   1    |   40    |  32.519269   |     -     |   13.97  \n",
      "   1    |   50    |  24.801992   |     -     |   13.96  \n",
      "   1    |   60    |  23.853972   |     -     |   14.01  \n",
      "   1    |   70    |  17.577939   |     -     |   13.85  \n",
      "   1    |   80    |  17.861504   |     -     |   13.93  \n",
      "   1    |   90    |  20.950386   |     -     |   14.26  \n",
      "   1    |   100   |  15.709005   |     -     |   14.02  \n",
      "   1    |   110   |  16.964316   |     -     |   13.91  \n",
      "   1    |   120   |  17.720364   |     -     |   13.83  \n",
      "   1    |   130   |  17.901567   |     -     |   14.19  \n",
      "   1    |   140   |  20.489372   |     -     |   14.06  \n",
      "   1    |   150   |  18.284148   |     -     |   14.11  \n",
      "   1    |   160   |  16.176845   |     -     |   13.83  \n",
      "   1    |   170   |  16.193965   |     -     |   13.84  \n",
      "   1    |   180   |  16.208730   |     -     |   13.95  \n",
      "   1    |   190   |  14.390851   |     -     |   13.80  \n",
      "   1    |   200   |  14.634925   |     -     |   14.54  \n",
      "   1    |   210   |  14.476623   |     -     |   13.85  \n",
      "   1    |   220   |  12.949761   |     -     |   14.03  \n",
      "   1    |   230   |  13.209922   |     -     |   13.84  \n",
      "   1    |   240   |  14.325075   |     -     |   14.25  \n",
      "   1    |   250   |  12.811993   |     -     |   14.24  \n",
      "   1    |   260   |  16.852325   |     -     |   14.34  \n",
      "   1    |   270   |  12.142911   |     -     |   14.35  \n",
      "   1    |   280   |  12.483139   |     -     |   13.89  \n",
      "   1    |   290   |  15.252526   |     -     |   13.85  \n",
      "   1    |   300   |  12.218477   |     -     |   13.88  \n",
      "   1    |   310   |  16.717028   |     -     |   14.08  \n",
      "   1    |   320   |  10.896912   |     -     |   13.98  \n",
      "   1    |   330   |  12.357252   |     -     |   14.18  \n",
      "   1    |   340   |  12.433967   |     -     |   13.91  \n",
      "   1    |   350   |  13.002037   |     -     |   14.12  \n",
      "   1    |   360   |  12.761934   |     -     |   13.89  \n",
      "   1    |   362   |   4.652700   |     -     |   2.18   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |  36.616932   | 16.995047  |  570.05  \n",
      "\n",
      "\n",
      " Epoch  | Macro Precision | Macro Recall | Macro F1 \n",
      "----------------------------------------------------------------------\n",
      "   1     |     65.921%     |   68.710%    |  67.287%  \n",
      " Epoch   | Micro Precision | Micro Recall | Micro F1 \n",
      "----------------------------------------------------------------------\n",
      "   1    |     65.921%     |   68.710%    |  67.287%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   2    |   10    |   8.743946   |     -     |   15.35  \n",
      "   2    |   20    |   8.854388   |     -     |   14.02  \n",
      "   2    |   30    |  10.880619   |     -     |   13.86  \n",
      "   2    |   40    |   8.724994   |     -     |   14.06  \n",
      "   2    |   50    |   7.962450   |     -     |   13.86  \n",
      "   2    |   60    |   8.064563   |     -     |   14.26  \n",
      "   2    |   70    |   9.911838   |     -     |   13.90  \n",
      "   2    |   80    |   7.663830   |     -     |   13.89  \n",
      "   2    |   90    |  10.777995   |     -     |   13.90  \n",
      "   2    |   100   |   8.930676   |     -     |   13.88  \n",
      "   2    |   110   |   7.697482   |     -     |   14.07  \n",
      "   2    |   120   |   6.288021   |     -     |   13.95  \n",
      "   2    |   130   |   9.592163   |     -     |   14.29  \n",
      "   2    |   140   |   7.836994   |     -     |   13.84  \n",
      "   2    |   150   |   8.380561   |     -     |   13.98  \n",
      "   2    |   160   |   7.933576   |     -     |   13.90  \n",
      "   2    |   170   |   9.184548   |     -     |   13.87  \n",
      "   2    |   180   |  10.201957   |     -     |   14.27  \n",
      "   2    |   190   |   8.279864   |     -     |   13.90  \n",
      "   2    |   200   |   7.962093   |     -     |   14.32  \n",
      "   2    |   210   |   8.152497   |     -     |   13.78  \n",
      "   2    |   220   |   6.994177   |     -     |   14.05  \n",
      "   2    |   230   |   9.389303   |     -     |   13.96  \n",
      "   2    |   240   |   8.392354   |     -     |   14.07  \n",
      "   2    |   250   |   8.002245   |     -     |   14.04  \n",
      "   2    |   260   |   6.319871   |     -     |   14.27  \n",
      "   2    |   270   |   9.061962   |     -     |   14.14  \n",
      "   2    |   280   |   7.799867   |     -     |   14.04  \n",
      "   2    |   290   |   7.802326   |     -     |   14.17  \n",
      "   2    |   300   |   8.388107   |     -     |   14.14  \n",
      "   2    |   310   |   8.840959   |     -     |   14.46  \n",
      "   2    |   320   |   9.806351   |     -     |   13.88  \n",
      "   2    |   330   |   8.517994   |     -     |   14.12  \n",
      "   2    |   340   |   9.083472   |     -     |   14.06  \n",
      "   2    |   350   |   6.044302   |     -     |   14.18  \n",
      "   2    |   360   |   7.197798   |     -     |   13.88  \n",
      "   2    |   362   |   9.889643   |     -     |   2.18   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   8.467361   | 16.323996  |  570.19  \n",
      "\n",
      "\n",
      " Epoch  | Macro Precision | Macro Recall | Macro F1 \n",
      "----------------------------------------------------------------------\n",
      "   2     |     72.387%     |   73.273%    |  72.827%  \n",
      " Epoch   | Micro Precision | Micro Recall | Micro F1 \n",
      "----------------------------------------------------------------------\n",
      "   2    |     72.387%     |   73.273%    |  72.827%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   3    |   10    |   4.555400   |     -     |   15.29  \n",
      "   3    |   20    |   4.441066   |     -     |   14.21  \n",
      "   3    |   30    |   3.893456   |     -     |   13.85  \n",
      "   3    |   40    |   5.406731   |     -     |   14.01  \n",
      "   3    |   50    |   4.370967   |     -     |   13.97  \n",
      "   3    |   60    |   6.219940   |     -     |   14.06  \n",
      "   3    |   70    |   5.247826   |     -     |   13.89  \n",
      "   3    |   80    |   5.602215   |     -     |   13.78  \n",
      "   3    |   90    |   5.173552   |     -     |   14.01  \n",
      "   3    |   100   |   5.674298   |     -     |   13.80  \n",
      "   3    |   110   |   6.395268   |     -     |   13.90  \n",
      "   3    |   120   |   4.908706   |     -     |   13.93  \n",
      "   3    |   130   |   4.111925   |     -     |   14.11  \n",
      "   3    |   140   |   5.808216   |     -     |   14.12  \n",
      "   3    |   150   |   3.277747   |     -     |   14.02  \n",
      "   3    |   160   |   4.475776   |     -     |   13.85  \n",
      "   3    |   170   |   7.477228   |     -     |   13.79  \n",
      "   3    |   180   |   4.914450   |     -     |   14.12  \n",
      "   3    |   190   |   5.567737   |     -     |   14.21  \n",
      "   3    |   200   |   4.292241   |     -     |   13.90  \n",
      "   3    |   210   |   4.480974   |     -     |   13.80  \n",
      "   3    |   220   |   6.048019   |     -     |   13.98  \n",
      "   3    |   230   |   4.342403   |     -     |   13.82  \n",
      "   3    |   240   |   4.187057   |     -     |   13.89  \n",
      "   3    |   250   |   3.417283   |     -     |   14.10  \n",
      "   3    |   260   |   4.529630   |     -     |   14.21  \n",
      "   3    |   270   |   3.445569   |     -     |   13.79  \n",
      "   3    |   280   |   5.124631   |     -     |   13.79  \n",
      "   3    |   290   |   4.575480   |     -     |   14.07  \n",
      "   3    |   300   |   5.649148   |     -     |   13.78  \n",
      "   3    |   310   |   4.284461   |     -     |   14.03  \n",
      "   3    |   320   |   6.085351   |     -     |   13.97  \n",
      "   3    |   330   |   5.137777   |     -     |   14.28  \n",
      "   3    |   340   |   4.398776   |     -     |   13.96  \n",
      "   3    |   350   |   5.137732   |     -     |   14.13  \n",
      "   3    |   360   |   4.139001   |     -     |   14.33  \n",
      "   3    |   362   |   1.849262   |     -     |   2.54   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   4.906725   | 22.730801  |  565.32  \n",
      "\n",
      "\n",
      " Epoch  | Macro Precision | Macro Recall | Macro F1 \n",
      "----------------------------------------------------------------------\n",
      "   3     |     70.372%     |   75.993%    |  73.075%  \n",
      " Epoch   | Micro Precision | Micro Recall | Micro F1 \n",
      "----------------------------------------------------------------------\n",
      "   3    |     70.372%     |   75.993%    |  73.075%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   4    |   10    |   2.812576   |     -     |   15.40  \n",
      "   4    |   20    |   2.826815   |     -     |   14.39  \n",
      "   4    |   30    |   3.178622   |     -     |   14.15  \n",
      "   4    |   40    |   3.427240   |     -     |   14.14  \n",
      "   4    |   50    |   2.609157   |     -     |   14.02  \n",
      "   4    |   60    |   2.174105   |     -     |   14.22  \n",
      "   4    |   70    |   3.437267   |     -     |   13.88  \n",
      "   4    |   80    |   3.238810   |     -     |   14.01  \n",
      "   4    |   90    |   3.010758   |     -     |   14.14  \n",
      "   4    |   100   |   2.632610   |     -     |   13.99  \n",
      "   4    |   110   |   2.622877   |     -     |   14.28  \n",
      "   4    |   120   |   3.535350   |     -     |   13.79  \n",
      "   4    |   130   |   2.225964   |     -     |   14.18  \n",
      "   4    |   140   |   1.941048   |     -     |   13.98  \n",
      "   4    |   150   |   3.680741   |     -     |   14.37  \n",
      "   4    |   160   |   3.507750   |     -     |   13.88  \n",
      "   4    |   170   |   4.370815   |     -     |   13.77  \n",
      "   4    |   180   |   2.970418   |     -     |   14.41  \n",
      "   4    |   190   |   2.908243   |     -     |   14.10  \n",
      "   4    |   200   |   3.681978   |     -     |   14.00  \n",
      "   4    |   210   |   2.303654   |     -     |   13.75  \n",
      "   4    |   220   |   2.674987   |     -     |   14.00  \n",
      "   4    |   230   |   2.250892   |     -     |   13.72  \n",
      "   4    |   240   |   2.805439   |     -     |   13.92  \n",
      "   4    |   250   |   3.289044   |     -     |   14.08  \n",
      "   4    |   260   |   3.727935   |     -     |   14.07  \n",
      "   4    |   270   |   3.177250   |     -     |   13.86  \n",
      "   4    |   280   |   2.855657   |     -     |   13.78  \n",
      "   4    |   290   |   2.610544   |     -     |   13.99  \n",
      "   4    |   300   |   4.218061   |     -     |   13.88  \n",
      "   4    |   310   |   2.834132   |     -     |   13.85  \n",
      "   4    |   320   |   2.839805   |     -     |   13.72  \n",
      "   4    |   330   |   4.196710   |     -     |   14.10  \n",
      "   4    |   340   |   3.558603   |     -     |   14.14  \n",
      "   4    |   350   |   3.071431   |     -     |   14.12  \n",
      "   4    |   360   |   3.796111   |     -     |   14.15  \n",
      "   4    |   362   |   2.818036   |     -     |   2.21   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   3.089731   | 20.681530  |  565.38  \n",
      "\n",
      "\n",
      " Epoch  | Macro Precision | Macro Recall | Macro F1 \n",
      "----------------------------------------------------------------------\n",
      "   4     |     69.790%     |   75.576%    |  72.568%  \n",
      " Epoch   | Micro Precision | Micro Recall | Micro F1 \n",
      "----------------------------------------------------------------------\n",
      "   4    |     69.790%     |   75.576%    |  72.568%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   5    |   10    |   1.554120   |     -     |   15.13  \n",
      "   5    |   20    |   1.850798   |     -     |   13.88  \n",
      "   5    |   30    |   2.290390   |     -     |   13.95  \n",
      "   5    |   40    |   3.216360   |     -     |   13.93  \n",
      "   5    |   50    |   1.905656   |     -     |   14.00  \n",
      "   5    |   60    |   1.311815   |     -     |   13.85  \n",
      "   5    |   70    |   2.043705   |     -     |   14.00  \n",
      "   5    |   80    |   1.644349   |     -     |   13.83  \n",
      "   5    |   90    |   2.144679   |     -     |   14.16  \n",
      "   5    |   100   |   2.042088   |     -     |   13.92  \n",
      "   5    |   110   |   1.788445   |     -     |   14.09  \n",
      "   5    |   120   |   1.919717   |     -     |   14.06  \n",
      "   5    |   130   |   2.240235   |     -     |   13.90  \n",
      "   5    |   140   |   1.411931   |     -     |   14.13  \n",
      "   5    |   150   |   1.909237   |     -     |   13.93  \n",
      "   5    |   160   |   1.341593   |     -     |   14.32  \n",
      "   5    |   170   |   1.809714   |     -     |   13.82  \n",
      "   5    |   180   |   2.694701   |     -     |   14.13  \n",
      "   5    |   190   |   1.600239   |     -     |   13.87  \n",
      "   5    |   200   |   2.681178   |     -     |   14.08  \n",
      "   5    |   210   |   3.370151   |     -     |   13.85  \n",
      "   5    |   220   |   1.604862   |     -     |   14.04  \n",
      "   5    |   230   |   3.129936   |     -     |   13.84  \n",
      "   5    |   240   |   1.757161   |     -     |   13.84  \n",
      "   5    |   250   |   2.305540   |     -     |   14.31  \n",
      "   5    |   260   |   2.091711   |     -     |   13.86  \n",
      "   5    |   270   |   1.693655   |     -     |   14.03  \n",
      "   5    |   280   |   1.785572   |     -     |   13.80  \n",
      "   5    |   290   |   2.722850   |     -     |   14.06  \n",
      "   5    |   300   |   2.604065   |     -     |   13.80  \n",
      "   5    |   310   |   1.674016   |     -     |   13.90  \n",
      "   5    |   320   |   1.556002   |     -     |   13.89  \n",
      "   5    |   330   |   1.900881   |     -     |   14.13  \n",
      "   5    |   340   |   1.463123   |     -     |   14.26  \n",
      "   5    |   350   |   2.073014   |     -     |   13.77  \n",
      "   5    |   360   |   2.098376   |     -     |   14.10  \n",
      "   5    |   362   |   3.843169   |     -     |   2.17   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   2.048506   | 23.831244  |  563.49  \n",
      "\n",
      "\n",
      " Epoch  | Macro Precision | Macro Recall | Macro F1 \n",
      "----------------------------------------------------------------------\n",
      "   5     |     72.189%     |   75.504%    |  73.809%  \n",
      " Epoch   | Micro Precision | Micro Recall | Micro F1 \n",
      "----------------------------------------------------------------------\n",
      "   5    |     72.189%     |   75.504%    |  73.809%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   6    |   10    |   1.258309   |     -     |   15.39  \n",
      "   6    |   20    |   0.586455   |     -     |   14.10  \n",
      "   6    |   30    |   2.172567   |     -     |   14.09  \n",
      "   6    |   40    |   1.291252   |     -     |   13.80  \n",
      "   6    |   50    |   1.448260   |     -     |   13.99  \n",
      "   6    |   60    |   1.014580   |     -     |   14.23  \n",
      "   6    |   70    |   1.629704   |     -     |   14.16  \n",
      "   6    |   80    |   0.607739   |     -     |   14.29  \n",
      "   6    |   90    |   1.290495   |     -     |   14.49  \n",
      "   6    |   100   |   1.923295   |     -     |   15.57  \n",
      "   6    |   110   |   0.937456   |     -     |   14.10  \n",
      "   6    |   120   |   1.511205   |     -     |   14.19  \n",
      "   6    |   130   |   0.831242   |     -     |   13.99  \n",
      "   6    |   140   |   3.154243   |     -     |   14.01  \n",
      "   6    |   150   |   1.268995   |     -     |   14.21  \n",
      "   6    |   160   |   1.099021   |     -     |   14.23  \n",
      "   6    |   170   |   1.734446   |     -     |   13.86  \n",
      "   6    |   180   |   1.985484   |     -     |   14.08  \n",
      "   6    |   190   |   2.079725   |     -     |   14.07  \n",
      "   6    |   200   |   1.756875   |     -     |   14.53  \n",
      "   6    |   210   |   1.197398   |     -     |   13.85  \n",
      "   6    |   220   |   1.224859   |     -     |   15.67  \n",
      "   6    |   230   |   1.664685   |     -     |   15.06  \n",
      "   6    |   240   |   1.382464   |     -     |   14.07  \n",
      "   6    |   250   |   1.069194   |     -     |   14.09  \n",
      "   6    |   260   |   1.367253   |     -     |   14.09  \n",
      "   6    |   270   |   0.989333   |     -     |   14.23  \n",
      "   6    |   280   |   1.032478   |     -     |   13.98  \n",
      "   6    |   290   |   4.428357   |     -     |   14.19  \n",
      "   6    |   300   |   1.639022   |     -     |   13.83  \n",
      "   6    |   310   |   1.684895   |     -     |   14.07  \n",
      "   6    |   320   |   1.548559   |     -     |   14.24  \n",
      "   6    |   330   |   1.047262   |     -     |   14.18  \n",
      "   6    |   340   |   1.664443   |     -     |   13.88  \n",
      "   6    |   350   |   1.268760   |     -     |   13.87  \n",
      "   6    |   360   |   0.683706   |     -     |   14.67  \n",
      "   6    |   362   |   2.324688   |     -     |   2.18   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   1.493502   | 32.424755  |  574.02  \n",
      "\n",
      "\n",
      " Epoch  | Macro Precision | Macro Recall | Macro F1 \n",
      "----------------------------------------------------------------------\n",
      "   6     |     72.841%     |   76.123%    |  74.446%  \n",
      " Epoch   | Micro Precision | Micro Recall | Micro F1 \n",
      "----------------------------------------------------------------------\n",
      "   6    |     72.841%     |   76.123%    |  74.446%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   7    |   10    |   1.009030   |     -     |   15.61  \n",
      "   7    |   20    |   1.119155   |     -     |   14.12  \n",
      "   7    |   30    |   1.381807   |     -     |   13.88  \n",
      "   7    |   40    |   0.909938   |     -     |   14.18  \n",
      "   7    |   50    |   1.259892   |     -     |   13.98  \n",
      "   7    |   60    |   0.919957   |     -     |   13.88  \n",
      "   7    |   70    |   1.462331   |     -     |   14.18  \n",
      "   7    |   80    |   0.494534   |     -     |   13.88  \n",
      "   7    |   90    |   0.504244   |     -     |   14.40  \n",
      "   7    |   100   |   1.013336   |     -     |   14.21  \n",
      "   7    |   110   |   1.021524   |     -     |   14.51  \n",
      "   7    |   120   |   0.821318   |     -     |   14.33  \n",
      "   7    |   130   |   0.703095   |     -     |   14.03  \n",
      "   7    |   140   |   1.234662   |     -     |   13.94  \n",
      "   7    |   150   |   0.991856   |     -     |   13.97  \n",
      "   7    |   160   |   0.850215   |     -     |   14.06  \n",
      "   7    |   170   |   1.115301   |     -     |   14.00  \n",
      "   7    |   180   |   0.761395   |     -     |   14.18  \n",
      "   7    |   190   |   0.747411   |     -     |   13.98  \n",
      "   7    |   200   |   0.747918   |     -     |   14.35  \n",
      "   7    |   210   |   2.046237   |     -     |   13.86  \n",
      "   7    |   220   |   0.995034   |     -     |   13.96  \n",
      "   7    |   230   |   1.509523   |     -     |   13.92  \n",
      "   7    |   240   |   1.318014   |     -     |   14.19  \n",
      "   7    |   250   |   1.059427   |     -     |   14.09  \n",
      "   7    |   260   |   0.543590   |     -     |   13.93  \n",
      "   7    |   270   |   1.269228   |     -     |   14.21  \n",
      "   7    |   280   |   0.295864   |     -     |   13.99  \n",
      "   7    |   290   |   0.944199   |     -     |   14.22  \n",
      "   7    |   300   |   0.654564   |     -     |   14.25  \n",
      "   7    |   310   |   1.074464   |     -     |   14.14  \n",
      "   7    |   320   |   0.681471   |     -     |   14.20  \n",
      "   7    |   330   |   0.868207   |     -     |   14.05  \n",
      "   7    |   340   |   2.019739   |     -     |   13.85  \n",
      "   7    |   350   |   1.278327   |     -     |   13.90  \n",
      "   7    |   360   |   1.214126   |     -     |   14.08  \n",
      "   7    |   362   |   3.903966   |     -     |   2.22   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   1.042062   | 27.677876  |  567.99  \n",
      "\n",
      "\n",
      " Epoch  | Macro Precision | Macro Recall | Macro F1 \n",
      "----------------------------------------------------------------------\n",
      "   7     |     69.610%     |   79.548%    |  74.248%  \n",
      " Epoch   | Micro Precision | Micro Recall | Micro F1 \n",
      "----------------------------------------------------------------------\n",
      "   7    |     69.610%     |   79.548%    |  74.248%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   8    |   10    |   0.617521   |     -     |   15.52  \n",
      "   8    |   20    |   0.862992   |     -     |   14.04  \n",
      "   8    |   30    |   0.669620   |     -     |   13.90  \n",
      "   8    |   40    |   0.459760   |     -     |   13.91  \n",
      "   8    |   50    |   0.902152   |     -     |   14.27  \n",
      "   8    |   60    |   0.638157   |     -     |   14.05  \n",
      "   8    |   70    |   0.712844   |     -     |   14.04  \n",
      "   8    |   80    |   0.434731   |     -     |   13.96  \n",
      "   8    |   90    |   1.520440   |     -     |   14.39  \n",
      "   8    |   100   |   0.744971   |     -     |   14.00  \n",
      "   8    |   110   |   1.128713   |     -     |   14.05  \n",
      "   8    |   120   |   1.233708   |     -     |   13.92  \n",
      "   8    |   130   |   1.380222   |     -     |   14.06  \n",
      "   8    |   140   |   0.608524   |     -     |   13.96  \n",
      "   8    |   150   |   0.277001   |     -     |   13.93  \n",
      "   8    |   160   |   1.155508   |     -     |   14.27  \n",
      "   8    |   170   |   1.386671   |     -     |   13.83  \n",
      "   8    |   180   |   0.406214   |     -     |   14.05  \n",
      "   8    |   190   |   0.463592   |     -     |   14.22  \n",
      "   8    |   200   |   1.385945   |     -     |   14.10  \n",
      "   8    |   210   |   0.772349   |     -     |   13.86  \n",
      "   8    |   220   |   0.616168   |     -     |   14.08  \n",
      "   8    |   230   |   0.739960   |     -     |   14.00  \n",
      "   8    |   240   |   0.548522   |     -     |   13.99  \n",
      "   8    |   250   |   0.512925   |     -     |   14.10  \n",
      "   8    |   260   |   0.865401   |     -     |   13.90  \n",
      "   8    |   270   |   0.811274   |     -     |   14.36  \n",
      "   8    |   280   |   0.597219   |     -     |   13.95  \n",
      "   8    |   290   |   0.711827   |     -     |   14.10  \n",
      "   8    |   300   |   1.157748   |     -     |   13.93  \n",
      "   8    |   310   |   0.847910   |     -     |   14.15  \n",
      "   8    |   320   |   0.881017   |     -     |   13.93  \n",
      "   8    |   330   |   0.457774   |     -     |   14.13  \n",
      "   8    |   340   |   0.523616   |     -     |   14.07  \n",
      "   8    |   350   |   0.610611   |     -     |   13.87  \n",
      "   8    |   360   |   0.996280   |     -     |   13.91  \n",
      "   8    |   362   |   0.626593   |     -     |   2.18   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   0.796325   | 35.383473  |  566.85  \n",
      "\n",
      "\n",
      " Epoch  | Macro Precision | Macro Recall | Macro F1 \n",
      "----------------------------------------------------------------------\n",
      "   8     |     70.718%     |   75.950%    |  73.241%  \n",
      " Epoch   | Micro Precision | Micro Recall | Micro F1 \n",
      "----------------------------------------------------------------------\n",
      "   8    |     70.718%     |   75.950%    |  73.241%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   9    |   10    |   0.679411   |     -     |   15.32  \n",
      "   9    |   20    |   0.578349   |     -     |   13.95  \n",
      "   9    |   30    |   1.234860   |     -     |   14.18  \n",
      "   9    |   40    |   0.622227   |     -     |   13.92  \n",
      "   9    |   50    |   0.435626   |     -     |   14.15  \n",
      "   9    |   60    |   0.743174   |     -     |   14.18  \n",
      "   9    |   70    |   0.477184   |     -     |   14.09  \n",
      "   9    |   80    |   0.618634   |     -     |   14.19  \n",
      "   9    |   90    |   0.611888   |     -     |   14.06  \n",
      "   9    |   100   |   0.769244   |     -     |   14.11  \n",
      "   9    |   110   |   0.469901   |     -     |   14.18  \n",
      "   9    |   120   |   0.824824   |     -     |   13.86  \n",
      "   9    |   130   |   0.504978   |     -     |   14.05  \n",
      "   9    |   140   |   0.573615   |     -     |   14.06  \n",
      "   9    |   150   |   0.523684   |     -     |   13.94  \n",
      "   9    |   160   |   0.589725   |     -     |   14.28  \n",
      "   9    |   170   |   0.851489   |     -     |   13.85  \n",
      "   9    |   180   |   0.458259   |     -     |   14.17  \n",
      "   9    |   190   |   0.850350   |     -     |   13.96  \n",
      "   9    |   200   |   0.510824   |     -     |   14.33  \n",
      "   9    |   210   |   0.887527   |     -     |   13.92  \n",
      "   9    |   220   |   0.506506   |     -     |   14.09  \n",
      "   9    |   230   |   0.461983   |     -     |   13.99  \n",
      "   9    |   240   |   0.428750   |     -     |   14.08  \n",
      "   9    |   250   |   1.510989   |     -     |   13.99  \n",
      "   9    |   260   |   0.327236   |     -     |   14.18  \n",
      "   9    |   270   |   0.528682   |     -     |   14.03  \n",
      "   9    |   280   |   0.909515   |     -     |   14.13  \n",
      "   9    |   290   |   0.575609   |     -     |   14.30  \n",
      "   9    |   300   |   0.374645   |     -     |   14.06  \n",
      "   9    |   310   |   0.614640   |     -     |   14.28  \n",
      "   9    |   320   |   0.298416   |     -     |   13.93  \n",
      "   9    |   330   |   0.292299   |     -     |   13.91  \n",
      "   9    |   340   |   1.546720   |     -     |   14.05  \n",
      "   9    |   350   |   1.657209   |     -     |   13.97  \n",
      "   9    |   360   |   0.784046   |     -     |   14.15  \n",
      "   9    |   362   |   0.397319   |     -     |   2.18   \n",
      "----------------------------------------------------------------------\n",
      "   9    |    -    |   0.684542   | 35.375697  |  567.78  \n",
      "\n",
      "\n",
      " Epoch  | Macro Precision | Macro Recall | Macro F1 \n",
      "----------------------------------------------------------------------\n",
      "   9     |     70.496%     |   76.036%    |  73.162%  \n",
      " Epoch   | Micro Precision | Micro Recall | Micro F1 \n",
      "----------------------------------------------------------------------\n",
      "   9    |     70.496%     |   76.036%    |  73.162%  \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(tp['epoch_size']):\n",
    "    print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10}  | {'Elapsed':^9}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    # Measure the elapsed time of each epoch\n",
    "    t0_epoch, t0_batch = time.time(), time.time()\n",
    "    total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        global_step +=1\n",
    "        batch_counts +=1\n",
    "\n",
    "        #Forward propogate\n",
    "        model.zero_grad()\n",
    "        feature = {k:v.to(device) for k, v in batch.items()}\n",
    "        logits = model(feature)\n",
    "        loss = model.compute_loss(feature, logits)\n",
    "        batch_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), tp.gradient_clip)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # Log steps for train loss logging\n",
    "        if (step % tp.log_steps == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "            time_elapsed = time.time() - t0_batch\n",
    "            print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "            tb.add_scalar('loss/batch_train', batch_loss / batch_counts, global_step=global_step)\n",
    "            batch_loss, batch_counts = 0, 0\n",
    "            t0_batch = time.time()\n",
    "\n",
    "        # Save steps for ckpt saving and dev evaluation\n",
    "        if (step % tp.save_steps == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "            val_metrics = seq_tag_metrics(model, valid_loader, tp.idx2label, tp.schema, device)\n",
    "            for key, val in val_metrics.items():\n",
    "                tb.add_scalar(f'metric/{key}', val, global_step=global_step)\n",
    "            avg_train_loss = total_loss / step\n",
    "            tb.add_scalars('loss/train_valid',{'train': avg_train_loss,\n",
    "                                                'valid': val_metrics['val_loss']}, global_step=global_step)\n",
    "            saver(total_loss / step, val_metrics['val_loss'], epoch_i, global_step, model, optimizer, scheduler)\n",
    "\n",
    "    # On Epoch End: calcualte train & valid loss and log overall metrics\n",
    "    time_elapsed = time.time() - t0_epoch\n",
    "    val_metrics = seq_tag_metrics(model, valid_loader, tp.idx2label, tp.schema, device)\n",
    "    avg_train_loss = total_loss / step\n",
    "\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_metrics['val_loss']:^10.6f} | {time_elapsed:^9.2f}\")\n",
    "    tag_cls_log(epoch_i, val_metrics)\n",
    "    print(\"\\n\")\n",
    "    if es.check(val_metrics):\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T13:40:39.806466Z",
     "iopub.status.busy": "2022-09-26T13:40:39.806076Z",
     "iopub.status.idle": "2022-09-26T13:40:40.735980Z",
     "shell.execute_reply": "2022-09-26T13:40:40.735005Z",
     "shell.execute_reply.started": "2022-09-26T13:40:39.806413Z"
    }
   },
   "outputs": [],
   "source": [
    "valid = pd.read_csv('/kaggle/input/finent/valid.csv')\n",
    "train = pd.read_csv('/kaggle/input/finent/train.csv')\n",
    "train.fillna({'entities':''},inplace=True)\n",
    "valid.fillna({'entities':''},inplace=True)\n",
    "# 抽取训练集中已知实体\n",
    "known_entity = set(chain(*train['entities'].map(lambda x: x.split(';')).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = seqlabel_inference(model,valid_loader, device)\n",
    "valid['pred_pos'] = [get_entity_bio(i, tp.idx2label) for i in pred]\n",
    "valid['pred_entity'] = valid.apply(lambda x: extract_entity(x.corpus, x.pred_pos)['FIN'], axis=1)\n",
    "valid['pred_entity'] = valid['pred_entity'].map(lambda x: [i for i in x if islegal(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T13:46:50.614301Z",
     "iopub.status.busy": "2022-09-26T13:46:50.613936Z",
     "iopub.status.idle": "2022-09-26T13:46:50.643755Z",
     "shell.execute_reply": "2022-09-26T13:46:50.642716Z",
     "shell.execute_reply.started": "2022-09-26T13:46:50.614271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Entity Evaluation\n",
      "   precision    recall        f1\n",
      "0   0.559633  0.683621  0.615444\n",
      "Unknown Entity Evalutation\n",
      "   precision    recall        f1\n",
      "0    0.37467  0.546154  0.444444\n"
     ]
    }
   ],
   "source": [
    "stat = aggregate_f1(valid['id'].values, \n",
    "                  valid['entities'].values,\n",
    "                  valid['pred_entity'].values, \n",
    "                  known_entity)\n",
    "stat.to_csv('bert_crf.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
