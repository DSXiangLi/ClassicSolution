{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import time \n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gensim.models.phrases import Phrases\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from src.train_utils import set_seed, ModelSave, get_torch_device, EarlyStop, TrainParams\n",
    "from src.evaluation import classification_inference\n",
    "from src.metric import  multi_cls_metrics,multi_cls_log\n",
    "from src.dataset.tokenizer import GensimTokenizer\n",
    "from src.enhancement.temporal import TemporalEnsemble\n",
    "\n",
    "from iflytek_app.dataset import MixDataset\n",
    "from iflytek_app.process import train_process, test_process, result_process,kfold_inference\n",
    "from iflytek_app.models import Textcnn\n",
    "device = get_torch_device()\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id           l1           l2          len\n",
      "count  4199.000000  4199.000000  4199.000000  4199.000000\n",
      "mean   2099.000000     8.969278    37.087878    46.057156\n",
      "std    1212.291219     4.576621    79.204914    79.332999\n",
      "min       0.000000     2.000000     1.000000     4.000000\n",
      "25%    1049.500000     5.000000     6.000000    15.000000\n",
      "50%    2099.000000     8.000000    12.000000    22.000000\n",
      "75%    3148.500000    12.000000    26.000000    36.000000\n",
      "max    4198.000000    32.000000   946.000000   961.000000\n",
      "{'14784131 14858934 14784131 14845064': 0, '14852788 14717848 15639958 15632020': 1, '14844856 14724258 14925237 14854807': 2, '14925756 15639967 14853254 14728639': 3, '14844593 14924945': 4, '15709098 14716590 14924703 14779559': 5, '14726332 14728344 14854542 14844591': 6, '14858934 15636660 15704193 14849963': 7, '15710359 14847407 14845602 14859696': 8, '14794687 14782344': 9, '15630486 15702410 14718849 15709093': 10, '15632285 15706536 14721977 14925219': 11, '14782903 15634620 15638402 15706300': 12, '14844093 15705739 14854331 15699885': 13, '14856354 14844592': 14, '14847385 14844587 14848641 14847398': 15, '14783134 15697333 14854817 14925479': 16, '14924216 14781104 14717848 14791612': 17, '14786237 15697082 14722731 14924977': 18}\n"
     ]
    }
   ],
   "source": [
    "c2v = GensimTokenizer( Word2Vec.load('./checkpoint/char_min1_win5_sg_d100'))\n",
    "w2v = GensimTokenizer(Word2Vec.load('./checkpoint/phrase_min1_win5_sg_d100'))\n",
    "w2v.init_vocab()\n",
    "c2v.init_vocab()\n",
    "phraser = Phrases.load('./checkpoint/phrase_tokenizer')\n",
    "df, label2idx = train_process()\n",
    "test = test_process()\n",
    "label2idx.update({'unlabel':-1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PseudoLabel(object):\n",
    "    def __init__(self, tp,):\n",
    "        self.T1 = tp.T1\n",
    "        self.T2 = tp.T2\n",
    "        self.alpha_f = tp.alpha_f\n",
    "        self.cur_epoch = 0\n",
    "        self.loss_fn = tp.loss_fn\n",
    "        self.alpha_t = 0 \n",
    "\n",
    "    def compute_loss(self, features, logits):\n",
    "        labels = features['label']\n",
    "        cond = labels >= 0\n",
    "        # supervised_loss\n",
    "        self.supervised_loss = self.loss_fn(logits[cond], labels[cond])\n",
    "        \n",
    "        # unsupervised_loss\n",
    "        with torch.no_grad():\n",
    "            pseudo_label = torch.argmax(logits, dim=-1)\n",
    "        cond = labels <0 \n",
    "        self.unsupervised_loss = self.loss_fn(logits[cond], pseudo_label[cond])\n",
    "        \n",
    "        loss = self.supervised_loss + self.alpha_t * self.unsupervised_loss\n",
    "\n",
    "        return loss \n",
    "\n",
    "    def epoch_update(self):\n",
    "        self.cur_epoch +=1 \n",
    "        self.alpha_t = self.unlabeled_weight() \n",
    "        \n",
    "    def unlabeled_weight(self):\n",
    "        alpha = 0.0\n",
    "        if self.cur_epoch > self.T1:\n",
    "            alpha = (self.cur_epoch - self.T1) / (self.T2 - self.T1) * self.alpha_f\n",
    "            if self.cur_epoch > self.T2:\n",
    "                alpha = self.alpha_f\n",
    "        return alpha\n",
    "\n",
    "\n",
    "class TextcnnPseudoLabel(PseudoLabel, Textcnn):\n",
    "    def __init__(self, tp):\n",
    "        # init nn.Module before others\n",
    "        Textcnn.__init__(self, tp)\n",
    "        PseudoLabel.__init__(self, tp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_steps = 10\n",
    "save_steps = 20\n",
    "kfold=5\n",
    "tp = TrainParams(\n",
    "    epoch_size=30,\n",
    "    lr=1e-3,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    max_seq_len=1000,\n",
    "    batch_size=64,\n",
    "    dropout_rate=0.5,\n",
    "    label_size = len(label2idx),\n",
    "    vocab_size = w2v.vocab_size,\n",
    "    embedding_dim = w2v.embedding_size + c2v.embedding_size,\n",
    "    embedding1=c2v.embedding, \n",
    "    embedding2 =w2v.embedding,\n",
    "    filter_size=70,\n",
    "    kernel_size_list = [2,3,4,5],\n",
    "    hidden_size = 100,\n",
    "    early_stop_params = {\n",
    "        'monitor':'f1_micro',\n",
    "        'mode':'max',\n",
    "        'min_delta': 0,\n",
    "        'patience':5,\n",
    "        'verbose':False\n",
    "    },\n",
    "    scheduler_params={'mode': 'max',\n",
    "                     'factor': 0.3,\n",
    "                     'patience': 1,\n",
    "                     'verbose': True,\n",
    "                     'threshold':0.0001,\n",
    "                     'threshold_mode':'rel',\n",
    "                     'cooldown':0,\n",
    "                     'min_lr':1e-6},\n",
    "    T1=5,\n",
    "    T2=20,\n",
    "    alpha_f=1.5\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoint/textcnn_pseudo_label/k0 model cleaned\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   10    |   3.912528   |     -     |   8.06   \n",
      "   1    |   20    |   2.671500   |     -     |   7.25   \n",
      "   1    |   30    |   2.725575   |     -     |   10.78  \n",
      "   1    |   40    |   2.505056   |     -     |   7.14   \n",
      "   1    |   50    |   2.357139   |     -     |   9.90   \n",
      "   1    |   60    |   2.118769   |     -     |   6.89   \n",
      "   1    |   70    |   2.048186   |     -     |   10.03  \n",
      "   1    |   80    |   1.808631   |     -     |   6.54   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   2.567329   |    nan     |   69.64  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   1    |  18.251%  |  72.732%  |  30.818%  |     21.304%     |   18.251%    |  42.738%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   1    |  42.738%  |     -     |     -     |     42.738%     |   42.738%    |  42.738%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   2    |   10    |   4.447065   |     -     |   8.40   \n",
      "   2    |   20    |   1.834670   |     -     |   7.25   \n",
      "   2    |   30    |   1.668354   |     -     |   10.04  \n",
      "   2    |   40    |   1.528733   |     -     |   7.15   \n",
      "   2    |   50    |   1.476189   |     -     |   10.28  \n",
      "   2    |   60    |   1.354194   |     -     |   7.15   \n",
      "   2    |   70    |   1.409804   |     -     |   10.20  \n",
      "   2    |   80    |   1.241293   |     -     |   6.73   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   1.925626   |    nan     |   70.40  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   2    |  36.600%  |  80.229%  |  47.855%  |     41.170%     |   36.600%    |  59.286%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   2    |  59.286%  |     -     |     -     |     59.286%     |   59.286%    |  59.286%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   3    |   10    |   1.422880   |     -     |   8.10   \n",
      "   3    |   20    |   1.421026   |     -     |   7.24   \n",
      "   3    |   30    |   1.128445   |     -     |   9.86   \n",
      "   3    |   40    |   1.067784   |     -     |   7.91   \n",
      "   3    |   50    |   0.938020   |     -     |   11.26  \n",
      "   3    |   60    |   1.080649   |     -     |   7.44   \n",
      "   3    |   70    |   0.996925   |     -     |   11.71  \n",
      "   3    |   80    |   0.885866   |     -     |   6.82   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   1.135485   |    nan     |   73.49  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   3    |  49.157%  |  82.344%  |  55.702%  |     48.079%     |   49.157%    |  65.833%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   3    |  65.833%  |     -     |     -     |     65.833%     |   65.833%    |  65.833%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   4    |   10    |   0.932323   |     -     |   7.85   \n",
      "   4    |   20    |   0.894299   |     -     |   7.00   \n",
      "   4    |   30    |   0.807506   |     -     |   10.30  \n",
      "   4    |   40    |   0.720713   |     -     |   6.99   \n",
      "   4    |   50    |   0.831538   |     -     |   10.13  \n",
      "   4    |   60    |   0.760785   |     -     |   7.29   \n",
      "   4    |   70    |   0.757456   |     -     |   10.66  \n",
      "   4    |   80    |   0.703643   |     -     |   6.79   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.812687   |    nan     |   70.16  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   4    |  50.870%  |  84.133%  |  60.209%  |     52.339%     |   50.870%    |  70.000%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   4    |  70.000%  |     -     |     -     |     70.000%     |   70.000%    |  70.000%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   5    |   10    |   0.663542   |     -     |   7.81   \n",
      "   5    |   20    |   0.738137   |     -     |   7.27   \n",
      "   5    |   30    |   0.641565   |     -     |   10.20  \n",
      "   5    |   40    |   0.536614   |     -     |   7.05   \n",
      "   5    |   50    |   0.514395   |     -     |   10.25  \n",
      "   5    |   60    |   0.538506   |     -     |   7.05   \n",
      "   5    |   70    |   0.497514   |     -     |   9.76   \n",
      "   5    |   80    |   0.498304   |     -     |   6.49   \n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=kfold, shuffle=True, random_state=24)\n",
    "for fold,(train_index, valid_index) in enumerate(kf.split(df)):\n",
    "    train, valid = df.iloc[train_index], df.iloc[valid_index]\n",
    "\n",
    "    # combine label and unlabel data\n",
    "    train_dataset = MixDataset(tp.max_seq_len, w2v, c2v, phraser, label2idx, \n",
    "                               train['name'].values.tolist() + test['name'].values.tolist(),\n",
    "                               train['description'].values.tolist() + test['name'].values.tolist(),\n",
    "                               train['label'].values.tolist() + ['unlabel']* test.shape[0])\n",
    "    valid_dataset = MixDataset(tp.max_seq_len, w2v, c2v, phraser, label2idx, \n",
    "                               valid['name'].values, valid['description'].values, valid['label'].values)\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    valid_sampler = SequentialSampler(valid_dataset)\n",
    "    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=tp.batch_size)\n",
    "    valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=tp.batch_size)\n",
    "    \n",
    "    tp.update({'num_train_steps': len(train_dataset)})\n",
    "    \n",
    "    CKPT = './checkpoint/textcnn_pseudo_label/k{}'.format(fold)\n",
    "    saver = ModelSave(CKPT, continue_train=False)\n",
    "    saver.init()\n",
    "    tb = SummaryWriter(CKPT)\n",
    "    es = EarlyStop(**tp.early_stop_params)\n",
    "    global_step = 0\n",
    "    model = TextcnnPseudoLabel(tp)\n",
    "    optimizer, scheduler = model.get_optimizer()\n",
    "    \n",
    "    for epoch_i in range(tp['epoch_size']):\n",
    "        if global_step==1:\n",
    "            print(model)\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10}  | {'Elapsed':^9}\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            global_step +=1\n",
    "            batch_counts +=1\n",
    "\n",
    "            #Forward propogate\n",
    "            model.zero_grad()\n",
    "            feature = {k:v.to(device) for k, v in batch.items()}\n",
    "            logits = model(feature)\n",
    "            loss= model.compute_loss(feature, logits)\n",
    "            tb.add_scalar('loss/sup_loss', model.supervised_loss, global_step=global_step)\n",
    "            tb.add_scalar('loss/unsup_loss', model.unsupervised_loss, global_step=global_step)\n",
    "            tb.add_scalar('loss/avg_loss', loss, global_step=global_step)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log steps for train loss logging\n",
    "            if (step % log_steps == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "            # Save steps for ckpt saving and dev evaluation\n",
    "            if (step % save_steps == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "                val_metrics = multi_cls_metrics(model, valid_loader, device)\n",
    "                for key, val in val_metrics.items():\n",
    "                    tb.add_scalar(f'metric/{key}', val, global_step=global_step)\n",
    "                avg_train_loss = total_loss / step\n",
    "                tb.add_scalars('loss/train_valid',{'train': avg_train_loss,\n",
    "                                                    'valid': val_metrics['val_loss']}, global_step=global_step)\n",
    "                saver(total_loss / step, val_metrics['val_loss'], epoch_i, global_step, model, optimizer, scheduler)\n",
    "        model.epoch_update()\n",
    "        # On Epoch End: calcualte train & valid loss and log overall metrics\n",
    "        time_elapsed = time.time() - t0_epoch\n",
    "        val_metrics = multi_cls_metrics(model, valid_loader, device)\n",
    "        avg_train_loss = total_loss / step\n",
    "        scheduler.step(val_metrics['f1_micro'])\n",
    "        print(\"-\"*70)\n",
    "        print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_metrics['val_loss']:^10.6f} | {time_elapsed:^9.2f}\")\n",
    "        multi_cls_log(epoch_i, val_metrics)\n",
    "        print(\"\\n\")\n",
    "        if es.check(val_metrics):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## K-Fold Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T13:31:28.634820Z",
     "iopub.status.busy": "2022-08-05T13:31:28.634312Z",
     "iopub.status.idle": "2022-08-05T13:31:41.406697Z",
     "shell.execute_reply": "2022-08-05T13:31:41.405987Z",
     "shell.execute_reply.started": "2022-08-05T13:31:28.634794Z"
    }
   },
   "outputs": [],
   "source": [
    "test = test_process()\n",
    "test_dataset = MixDataset(tp.max_seq_len, w2v, c2v, phraser, label2idx,  test['name'].values, test['description'].values)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_loader = DataLoader(test_dataset, sampler=test_sampler, batch_size=tp.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result = kfold_inference(test_loader, tp, Textcnn, './checkpoint/textcnn_pseudo_label', 5, device)\n",
    "result['pred'] = result['pred_avg']\n",
    "result_process(result, label2idx, './submit/textcnn_pseudo_label_5fold_avg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_tf23",
   "language": "python",
   "name": "py36_tf23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
