{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9311ab33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import time \n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gensim.models.phrases import Phrases\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from src.train_utils import set_seed, ModelSave, get_torch_device, EarlyStop, TrainParams\n",
    "from src.evaluation import classification_inference\n",
    "from src.metric import  multi_cls_metrics,multi_cls_log\n",
    "from src.dataset.tokenizer import GensimTokenizer\n",
    "\n",
    "from iflytek_app.dataset import MixDataset\n",
    "from iflytek_app.models import Textcnn\n",
    "from iflytek_app.process import train_process, test_process, result_process\n",
    "\n",
    "device = get_torch_device()\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "470c4855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id           l1           l2          len\n",
      "count  4199.000000  4199.000000  4199.000000  4199.000000\n",
      "mean   2099.000000     8.969278    37.087878    46.057156\n",
      "std    1212.291219     4.576621    79.204914    79.332999\n",
      "min       0.000000     2.000000     1.000000     4.000000\n",
      "25%    1049.500000     5.000000     6.000000    15.000000\n",
      "50%    2099.000000     8.000000    12.000000    22.000000\n",
      "75%    3148.500000    12.000000    26.000000    36.000000\n",
      "max    4198.000000    32.000000   946.000000   961.000000\n",
      "{'14784131 14858934 14784131 14845064': 0, '14852788 14717848 15639958 15632020': 1, '14844856 14724258 14925237 14854807': 2, '14925756 15639967 14853254 14728639': 3, '14844593 14924945': 4, '15709098 14716590 14924703 14779559': 5, '14726332 14728344 14854542 14844591': 6, '14858934 15636660 15704193 14849963': 7, '15710359 14847407 14845602 14859696': 8, '14794687 14782344': 9, '15630486 15702410 14718849 15709093': 10, '15632285 15706536 14721977 14925219': 11, '14782903 15634620 15638402 15706300': 12, '14844093 15705739 14854331 15699885': 13, '14856354 14844592': 14, '14847385 14844587 14848641 14847398': 15, '14783134 15697333 14854817 14925479': 16, '14924216 14781104 14717848 14791612': 17, '14786237 15697082 14722731 14924977': 18}\n"
     ]
    }
   ],
   "source": [
    "c2v = GensimTokenizer( Word2Vec.load('./checkpoint/char_min1_win5_sg_d100'))\n",
    "w2v = GensimTokenizer(Word2Vec.load('./checkpoint/phrase_min1_win5_sg_d100'))\n",
    "w2v.init_vocab()\n",
    "c2v.init_vocab()\n",
    "phraser = Phrases.load('./checkpoint/phrase_tokenizer')\n",
    "df, label2idx = train_process()\n",
    "test = test_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7568649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-coding:utf-8 -*-\n",
    "\n",
    "from src.preprocess.str_utils import *\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "class Augmenter(object):\n",
    "    \"\"\"\n",
    "    Action: Delete, Swap, Substitute\n",
    "    Granularity: char, word, entity, sentence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_sample, max_sample, prob):\n",
    "        self.min_sample = min_sample\n",
    "        self.max_sample = max_sample\n",
    "        self.prob = prob\n",
    "\n",
    "    def action(self, text):\n",
    "        \"\"\"\n",
    "        Core Augment action\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def _data_check(data):\n",
    "        if not data or len(data) == 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def _param_check(self):\n",
    "        pass\n",
    "\n",
    "    def augment(self, data, n_thread=4):\n",
    "        \"\"\"\n",
    "\n",
    "        :param augment:\n",
    "        :param data:\n",
    "        :param n:\n",
    "        :param n_thead:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        min_output = len(data) * self.min_sample\n",
    "        max_output = len(data) * self.max_sample  # 默认增强样本<=原始样本\n",
    "        max_retry = 3\n",
    "        result = set()  # only keep non-dupulicate\n",
    "        for _ in range(max_retry):\n",
    "            with ThreadPoolExecutor(n_thread) as executor:\n",
    "                for aug_data in executor.map(self.action, data):\n",
    "                    if self._data_check(aug_data):\n",
    "                        result.add(aug_data)\n",
    "            if len(result) > min_output:\n",
    "                break\n",
    "        if len(result) > max_output:\n",
    "            return random.sample(result, max_output)\n",
    "        else:\n",
    "            return result\n",
    "        \n",
    "        \n",
    "class WordAugmenter(Augmenter):\n",
    "    def __init__(self, min_sample, max_sample, prob, tokenizer):\n",
    "        super(WordAugmenter, self).__init__(min_sample, max_sample, prob)\n",
    "        self.filters = [stop_word_handler, punctuation_handler, emoji_handler]\n",
    "        self.tokenizer = tokenizer \n",
    "\n",
    "    def get_aug_index(self, tokens):\n",
    "        index = set()\n",
    "        for i, t in enumerate(tokens):\n",
    "            if any(f.check(t) for f in self.filters):\n",
    "                continue\n",
    "            index.add(i)\n",
    "        return index\n",
    "\n",
    "\n",
    "class W2vSynomous(WordAugmenter):\n",
    "    def __init__(self, min_sample, max_sample, prob, tokenizer, topn=10):\n",
    "        super(W2vSynomous, self).__init__(min_sample, max_sample, prob, tokenizer)\n",
    "        self.topn = topn\n",
    "\n",
    "    def gen_synom(self, word):\n",
    "        if random.random() < self.prob:\n",
    "            try:\n",
    "                nn = self.tokenizer.model.most_similar(word, topn=self.topn)\n",
    "                return random.choice(nn)[0]\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def action(self, text):\n",
    "        new_sample = []\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        flag = False\n",
    "        for i, t in enumerate(words):\n",
    "            if i in self.get_aug_index(words):\n",
    "                self.gen_synom(t)\n",
    "                flag = True\n",
    "            else:\n",
    "                new_sample.append(t)\n",
    "        if flag:\n",
    "            return new_sample\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "class WordnetSynomous(WordAugmenter):\n",
    "    def __init__(self, min_sample, max_sample, prob, tokenizer):\n",
    "        super(WordnetSynomous, self).__init__(min_sample, max_sample, prob, tokenizer)\n",
    "        self.wordnet = self.load('word_net.text')\n",
    "\n",
    "    def load(self, file):\n",
    "        wordnet = {}\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split(\" \")\n",
    "                if not line[0].endswith('='):\n",
    "                    continue\n",
    "                for i in range(1, len(line)):\n",
    "                    wordnet[line[i]] = line[1:i] + line[(i + 1):]\n",
    "        return wordnet\n",
    "\n",
    "    def gen_synom(self, word):\n",
    "        if word in self.wordnet and random.random() < self.prob:\n",
    "            return random.choice(self.wordnet[word])\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "    def action(self, text):\n",
    "        new_sample = []\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        flag = False\n",
    "        for i, t in enumerate(tokens):\n",
    "            if i in self.get_aug_index(tokens):\n",
    "                self.gen_synom(t)\n",
    "                flag = True\n",
    "            else:\n",
    "                new_sample.append(t)\n",
    "        if flag:\n",
    "            return tokens\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "class WordShuffle(WordAugmenter):\n",
    "    def __init__(self, min_sample, max_sample, prob, tokenizer):\n",
    "        super(WordShuffle, self).__init__(min_sample, max_sample, prob, tokenizer)\n",
    "\n",
    "    def get_swap_pos(self, left, right):\n",
    "        if random.random() < self.prob:\n",
    "            return random.randint(left, right)\n",
    "        else:\n",
    "            return left - 1\n",
    "\n",
    "    def action(self, text):\n",
    "        new_sample = []\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        l = len(text)\n",
    "        for i, t in enumerate(tokens):\n",
    "            if i in self.get_aug_index(tokens):\n",
    "                pos = self.get_swap_pos(i + 1, l - 1)\n",
    "                tokens[i], tokens[pos] = tokens[pos], tokens[i]\n",
    "            new_sample.append(tokens[i])\n",
    "        return new_sample\n",
    "\n",
    "\n",
    "class WordDelete(WordAugmenter):\n",
    "    def __init__(self, min_sample, max_sample, prob, tokenizer):\n",
    "        super(WordDelete, self).__init__(min_sample, max_sample, prob, tokenizer)\n",
    "\n",
    "    def action(self, text):\n",
    "        new_sample = []\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        for i, t in enumerate(tokens):\n",
    "            if i in self.get_aug_index(tokens):\n",
    "                if random.random()< self.prob:\n",
    "                    continue\n",
    "            new_sample.append(t)\n",
    "        return new_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a24c4",
   "metadata": {},
   "source": [
    "## Random Delete "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf0991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80aa003f",
   "metadata": {},
   "source": [
    "## Random Insert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_tf23",
   "language": "python",
   "name": "py36_tf23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
