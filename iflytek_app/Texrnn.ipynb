{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "\n",
    "import torch \n",
    "import time \n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gensim.models.phrases import Phrases\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from src.train_utils import set_seed, ModelSave, get_torch_device, EarlyStop, TrainParams\n",
    "from src.evaluation import multi_cls_report, classification_inference\n",
    "from src.metric import  multi_cls_metrics,multi_cls_log\n",
    "from src.dataset.tokenizer import GensimTokenizer\n",
    "\n",
    "from iflytek_app.dataset import MixDataset\n",
    "from iflytek_app.models import Textrnn\n",
    "from iflytek_app.process import train_process, test_process, result_process\n",
    "\n",
    "device = get_torch_device()\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id           l1           l2          len\n",
      "count  4199.000000  4199.000000  4199.000000  4199.000000\n",
      "mean   2099.000000     8.969278    37.087878    46.057156\n",
      "std    1212.291219     4.576621    79.204914    79.332999\n",
      "min       0.000000     2.000000     1.000000     4.000000\n",
      "25%    1049.500000     5.000000     6.000000    15.000000\n",
      "50%    2099.000000     8.000000    12.000000    22.000000\n",
      "75%    3148.500000    12.000000    26.000000    36.000000\n",
      "max    4198.000000    32.000000   946.000000   961.000000\n",
      "{'14784131 14858934 14784131 14845064': 0, '14852788 14717848 15639958 15632020': 1, '14844856 14724258 14925237 14854807': 2, '14925756 15639967 14853254 14728639': 3, '14844593 14924945': 4, '15709098 14716590 14924703 14779559': 5, '14726332 14728344 14854542 14844591': 6, '14858934 15636660 15704193 14849963': 7, '15710359 14847407 14845602 14859696': 8, '14794687 14782344': 9, '15630486 15702410 14718849 15709093': 10, '15632285 15706536 14721977 14925219': 11, '14782903 15634620 15638402 15706300': 12, '14844093 15705739 14854331 15699885': 13, '14856354 14844592': 14, '14847385 14844587 14848641 14847398': 15, '14783134 15697333 14854817 14925479': 16, '14924216 14781104 14717848 14791612': 17, '14786237 15697082 14722731 14924977': 18}\n"
     ]
    }
   ],
   "source": [
    "c2v = GensimTokenizer( Word2Vec.load('./checkpoint/char_min1_win5_sg_d100'))\n",
    "w2v = GensimTokenizer(Word2Vec.load('./checkpoint/phrase_min1_win5_sg_d100'))\n",
    "w2v.init_vocab()\n",
    "c2v.init_vocab()\n",
    "phraser = Phrases.load('./checkpoint/phrase_tokenizer')\n",
    "df, label2idx = train_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 5-Fold Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoint/textrnn/k0 model cleaned\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   10    |   2.571497   |     -     |  110.04  \n",
      "   1    |   20    |   2.196075   |     -     |  101.90  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86188\\.conda\\envs\\py36_tf23\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\86188\\.conda\\envs\\py36_tf23\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\86188\\.conda\\envs\\py36_tf23\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\86188\\.conda\\envs\\py36_tf23\\lib\\site-packages\\torchmetrics\\functional\\classification\\average_precision.py:168: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in average\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |   30    |   2.324364   |     -     |  149.04  \n",
      "   1    |   40    |   2.003108   |     -     |  102.15  \n",
      "   1    |   50    |   1.899856   |     -     |  147.36  \n",
      "   1    |   52    |   1.571112   |     -     |   15.93  \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   2.224283   |  1.814159  |  673.17  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   1    |  19.445%  |  76.008%  |  27.603%  |     23.066%     |   19.445%    |  41.786%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   1    |  41.786%  |     -     |     -     |     41.786%     |   41.786%    |  41.786%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   2    |   10    |   1.801233   |     -     |  312.65  \n",
      "   2    |   20    |   1.714370   |     -     |   54.81  \n",
      "   2    |   30    |   1.532587   |     -     |  147.93  \n",
      "   2    |   40    |   1.467958   |     -     |  1489.25 \n",
      "   2    |   50    |   1.404660   |     -     |   80.54  \n",
      "   2    |   52    |   1.432830   |     -     |   12.30  \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   1.612980   |  1.460855  |  2144.30 \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   2    |  33.675%  |  82.625%  |  44.331%  |     35.377%     |   33.675%    |  54.881%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   2    |  54.881%  |     -     |     -     |     54.881%     |   54.881%    |  54.881%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   3    |   10    |   1.272777   |     -     |  112.17  \n",
      "   3    |   20    |   1.162103   |     -     |  301.21  \n",
      "   3    |   30    |   1.122874   |     -     |  141.97  \n",
      "   3    |   40    |   1.005740   |     -     |  104.56  \n",
      "   3    |   50    |   1.063242   |     -     |  153.30  \n",
      "   3    |   52    |   1.471388   |     -     |   16.81  \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   1.163133   |  1.181218  |  880.17  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   3    |  44.651%  |  86.628%  |  54.615%  |     52.806%     |   44.651%    |  64.643%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   3    |  64.643%  |     -     |     -     |     64.643%     |   64.643%    |  64.643%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   4    |   10    |   0.856775   |     -     |  114.40  \n",
      "   4    |   20    |   0.768154   |     -     |  104.28  \n",
      "   4    |   30    |   0.700225   |     -     |  157.47  \n",
      "   4    |   40    |   0.766133   |     -     |  103.20  \n",
      "   4    |   50    |   0.785462   |     -     |  151.56  \n",
      "   4    |   52    |   0.730937   |     -     |   15.93  \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.790118   |  1.085877  |  693.91  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   4    |  49.872%  |  88.199%  |  58.503%  |     59.993%     |   49.872%    |  67.500%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   4    |  67.500%  |     -     |     -     |     67.500%     |   67.500%    |  67.500%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   5    |   10    |   0.564585   |     -     |  208.77  \n",
      "   5    |   20    |   0.583123   |     -     |  171.44  \n",
      "   5    |   30    |   0.571618   |     -     |  228.49  \n",
      "   5    |   40    |   0.478292   |     -     |  225.49  \n",
      "   5    |   50    |   0.539729   |     -     |  255.83  \n",
      "   5    |   52    |   0.453792   |     -     |   25.80  \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.554724   |  1.048159  |  1198.76 \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   5    |  55.467%  |  89.397%  |  66.036%  |     62.897%     |   55.467%    |  71.905%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   5    |  71.905%  |     -     |     -     |     71.905%     |   71.905%    |  71.905%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   6    |   10    |   0.377996   |     -     |  164.14  \n",
      "   6    |   20    |   0.422209   |     -     |  164.95  \n",
      "   6    |   30    |   0.380082   |     -     |  231.10  \n",
      "   6    |   40    |   0.335797   |     -     |  163.61  \n",
      "   6    |   50    |   0.403411   |     -     |  252.92  \n",
      "   6    |   52    |   0.242709   |     -     |   23.02  \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   0.385738   |  1.156434  |  1072.08 \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   6    |  56.995%  |  88.876%  |  62.673%  |     59.294%     |   56.995%    |  69.524%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   6    |  69.524%  |     -     |     -     |     69.524%     |   69.524%    |  69.524%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   7    |   10    |   0.255592   |     -     |  174.47  \n",
      "   7    |   20    |   0.291549   |     -     |  152.40  \n",
      "   7    |   30    |   0.256392   |     -     |  206.86  \n",
      "   7    |   40    |   0.286281   |     -     |  144.61  \n",
      "   7    |   50    |   0.293720   |     -     |  251.61  \n",
      "   7    |   52    |   0.181125   |     -     |   28.18  \n",
      "Epoch     7: reducing learning rate of group 0 to 3.0000e-04.\n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   0.277946   |  1.245895  |  1032.62 \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   7    |  57.131%  |  89.131%  |  61.913%  |     61.050%     |   57.131%    |  70.119%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   7    |  70.119%  |     -     |     -     |     70.119%     |   70.119%    |  70.119%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   8    |   10    |   0.181360   |     -     |  172.58  \n",
      "   8    |   20    |   0.173022   |     -     |  158.43  \n",
      "   8    |   30    |   0.149135   |     -     |  218.97  \n",
      "   8    |   40    |   0.143942   |     -     |  134.12  \n",
      "   8    |   50    |   0.156200   |     -     |  158.03  \n",
      "   8    |   52    |   0.101512   |     -     |   16.89  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   0.161942   |  1.247404  |  905.51  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   8    |  59.529%  |  89.528%  |  63.325%  |     61.600%     |   59.529%    |  71.429%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   8    |  71.429%  |     -     |     -     |     71.429%     |   71.429%    |  71.429%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   9    |   10    |   0.117301   |     -     |  120.75  \n",
      "   9    |   20    |   0.119059   |     -     |  106.71  \n",
      "   9    |   30    |   0.096993   |     -     |  149.13  \n",
      "   9    |   40    |   0.115846   |     -     |  107.41  \n",
      "   9    |   50    |   0.092280   |     -     |  152.22  \n",
      "   9    |   52    |   0.110465   |     -     |   16.61  \n",
      "Epoch     9: reducing learning rate of group 0 to 9.0000e-05.\n",
      "----------------------------------------------------------------------\n",
      "   9    |    -    |   0.110635   |  1.319922  |  698.83  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   9    |  58.405%  |  89.423%  |  63.492%  |     59.295%     |   58.405%    |  71.071%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   9    |  71.071%  |     -     |     -     |     71.071%     |   71.071%    |  71.071%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "  10    |   10    |   0.089007   |     -     |  138.53  \n",
      "  10    |   20    |   0.092589   |     -     |  166.82  \n",
      "  10    |   30    |   0.078000   |     -     |  225.50  \n",
      "  10    |   40    |   0.064300   |     -     |  164.55  \n",
      "  10    |   50    |   0.103100   |     -     |  222.18  \n",
      "  10    |   52    |   0.043514   |     -     |   24.63  \n",
      "----------------------------------------------------------------------\n",
      "  10    |    -    |   0.085500   |  1.329266  |  1017.19 \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  10    |  58.242%  |  89.470%  |  63.445%  |     59.029%     |   58.242%    |  70.833%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  10    |  70.833%  |     -     |     -     |     70.833%     |   70.833%    |  70.833%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "  11    |   10    |   0.072207   |     -     |  160.58  \n",
      "  11    |   20    |   0.085956   |     -     |  155.01  \n",
      "  11    |   30    |   0.061016   |     -     |  230.10  \n",
      "  11    |   40    |   0.074277   |     -     |  141.11  \n",
      "  11    |   50    |   0.081225   |     -     |  202.63  \n",
      "  11    |   52    |   0.086565   |     -     |   21.46  \n",
      "Epoch    11: reducing learning rate of group 0 to 2.7000e-05.\n",
      "----------------------------------------------------------------------\n",
      "  11    |    -    |   0.076772   |  1.357265  |  965.76  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  11    |  57.935%  |  89.509%  |  62.997%  |     59.959%     |   57.935%    |  70.833%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  11    |  70.833%  |     -     |     -     |     70.833%     |   70.833%    |  70.833%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "  12    |   10    |   0.089161   |     -     |  151.94  \n",
      "  12    |   20    |   0.088461   |     -     |  135.96  \n",
      "  12    |   30    |   0.048622   |     -     |  193.87  \n",
      "  12    |   40    |   0.065487   |     -     |  135.41  \n",
      "  12    |   50    |   0.065698   |     -     |  195.54  \n",
      "  12    |   52    |   0.166373   |     -     |   21.32  \n",
      "----------------------------------------------------------------------\n",
      "  12    |    -    |   0.076850   |  1.363084  |  893.17  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  12    |  58.010%  |  89.545%  |  63.481%  |     59.075%     |   58.010%    |  70.952%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  12    |  70.952%  |     -     |     -     |     70.952%     |   70.952%    |  70.952%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "  13    |   10    |   0.048422   |     -     |  151.34  \n",
      "  13    |   20    |   0.061659   |     -     |  136.35  \n",
      "  13    |   30    |   0.074593   |     -     |  411.81  \n",
      "  13    |   40    |   0.080013   |     -     |  110.76  \n",
      "  13    |   50    |   0.072823   |     -     |  158.71  \n",
      "  13    |   52    |   0.052518   |     -     |   17.41  \n",
      "Epoch    13: reducing learning rate of group 0 to 8.1000e-06.\n",
      "----------------------------------------------------------------------\n",
      "  13    |    -    |   0.067857   |  1.367341  |  1033.49 \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  13    |  57.902%  |  89.530%  |  62.781%  |     59.099%     |   57.902%    |  70.833%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  13    |  70.833%  |     -     |     -     |     70.833%     |   70.833%    |  70.833%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "  14    |   10    |   0.081632   |     -     |  120.67  \n",
      "  14    |   20    |   0.078173   |     -     |  111.58  \n",
      "  14    |   30    |   0.057356   |     -     |  156.09  \n",
      "  14    |   40    |   0.061299   |     -     |  109.27  \n",
      "  14    |   50    |   0.055862   |     -     |  154.12  \n",
      "  14    |   52    |   0.059637   |     -     |   17.51  \n",
      "----------------------------------------------------------------------\n",
      "  14    |    -    |   0.068156   |  1.369793  |  716.42  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  14    |  57.706%  |  89.523%  |  63.086%  |     59.120%     |   57.706%    |  70.952%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  14    |  70.952%  |     -     |     -     |     70.952%     |   70.952%    |  70.952%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "  15    |   10    |   0.066110   |     -     |  121.77  \n",
      "  15    |   20    |   0.058734   |     -     |  110.74  \n",
      "  15    |   30    |   0.070241   |     -     |  157.82  \n",
      "  15    |   40    |   0.067686   |     -     |  110.61  \n",
      "  15    |   50    |   0.061618   |     -     |  315.22  \n",
      "  15    |   52    |   0.104365   |     -     |   16.12  \n",
      "Epoch    15: reducing learning rate of group 0 to 2.4300e-06.\n",
      "----------------------------------------------------------------------\n",
      "  15    |    -    |   0.067668   |  1.371830  |  876.76  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  15    |  57.746%  |  89.520%  |  63.013%  |     58.971%     |   57.746%    |  71.071%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  15    |  71.071%  |     -     |     -     |     71.071%     |   71.071%    |  71.071%  \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoint/textrnn/k1 model cleaned\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   10    |   2.624667   |     -     |  129.88  \n",
      "   1    |   20    |   2.233711   |     -     |  113.71  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-13996f821d56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py36_tf23\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py36_tf23\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log_steps = 10\n",
    "save_steps = 20\n",
    "kfold= 5\n",
    "tp = TrainParams(\n",
    "    epoch_size=30,\n",
    "    lr=1e-3,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    max_seq_len=1000,\n",
    "    batch_size=64,\n",
    "    dropout_rate=0.5,\n",
    "    label_size = len(label2idx),\n",
    "    vocab_size = w2v.vocab_size,\n",
    "    embedding_dim = w2v.embedding_size + c2v.embedding_size,\n",
    "    embedding1=c2v.embedding, \n",
    "    embedding2 =w2v.embedding,\n",
    "    num_train_steps=int(df.shape[0]/kfold *(kfold-1)), \n",
    "    topk=10,\n",
    "    hidden_size = 200,\n",
    "    num_layers=1,\n",
    "    layer_type='lstm'\n",
    ")\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=kfold, shuffle=True, random_state=24)\n",
    "for fold,(train_index, valid_index) in enumerate(kf.split(df)):\n",
    "    train, valid = df.iloc[train_index], df.iloc[valid_index]\n",
    "\n",
    "    train_dataset = MixDataset(tp.max_seq_len, w2v, c2v, phraser, label2idx, \n",
    "                               train['name'].values, train['description'].values, train['label'].values)\n",
    "    valid_dataset = MixDataset(tp.max_seq_len, w2v, c2v, phraser, label2idx, \n",
    "                               valid['name'].values, valid['description'].values, valid['label'].values)\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    valid_sampler = SequentialSampler(valid_dataset)\n",
    "    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=tp.batch_size)\n",
    "    valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=tp.batch_size)\n",
    "    \n",
    "    CKPT = './checkpoint/textrnn/k{}'.format(fold)\n",
    "    saver = ModelSave(CKPT, continue_train=False)\n",
    "    tb = SummaryWriter(CKPT)\n",
    "    es = EarlyStop('f1_micro', mode='max', min_delta=0.0, patience=10, verbose=False)\n",
    "    global_step = 0\n",
    "    saver.init()\n",
    "    model = Textrnn(tp)\n",
    "\n",
    "    \n",
    "    for epoch_i in range(tp['epoch_size']):\n",
    "        if global_step==1:\n",
    "            print(model)\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10}  | {'Elapsed':^9}\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            global_step +=1\n",
    "            batch_counts +=1\n",
    "\n",
    "            #Forward propogate\n",
    "            model.zero_grad()\n",
    "            feature = {k:v.to(device) for k, v in batch.items()}\n",
    "            logits= model(feature)\n",
    "            loss = model.compute_loss(feature, logits)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log steps for train loss logging\n",
    "            if (step % log_steps == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "                tb.add_scalar('loss/batch_train', batch_loss / batch_counts, global_step=global_step)\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "            # Save steps for ckpt saving and dev evaluation\n",
    "            if (step % save_steps == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "                val_metrics = multi_cls_metrics(model, valid_loader, device)\n",
    "                for key, val in val_metrics.items():\n",
    "                    tb.add_scalar(f'metric/{key}', val, global_step=global_step)\n",
    "                avg_train_loss = total_loss / step\n",
    "                tb.add_scalars('loss/train_valid',{'train_loss': avg_train_loss,\n",
    "                                                    'valid_loss': val_metrics['val_loss']}, global_step=global_step)\n",
    "                saver(total_loss / step, val_metrics['val_loss'], epoch_i, global_step, model, optimizer, scheduler)\n",
    "\n",
    "        # On Epoch End: calcualte train & valid loss and log overall metrics\n",
    "        time_elapsed = time.time() - t0_epoch\n",
    "        val_metrics = multi_cls_metrics(model, valid_loader, device)\n",
    "        avg_train_loss = total_loss / step\n",
    "        scheduler.step(val_metrics['f1_micro'])\n",
    "        print(\"-\"*70)\n",
    "        print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_metrics['val_loss']:^10.6f} | {time_elapsed:^9.2f}\")\n",
    "        multi_cls_log(epoch_i, val_metrics)\n",
    "        print(\"\\n\")\n",
    "        if es.check(val_metrics):\n",
    "            break "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_tf23",
   "language": "python",
   "name": "py36_tf23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
