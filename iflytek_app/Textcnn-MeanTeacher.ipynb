{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import time \n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gensim.models.phrases import Phrases\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from src.train_utils import set_seed, ModelSave, get_torch_device, EarlyStop, TrainParams\n",
    "from src.metric import  multi_cls_metrics,multi_cls_log\n",
    "from src.dataset.tokenizer import GensimTokenizer\n",
    "from src.enhancement.consistency import  create_ema_model, MeanTeacher\n",
    "\n",
    "from iflytek_app.dataset import MixDataset\n",
    "from iflytek_app.process import train_process, test_process, result_process,kfold_inference\n",
    "from iflytek_app.models import Textcnn, TextcnnAugment\n",
    "\n",
    "device = get_torch_device()\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id           l1           l2          len\n",
      "count  4199.000000  4199.000000  4199.000000  4199.000000\n",
      "mean   2099.000000     8.969278    37.087878    46.057156\n",
      "std    1212.291219     4.576621    79.204914    79.332999\n",
      "min       0.000000     2.000000     1.000000     4.000000\n",
      "25%    1049.500000     5.000000     6.000000    15.000000\n",
      "50%    2099.000000     8.000000    12.000000    22.000000\n",
      "75%    3148.500000    12.000000    26.000000    36.000000\n",
      "max    4198.000000    32.000000   946.000000   961.000000\n",
      "{'14784131 14858934 14784131 14845064': 0, '14852788 14717848 15639958 15632020': 1, '14844856 14724258 14925237 14854807': 2, '14925756 15639967 14853254 14728639': 3, '14844593 14924945': 4, '15709098 14716590 14924703 14779559': 5, '14726332 14728344 14854542 14844591': 6, '14858934 15636660 15704193 14849963': 7, '15710359 14847407 14845602 14859696': 8, '14794687 14782344': 9, '15630486 15702410 14718849 15709093': 10, '15632285 15706536 14721977 14925219': 11, '14782903 15634620 15638402 15706300': 12, '14844093 15705739 14854331 15699885': 13, '14856354 14844592': 14, '14847385 14844587 14848641 14847398': 15, '14783134 15697333 14854817 14925479': 16, '14924216 14781104 14717848 14791612': 17, '14786237 15697082 14722731 14924977': 18}\n"
     ]
    }
   ],
   "source": [
    "phraser = Phrases.load('./checkpoint/phrase_tokenizer')\n",
    "c2v = GensimTokenizer( Word2Vec.load('./checkpoint/char_min1_win5_sg_d100'))\n",
    "w2v = GensimTokenizer(Word2Vec.load('./checkpoint/phrase_min1_win5_sg_d100'), phraser)\n",
    "w2v.init_vocab()\n",
    "c2v.init_vocab()\n",
    "\n",
    "df, label2idx = train_process()\n",
    "test = test_process()\n",
    "label2idx.update({'unlabel':-1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-coding:utf-8 -*-\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def ramp_up(cur_epoch, max_epoch, method):\n",
    "    \"\"\"\n",
    "    根据训练epoch来调整无标注loss部分的权重，初始epoch无标注loss权重为0\n",
    "    \"\"\"\n",
    "\n",
    "    def linear(cur_epoch, max_epoch):\n",
    "        return cur_epoch / max_epoch\n",
    "\n",
    "    def sigmoid(cur_epoch, max_epoch):\n",
    "        p = 1.0 - cur_epoch / max_epoch\n",
    "        return np.exp(-5.0 * p ** 2)\n",
    "\n",
    "    def cosine(cur_epoch, max_epoch):\n",
    "        p = cur_epoch / max_epoch\n",
    "        return 0.5 * (np.cos(np.pi * p) + 1)\n",
    "\n",
    "    if cur_epoch == 0:\n",
    "        weight = 0.0\n",
    "    else:\n",
    "        if method == 'linear':\n",
    "            weight = linear(cur_epoch, max_epoch)\n",
    "        elif method == 'sigmoid':\n",
    "            weight = sigmoid(cur_epoch, max_epoch)\n",
    "        elif method == 'cosine':\n",
    "            weight = cosine(cur_epoch, max_epoch)\n",
    "        else:\n",
    "            raise ValueError('Only linear, sigmoid, cosine method are supported')\n",
    "    return weight\n",
    "\n",
    "def create_ema_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.detach_()\n",
    "    return model\n",
    "\n",
    "\n",
    "class MeanTeacher(object):\n",
    "    def __init__(self, model, ema_model, tp, tb):\n",
    "        self.model = model\n",
    "        self.ema_model = ema_model\n",
    "        self.tp = tp\n",
    "        self.tb = tb\n",
    "        self.global_step = 0\n",
    "        self.epoch = 0\n",
    "        self.log_step = self.tp.log_steps\n",
    "        self.num_train_steps = tp.num_train_steps\n",
    "        self.loss_fn = tp.loss_fn\n",
    "        self.epoch_size = tp.epoch_size\n",
    "        self.ramp_up_method = tp.ramp_up_method\n",
    "        self.wmax = tp.max_unsupervised * tp.labeled_size / tp.num_train_steps\n",
    "        self.alpha = tp.alpha\n",
    "        self.decay = 0.02 * tp.lr\n",
    "\n",
    "        # use state dict instead of named_parameters when batch norm exits\n",
    "        for param, ema_param in zip(self.model.state_dict().values(), self.ema_model.state_dict().values()):\n",
    "            param.data.copy_(ema_param.data)\n",
    "\n",
    "    def step(self):\n",
    "        self.global_step +=1\n",
    "        self.epoch = int(self.global_step//self.num_train_steps)\n",
    "        # alpha = min(1 - 1 /(self.step+1), self.alpha)\n",
    "        for name in self.model.state_dict():\n",
    "            param = self.model.state_dict()[name]\n",
    "            ema_param = self.ema_model.state_dict()[name]\n",
    "            if ema_param.dtype==torch.float32:\n",
    "                ema_param.mul_(self.alpha)\n",
    "                ema_param.add_(param * (1-self.alpha))\n",
    "                param.mul_(1 - self.decay)\n",
    "                if self.global_step % self.log_step==0:\n",
    "                    self.tb.add_histogram(name, param, global_step=self.global_step)\n",
    "                    self.tb.add_histogram(name+'_ema', ema_param, global_step=self.global_step)\n",
    "\n",
    "    def compute_loss(self, features, logits):\n",
    "        weight = ramp_up(self.epoch, self.epoch_size, self.ramp_up_method) * self.wmax\n",
    "\n",
    "        labels = features['label']\n",
    "        cond = labels >= 0\n",
    "        # supervised_loss\n",
    "        self.supervised_loss = self.loss_fn(logits[cond], labels[cond])\n",
    "\n",
    "        # consistency loss\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = self.ema_model(features)\n",
    "            self.teacher_loss = self.loss_fn(teacher_logits[cond], labels[cond])\n",
    "\n",
    "        self.consistency_loss = torch.mean((F.softmax(logits, dim=1) - F.softmax(teacher_logits, dim=1)) ** 2)\n",
    "\n",
    "        self.tb.add_scalars('loss/sup_loss', {\n",
    "            'student': self.supervised_loss,\n",
    "            'teacher': self.teacher_loss\n",
    "        }, global_step=self.global_step)\n",
    "\n",
    "        self.tb.add_scalar('loss/consistency', self.consistency_loss, global_step=self.global_step)\n",
    "        self.tb.add_scalar('loss/weight', weight, global_step=self.global_step)\n",
    "        loss = self.supervised_loss + weight * self.consistency_loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_steps = 10\n",
    "save_steps = 20\n",
    "kfold=5\n",
    "tp = TrainParams(\n",
    "    log_steps = log_steps,\n",
    "    save_steps = save_steps,\n",
    "    epoch_size=30,\n",
    "    lr=1e-3,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    max_seq_len=1000,\n",
    "    batch_size=64,\n",
    "    dropout_rate=0.5,\n",
    "    label_size = len(label2idx),\n",
    "    vocab_size = w2v.vocab_size,\n",
    "    embedding_dim = w2v.embedding_size + c2v.embedding_size,\n",
    "    embedding1=c2v.embedding, \n",
    "    embedding2 =w2v.embedding,\n",
    "    filter_size=70,\n",
    "    kernel_size_list = [2,3,4,5],\n",
    "    hidden_size = 100,\n",
    "    early_stop_params = {\n",
    "        'monitor':'f1_micro',\n",
    "        'mode':'max',\n",
    "        'min_delta': 0,\n",
    "        'patience':5,\n",
    "        'verbose':False\n",
    "    },\n",
    "    scheduler_params={'mode': 'max',\n",
    "                     'factor': 0.3,\n",
    "                     'patience': 1,\n",
    "                     'verbose': True,\n",
    "                     'threshold':0.0001,\n",
    "                     'threshold_mode':'rel',\n",
    "                     'cooldown':0,\n",
    "                     'min_lr':1e-6},\n",
    "    alpha=0.99,\n",
    "    max_unsupervised=50,\n",
    "    labeled_size = int(df.shape[0]/kfold *(kfold-1)),\n",
    "    ramp_up_method='sigmoid',\n",
    "    spatial_dropout_rate=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor_id = df.loc[df['label'].isin(['14784131 14858934 14784131 14845064',\n",
    "                                    '14852788 14717848 15639958 15632020']),'id'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86188\\.conda\\envs\\py36_tf23\\lib\\site-packages\\sklearn\\model_selection\\_split.py:668: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoint/textcnn_mean_teacher/k0 model cleaned\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   10    |   5.151386   |     -     |   12.41  \n",
      "   1    |   20    |   2.990473   |     -     |   10.66  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86188\\.conda\\envs\\py36_tf23\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\86188\\.conda\\envs\\py36_tf23\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\86188\\.conda\\envs\\py36_tf23\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\86188\\.conda\\envs\\py36_tf23\\lib\\site-packages\\torchmetrics\\functional\\classification\\average_precision.py:168: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in average\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |   30    |   2.663738   |     -     |   14.00  \n",
      "   1    |   40    |   2.516852   |     -     |   10.51  \n",
      "   1    |   50    |   2.356057   |     -     |   13.87  \n",
      "   1    |   52    |   2.164567   |     -     |   1.56   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   3.197415   |  2.780704  |   66.36  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   1    |  12.311%  |  58.950%  |  18.401%  |     20.033%     |   12.311%    |  30.833%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   1    |  30.833%  |     -     |     -     |     30.833%     |   30.833%    |  30.833%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   2    |   10    |   2.556621   |     -     |   11.49  \n",
      "   2    |   20    |   2.491397   |     -     |   10.40  \n",
      "   2    |   30    |   2.396584   |     -     |   13.83  \n",
      "   2    |   40    |   2.360124   |     -     |   10.69  \n",
      "   2    |   50    |   2.305738   |     -     |   13.87  \n",
      "   2    |   52    |   2.318555   |     -     |   1.54   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   2.467276   |  2.616155  |   65.19  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   2    |  17.809%  |  66.813%  |  24.738%  |     26.267%     |   17.809%    |  40.119%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   2    |  40.119%  |     -     |     -     |     40.119%     |   40.119%    |  40.119%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   3    |   10    |   2.326429   |     -     |   11.47  \n",
      "   3    |   20    |   2.320712   |     -     |   10.48  \n",
      "   3    |   30    |   2.259462   |     -     |   13.73  \n",
      "   3    |   40    |   2.187598   |     -     |   10.74  \n",
      "   3    |   50    |   2.219617   |     -     |   14.04  \n",
      "   3    |   52    |   2.159200   |     -     |   1.55   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   2.303520   |  2.348108  |   65.63  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   3    |  27.106%  |  71.515%  |  29.142%  |     37.578%     |   27.106%    |  50.119%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   3    |  50.119%  |     -     |     -     |     50.119%     |   50.119%    |  50.119%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   4    |   10    |   2.198364   |     -     |   11.91  \n",
      "   4    |   20    |   2.242498   |     -     |   10.48  \n",
      "   4    |   30    |   2.073492   |     -     |   13.89  \n",
      "   4    |   40    |   2.200162   |     -     |   10.77  \n",
      "   4    |   50    |   2.141173   |     -     |   16.33  \n",
      "   4    |   52    |   2.095936   |     -     |   1.63   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   2.210522   |  2.176547  |   68.88  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   4    |  30.824%  |  73.330%  |  32.119%  |     37.326%     |   30.824%    |  54.881%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   4    |  54.881%  |     -     |     -     |     54.881%     |   54.881%    |  54.881%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   5    |   10    |   2.154319   |     -     |   11.94  \n",
      "   5    |   20    |   2.131027   |     -     |   10.48  \n",
      "   5    |   30    |   2.066167   |     -     |   13.78  \n",
      "   5    |   40    |   2.116243   |     -     |   10.45  \n",
      "   5    |   50    |   2.108830   |     -     |   13.83  \n",
      "   5    |   52    |   2.162785   |     -     |   1.48   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   2.158572   |  2.126852  |   65.32  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   5    |  34.299%  |  74.488%  |  34.173%  |     42.581%     |   34.299%    |  57.976%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   5    |  57.976%  |     -     |     -     |     57.976%     |   57.976%    |  57.976%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   6    |   10    |   2.106620   |     -     |   11.44  \n",
      "   6    |   20    |   2.174537   |     -     |   10.45  \n",
      "   6    |   30    |   2.116538   |     -     |   13.84  \n",
      "   6    |   40    |   2.116236   |     -     |   10.33  \n",
      "   6    |   50    |   2.102011   |     -     |   13.70  \n",
      "   6    |   52    |   2.178905   |     -     |   1.51   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   2.165843   |  2.106784  |   64.67  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   6    |  36.555%  |  74.940%  |  35.338%  |     49.604%     |   36.555%    |  60.714%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   6    |  60.714%  |     -     |     -     |     60.714%     |   60.714%    |  60.714%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   7    |   10    |   2.106588   |     -     |   11.41  \n",
      "   7    |   20    |   2.113935   |     -     |   10.28  \n",
      "   7    |   30    |   2.114337   |     -     |   13.73  \n",
      "   7    |   40    |   2.100455   |     -     |   10.51  \n",
      "   7    |   50    |   2.100581   |     -     |   14.14  \n",
      "   7    |   52    |   2.088292   |     -     |   1.77   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   2.146964   |  2.111173  |   65.28  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   7    |  38.882%  |  74.833%  |  36.515%  |     49.005%     |   38.882%    |  62.500%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   7    |  62.500%  |     -     |     -     |     62.500%     |   62.500%    |  62.500%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   8    |   10    |   2.131249   |     -     |   11.29  \n",
      "   8    |   20    |   2.159007   |     -     |   10.23  \n",
      "   8    |   30    |   2.115954   |     -     |   13.59  \n",
      "   8    |   40    |   2.123244   |     -     |   11.03  \n",
      "   8    |   50    |   2.151858   |     -     |   14.53  \n",
      "   8    |   52    |   2.140333   |     -     |   1.46   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   2.177404   |  2.138069  |   65.45  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   8    |  39.534%  |  74.951%  |  37.318%  |     50.094%     |   39.534%    |  63.690%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   8    |  63.690%  |     -     |     -     |     63.690%     |   63.690%    |  63.690%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   9    |   10    |   2.189242   |     -     |   11.38  \n",
      "   9    |   20    |   2.148754   |     -     |   10.41  \n",
      "   9    |   30    |   2.191928   |     -     |   13.65  \n",
      "   9    |   40    |   2.196785   |     -     |   10.30  \n",
      "   9    |   50    |   2.161627   |     -     |   13.69  \n",
      "   9    |   52    |   2.235143   |     -     |   1.54   \n",
      "----------------------------------------------------------------------\n",
      "   9    |    -    |   2.221979   |  2.182949  |   64.27  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   9    |  39.910%  |  75.018%  |  37.795%  |     51.286%     |   39.910%    |  63.452%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   9    |  63.452%  |     -     |     -     |     63.452%     |   63.452%    |  63.452%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "  10    |   10    |   2.206040   |     -     |   11.39  \n",
      "  10    |   20    |   2.216779   |     -     |   10.22  \n",
      "  10    |   30    |   2.215378   |     -     |   13.69  \n",
      "  10    |   40    |   2.222494   |     -     |   10.47  \n",
      "  10    |   50    |   2.236079   |     -     |   13.95  \n",
      "  10    |   52    |   2.189321   |     -     |   1.79   \n",
      "Epoch    10: reducing learning rate of group 0 to 3.0000e-04.\n",
      "----------------------------------------------------------------------\n",
      "  10    |    -    |   2.260623   |  2.228678  |   64.91  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  10    |  37.816%  |  74.941%  |  38.041%  |     57.261%     |   37.816%    |  61.786%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  10    |  61.786%  |     -     |     -     |     61.786%     |   61.786%    |  61.786%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "  11    |   10    |   2.261721   |     -     |   11.46  \n",
      "  11    |   20    |   2.234500   |     -     |   10.36  \n",
      "  11    |   30    |   2.244730   |     -     |   14.89  \n",
      "  11    |   40    |   2.239761   |     -     |   10.44  \n",
      "  11    |   50    |   2.249633   |     -     |   13.57  \n",
      "  11    |   52    |   2.244860   |     -     |   1.52   \n",
      "----------------------------------------------------------------------\n",
      "  11    |    -    |   2.289517   |  2.261343  |   65.58  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  11    |  36.529%  |  74.670%  |  39.290%  |     52.159%     |   36.529%    |  60.357%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  11    |  60.357%  |     -     |     -     |     60.357%     |   60.357%    |  60.357%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "  12    |   10    |   2.255382   |     -     |   11.29  \n",
      "  12    |   20    |   2.293155   |     -     |   10.49  \n",
      "  12    |   30    |   2.287875   |     -     |   13.77  \n",
      "  12    |   40    |   2.284231   |     -     |   10.37  \n",
      "  12    |   50    |   2.293845   |     -     |   13.75  \n",
      "  12    |   52    |   2.340341   |     -     |   1.57   \n",
      "Epoch    12: reducing learning rate of group 0 to 9.0000e-05.\n",
      "----------------------------------------------------------------------\n",
      "  12    |    -    |   2.328480   |  2.310861  |   64.61  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  12    |  36.008%  |  74.358%  |  39.432%  |     58.309%     |   36.008%    |  60.000%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  12    |  60.000%  |     -     |     -     |     60.000%     |   60.000%    |  60.000%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "  13    |   10    |   2.334864   |     -     |   11.39  \n",
      "  13    |   20    |   2.334607   |     -     |   10.32  \n",
      "  13    |   30    |   2.351135   |     -     |   13.55  \n",
      "  13    |   40    |   2.304686   |     -     |   10.37  \n",
      "  13    |   50    |   2.350143   |     -     |   13.83  \n",
      "  13    |   52    |   2.324518   |     -     |   1.63   \n",
      "----------------------------------------------------------------------\n",
      "  13    |    -    |   2.379582   |  2.347734  |   64.83  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  13    |  35.932%  |  74.137%  |  39.013%  |     58.308%     |   35.932%    |  59.643%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "  13    |  59.643%  |     -     |     -     |     59.643%     |   59.643%    |  59.643%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "./checkpoint/textcnn_mean_teacher/k1 model cleaned\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   10    |   4.598233   |     -     |   11.30  \n",
      "   1    |   20    |   2.916772   |     -     |   10.52  \n",
      "   1    |   30    |   2.510343   |     -     |   13.65  \n",
      "   1    |   40    |   2.402770   |     -     |   10.50  \n",
      "   1    |   50    |   2.254355   |     -     |   13.70  \n",
      "   1    |   52    |   2.435470   |     -     |   1.49   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   3.005652   |  2.747124  |   64.55  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   1    |  15.857%  |  62.009%  |  21.729%  |     24.533%     |   15.857%    |  35.833%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   1    |  35.833%  |     -     |     -     |     35.833%     |   35.833%    |  35.833%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   2    |   10    |   2.522392   |     -     |   11.44  \n",
      "   2    |   20    |   2.417172   |     -     |   10.50  \n",
      "   2    |   30    |   2.376368   |     -     |   13.58  \n",
      "   2    |   40    |   2.307390   |     -     |   10.58  \n",
      "   2    |   50    |   2.258417   |     -     |   13.67  \n",
      "   2    |   52    |   2.191790   |     -     |   1.55   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   2.417757   |  2.546267  |   64.68  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   2    |  22.704%  |  67.578%  |  27.620%  |     33.511%     |   22.704%    |  44.524%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   2    |  44.524%  |     -     |     -     |     44.524%     |   44.524%    |  44.524%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   3    |   10    |   2.218072   |     -     |   11.52  \n",
      "   3    |   20    |   2.317200   |     -     |   10.84  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3    |   30    |   2.213475   |     -     |   14.48  \n",
      "   3    |   40    |   2.186375   |     -     |   12.45  \n",
      "   3    |   50    |   2.150411   |     -     |   17.99  \n",
      "   3    |   52    |   2.234103   |     -     |   2.08   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   2.260416   |  2.253907  |   73.43  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   3    |  31.791%  |  71.183%  |  32.356%  |     39.203%     |   31.791%    |  55.833%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   3    |  55.833%  |     -     |     -     |     55.833%     |   55.833%    |  55.833%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   4    |   10    |   2.183380   |     -     |   13.10  \n",
      "   4    |   20    |   2.121086   |     -     |   11.38  \n",
      "   4    |   30    |   2.119824   |     -     |   15.22  \n",
      "   4    |   40    |   2.109052   |     -     |   11.99  \n",
      "   4    |   50    |   2.071084   |     -     |   15.75  \n",
      "   4    |   52    |   2.153681   |     -     |   1.82   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   2.164135   |  2.059075  |   73.19  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   4    |  35.361%  |  72.814%  |  35.752%  |     38.402%     |   35.361%    |  60.238%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   4    |  60.238%  |     -     |     -     |     60.238%     |   60.238%    |  60.238%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   5    |   10    |   2.092766   |     -     |   12.67  \n",
      "   5    |   20    |   2.099843   |     -     |   11.26  \n",
      "   5    |   30    |   2.085676   |     -     |   14.93  \n",
      "   5    |   40    |   2.089187   |     -     |   11.64  \n",
      "   5    |   50    |   2.082258   |     -     |   15.45  \n",
      "   5    |   52    |   2.151660   |     -     |   1.69   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   2.132565   |  1.992719  |   71.46  \n",
      "\n",
      "\n",
      " Epoch  | Macro Acc | Macro AUC | Macro AP  | Macro Precision | Macro Recall | Macro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   5    |  35.427%  |  73.536%  |  37.274%  |     42.750%     |   35.427%    |  59.643%  \n",
      " Epoch  | Micro Acc | Micro AUC | Micro AP  | Micro Precision | Micro Recall | Micro F1 \n",
      "------------------------------------------------------------------------------------------\n",
      "   5    |  59.643%  |     -     |     -     |     59.643%     |   59.643%    |  59.643%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   6    |   10    |   2.031115   |     -     |   12.73  \n",
      "   6    |   20    |   2.094419   |     -     |   11.99  \n",
      "   6    |   30    |   2.107769   |     -     |   16.67  \n",
      "   6    |   40    |   2.075618   |     -     |   12.94  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-fb26fbfad3c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py36_tf23\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py36_tf23\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=24)\n",
    "for fold,(train_index, valid_index) in enumerate(kf.split(df.values, df[['label']].values)):\n",
    "    train, valid = df.iloc[train_index], df.iloc[valid_index]\n",
    "\n",
    "    # combine label and unlabel data\n",
    "    train_dataset = MixDataset(tp.max_seq_len, w2v, c2v, phraser, label2idx, \n",
    "                               train['name'].values.tolist(),train['description'].values.tolist() ,\n",
    "                               train['label'].values.tolist())\n",
    "\n",
    "    valid_dataset = MixDataset(tp.max_seq_len, w2v, c2v, phraser, label2idx, \n",
    "                               valid['name'].values, valid['description'].values, valid['label'].values)\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "\n",
    "    valid_sampler = SequentialSampler(valid_dataset)\n",
    "    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=tp.batch_size)\n",
    "    valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=int(tp.batch_size*2))\n",
    "    unlabel_sampler = RandomSampler(unlabel_dataset)\n",
    "    unlabel_dataset = MixDataset(tp.max_seq_len, w2v, c2v, phraser, label2idx,\n",
    "                             test['name'].values, test['description'].values, ['unlabel'] *test.shape[0])\n",
    "    unlabel_loader = DataLoader(unlabel_dataset, sampler=unlabel_sampler, batch_size=tp.batch_size)\n",
    "    \n",
    "    tp.update({'num_train_steps': len(train_loader)})\n",
    "    CKPT = './checkpoint/textcnn_mean_teacher/k{}'.format(fold)\n",
    "    saver = ModelSave(CKPT, continue_train=False)\n",
    "    saver.init()\n",
    "    \n",
    "    tb = SummaryWriter(CKPT)\n",
    "    es = EarlyStop(**tp.early_stop_params)\n",
    "\n",
    "    global_step = 0\n",
    "    model = TextcnnAugment(tp)\n",
    "    model_ema = create_ema_model( TextcnnAugment(tp))\n",
    "    mean_teacher = MeanTeacher(model, model_ema, tp, tb)\n",
    "    optimizer, scheduler = model.get_optimizer()\n",
    "    \n",
    "    for epoch_i in range(tp['epoch_size']):\n",
    "        if global_step==1:\n",
    "            print(model)\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10}  | {'Elapsed':^9}\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        model.train()\n",
    "        unlabel_iter = iter(unlabel_loader)\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            try:\n",
    "                unlabel_batch = next(unlabel_iter)\n",
    "            except:\n",
    "                unlabel_iter = iter(unlabel_loader)\n",
    "                unlabel_batch = next(unlabel_iter)\n",
    "            global_step +=1\n",
    "            batch_counts +=1\n",
    "\n",
    "            #Forward propogate\n",
    "            model.zero_grad()\n",
    "            feature = {k:v.to(device) for k,v in batch.items()}\n",
    "            #eature = {k:torch.cat([v,unlabel_batch[k]],dim=0).to(device) for k,v in batch.items()}\n",
    "            logits = model(feature)\n",
    "            loss = mean_teacher.compute_loss(feature, logits)\n",
    "            tb.add_scalar('loss/avg_loss', loss, global_step=global_step)\n",
    "            \n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            mean_teacher.step()\n",
    "            \n",
    "            # Log steps for train loss logging\n",
    "            if (step % tp.log_steps == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "            # Save steps for ckpt saving and dev evaluation\n",
    "            if (step % tp.save_steps == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "                val_metrics = multi_cls_metrics(model_ema, valid_loader, device)\n",
    "\n",
    "                for key, val in val_metrics.items():\n",
    "                    tb.add_scalar(f'metric/{key}', val, global_step=global_step)\n",
    "                avg_train_loss = total_loss / step\n",
    "                tb.add_scalars('loss/train_valid',{'train': avg_train_loss,\n",
    "                                                    'valid': val_metrics['val_loss']}, global_step=global_step)\n",
    "                \n",
    "                saver(total_loss / step, val_metrics['val_loss'], epoch_i, global_step, model_ema, optimizer, scheduler)\n",
    "        # On Epoch End: calcualte train & valid loss and log overall metrics\n",
    "        time_elapsed = time.time() - t0_epoch\n",
    "        val_metrics = multi_cls_metrics(model_ema, valid_loader, device)\n",
    "        avg_train_loss = total_loss / step\n",
    "        scheduler.step(val_metrics['f1_micro'])\n",
    "        print(\"-\"*70)\n",
    "        print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_metrics['val_loss']:^10.6f} | {time_elapsed:^9.2f}\")\n",
    "        multi_cls_log(epoch_i, val_metrics)\n",
    "        print(\"\\n\")\n",
    "        if es.check(val_metrics):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('embedding1.weight', Parameter containing:\n",
      "tensor([[-0.3901,  0.2356,  0.2300,  ..., -0.1065,  0.2393, -0.0179],\n",
      "        [-0.3489,  0.1862,  0.0886,  ..., -0.0276, -0.0638,  0.1627],\n",
      "        [-0.5828,  0.0614,  0.0103,  ..., -0.2199,  0.5395, -0.1665],\n",
      "        ...,\n",
      "        [ 0.7524, -0.9165,  0.8643,  ..., -1.2435,  0.9184, -0.1838],\n",
      "        [ 0.0174, -0.0267,  0.0235,  ..., -0.0126,  0.0152, -0.0129],\n",
      "        [-0.5238,  1.0917, -0.7439,  ...,  0.6906,  0.0059,  0.0552]],\n",
      "       requires_grad=True))\n",
      "('embedding2.weight', Parameter containing:\n",
      "tensor([[-0.1464,  0.1063,  0.1207,  ...,  0.0991,  0.3511,  0.1225],\n",
      "        [-0.1102,  0.1911,  0.1718,  ...,  0.1894, -0.2257,  0.1983],\n",
      "        [-0.1472,  0.4720, -0.2492,  ...,  0.2005,  0.2968, -0.0251],\n",
      "        ...,\n",
      "        [-1.5587, -0.5849, -0.0201,  ..., -0.2185, -0.8727, -1.3916],\n",
      "        [-0.0181, -0.0282,  0.0131,  ...,  0.0384,  0.0255,  0.0107],\n",
      "        [ 0.9340,  1.8907, -1.3842,  ..., -1.0192, -0.4836,  0.3365]],\n",
      "       requires_grad=True))\n",
      "('projector.0.weight', Parameter containing:\n",
      "tensor([[ 6.0422e-02,  5.8363e-02,  8.3660e-02,  ..., -6.3337e-02,\n",
      "          7.6621e-02, -2.3060e-02],\n",
      "        [ 6.4637e-02, -2.1144e-05,  4.6618e-02,  ...,  3.4927e-02,\n",
      "          1.4858e-02,  4.6883e-02],\n",
      "        [ 4.7127e-02, -8.1397e-02,  6.6868e-02,  ...,  2.1732e-02,\n",
      "          5.5013e-02, -7.1961e-02],\n",
      "        ...,\n",
      "        [ 8.6695e-02, -2.0184e-02,  4.8924e-02,  ...,  2.3190e-02,\n",
      "          2.8676e-02, -6.9682e-03],\n",
      "        [-4.3724e-02,  2.3515e-02,  5.6518e-02,  ..., -6.0876e-03,\n",
      "         -3.4586e-02,  3.9271e-02],\n",
      "        [ 3.1705e-02, -7.2280e-02,  1.1068e-02,  ...,  4.7898e-02,\n",
      "         -6.1708e-02,  1.3548e-02]], requires_grad=True))\n",
      "('projector.0.bias', Parameter containing:\n",
      "tensor([-0.0264,  0.0518, -0.0470, -0.0361, -0.0150,  0.0081,  0.0133, -0.0540,\n",
      "         0.0405,  0.0004, -0.0082, -0.0106,  0.0071, -0.0192, -0.0331, -0.0674,\n",
      "        -0.0327,  0.0123, -0.0075,  0.0905, -0.0173, -0.0231, -0.0435, -0.0175,\n",
      "         0.0467,  0.0320,  0.0090, -0.0681, -0.0248, -0.0321,  0.0483,  0.0775,\n",
      "        -0.0368,  0.0450,  0.0519,  0.0809, -0.0588, -0.0108, -0.0535,  0.0436,\n",
      "        -0.0434, -0.0462,  0.0161,  0.0661,  0.0013, -0.0227,  0.0691, -0.0242,\n",
      "        -0.0267,  0.0299, -0.0191,  0.0070,  0.0328,  0.0609,  0.0386,  0.0570,\n",
      "         0.0041, -0.0609, -0.0042,  0.0091, -0.0087,  0.0560, -0.0498, -0.0125,\n",
      "        -0.0065,  0.0492,  0.0632,  0.0544, -0.0489, -0.0562,  0.0454,  0.0073,\n",
      "        -0.0656, -0.0360, -0.0181,  0.0216,  0.0308, -0.0038, -0.0826, -0.0436,\n",
      "        -0.0374,  0.0106,  0.0487, -0.0784, -0.0225,  0.0190,  0.0438, -0.0757,\n",
      "         0.0710, -0.0390, -0.0171,  0.0600,  0.0289, -0.0191, -0.0014,  0.0418,\n",
      "        -0.0292,  0.0297, -0.0587,  0.0152], requires_grad=True))\n",
      "('projector.1.weight', Parameter containing:\n",
      "tensor([0.9627, 0.9495, 0.9704, 0.9503, 0.9766, 0.8865, 0.9624, 0.9696, 0.9579,\n",
      "        0.8358, 0.9724, 0.9773, 0.9625, 0.9772, 0.9796, 0.9576, 0.9252, 0.9774,\n",
      "        0.9251, 0.9845, 0.9933, 0.9639, 0.9725, 0.9749, 0.9702, 0.9701, 0.9628,\n",
      "        0.9583, 0.9740, 0.9768, 0.9763, 0.9456, 0.9746, 0.9891, 0.9806, 0.9645,\n",
      "        0.9630, 0.9792, 0.9787, 0.9686, 0.9757, 0.9701, 0.9619, 0.9701, 0.9820,\n",
      "        0.8631, 0.9555, 0.9951, 0.9821, 0.8722, 0.9798, 0.9779, 0.9678, 0.9742,\n",
      "        0.9835, 0.9652, 0.9911, 0.9859, 0.9783, 0.9730, 0.9797, 0.8608, 0.9826,\n",
      "        0.9825, 0.9729, 0.9614, 0.9772, 0.9227, 0.9759, 0.9277, 0.9908, 0.9831,\n",
      "        0.9657, 0.9601, 0.9689, 0.9664, 0.9676, 0.9719, 0.9898, 0.9629, 0.9551,\n",
      "        0.9944, 0.9788, 0.9775, 0.9715, 0.9262, 0.9619, 0.9711, 0.9687, 0.8171,\n",
      "        0.9831, 0.9748, 0.9707, 0.9640, 0.9286, 0.9706, 0.9606, 0.9636, 0.9731,\n",
      "        0.9651], requires_grad=True))\n",
      "('projector.1.bias', Parameter containing:\n",
      "tensor([-0.0215, -0.0358, -0.0155, -0.0188, -0.0213, -0.0854, -0.0340,  0.0163,\n",
      "        -0.0289, -0.1491, -0.0167, -0.0406,  0.0089, -0.0300, -0.0140, -0.0079,\n",
      "        -0.0773, -0.0143, -0.0695, -0.0091, -0.0078, -0.0219, -0.0122,  0.0175,\n",
      "        -0.0002,  0.0138, -0.0269, -0.0283, -0.0131, -0.0330, -0.0168, -0.0359,\n",
      "        -0.0126, -0.0211,  0.0007, -0.0039, -0.0136, -0.0182, -0.0264, -0.0223,\n",
      "        -0.0185, -0.0185, -0.0163, -0.0333,  0.0113, -0.1112, -0.0108, -0.0057,\n",
      "        -0.0021, -0.1189, -0.0168, -0.0179, -0.0399, -0.0273, -0.0058, -0.0229,\n",
      "         0.0182, -0.0134,  0.0081, -0.0221, -0.0371, -0.1271, -0.0333, -0.0106,\n",
      "        -0.0052, -0.0174, -0.0005, -0.0673, -0.0070, -0.0532, -0.0208, -0.0192,\n",
      "         0.0040, -0.0136,  0.0118, -0.0378, -0.0246, -0.0064, -0.0319, -0.0269,\n",
      "        -0.0353,  0.0230, -0.0122,  0.0022, -0.0209, -0.0559, -0.0345,  0.0048,\n",
      "        -0.0180, -0.1627,  0.0217, -0.0249, -0.0272, -0.0039, -0.0601,  0.0171,\n",
      "        -0.0177, -0.0370,  0.0130, -0.0480], requires_grad=True))\n",
      "('convs.0.0.weight', Parameter containing:\n",
      "tensor([[[-8.1800e-02, -9.1269e-02],\n",
      "         [-7.9786e-03,  2.6251e-02],\n",
      "         [-7.0329e-02,  7.9340e-03],\n",
      "         ...,\n",
      "         [-5.6357e-02,  2.1187e-02],\n",
      "         [-6.2027e-02, -6.1125e-02],\n",
      "         [-4.5864e-03, -4.8408e-02]],\n",
      "\n",
      "        [[ 4.1923e-02, -5.5492e-03],\n",
      "         [-5.0196e-02, -1.6499e-07],\n",
      "         [-6.8286e-03, -2.1673e-02],\n",
      "         ...,\n",
      "         [ 3.1363e-02, -4.5343e-03],\n",
      "         [-4.5585e-03,  6.8195e-02],\n",
      "         [-1.7001e-02, -5.8528e-02]],\n",
      "\n",
      "        [[-5.7813e-02, -7.3099e-03],\n",
      "         [-6.0137e-02, -1.0258e-01],\n",
      "         [ 6.1727e-02,  8.3164e-02],\n",
      "         ...,\n",
      "         [-6.8902e-02,  3.1300e-02],\n",
      "         [-6.5859e-02, -6.8770e-03],\n",
      "         [ 1.0850e-02, -3.1674e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.8625e-02,  7.8893e-02],\n",
      "         [ 5.4655e-02,  2.3284e-02],\n",
      "         [ 2.8520e-02, -2.6823e-02],\n",
      "         ...,\n",
      "         [ 3.7951e-02, -2.3598e-02],\n",
      "         [-7.7022e-02, -5.4134e-02],\n",
      "         [ 4.0810e-02, -5.2525e-02]],\n",
      "\n",
      "        [[-4.5567e-02,  3.0653e-02],\n",
      "         [ 3.2457e-02, -5.3069e-02],\n",
      "         [-3.0726e-03, -1.9033e-02],\n",
      "         ...,\n",
      "         [-8.5349e-02, -7.7974e-02],\n",
      "         [ 2.5663e-03,  1.6316e-02],\n",
      "         [-6.9930e-02,  5.6925e-02]],\n",
      "\n",
      "        [[-1.9331e-03,  5.9196e-02],\n",
      "         [-1.3815e-02, -7.8119e-02],\n",
      "         [-3.2585e-02, -5.4762e-02],\n",
      "         ...,\n",
      "         [-9.5361e-03, -2.4769e-02],\n",
      "         [ 5.8926e-02,  3.1799e-05],\n",
      "         [-3.3323e-02, -1.0026e-02]]], requires_grad=True))\n",
      "('convs.0.0.bias', Parameter containing:\n",
      "tensor([ 0.0397, -0.0416, -0.0179, -0.0356, -0.0619,  0.0223, -0.0129,  0.0312,\n",
      "        -0.0465, -0.0690,  0.0606, -0.0578, -0.0205, -0.0037, -0.0676, -0.0605,\n",
      "        -0.0231, -0.0691, -0.0392, -0.0824,  0.0376,  0.0203, -0.0545,  0.0117,\n",
      "        -0.0773,  0.0534, -0.0243, -0.0736, -0.0321,  0.0512,  0.0707, -0.0476,\n",
      "         0.0114,  0.0092,  0.0013, -0.0333,  0.0259, -0.0220, -0.0347,  0.0121,\n",
      "         0.0258, -0.0650, -0.0901, -0.0617, -0.0402, -0.0391,  0.0399, -0.0537,\n",
      "        -0.0717, -0.0858, -0.0746, -0.0661, -0.0086, -0.1114,  0.0274, -0.0871,\n",
      "        -0.0307, -0.0809, -0.0584, -0.0791, -0.0758, -0.0456, -0.0535, -0.0309,\n",
      "         0.0340, -0.0138, -0.0484, -0.0919, -0.0436, -0.0983],\n",
      "       requires_grad=True))\n",
      "('convs.1.0.weight', Parameter containing:\n",
      "tensor([[[-6.5659e-02, -1.6255e-02, -3.0297e-02],\n",
      "         [-2.8909e-02,  6.5441e-02, -2.8353e-03],\n",
      "         [ 8.1739e-03,  6.1074e-03, -7.5320e-02],\n",
      "         ...,\n",
      "         [ 1.8510e-02,  5.5077e-02,  3.7953e-03],\n",
      "         [-2.0064e-03, -3.6728e-02, -1.9484e-02],\n",
      "         [-4.1662e-02, -9.3692e-03, -4.0025e-02]],\n",
      "\n",
      "        [[-3.7969e-02,  3.3952e-02,  5.5280e-02],\n",
      "         [ 1.7694e-02, -4.4126e-02, -3.9362e-02],\n",
      "         [ 2.0924e-02, -3.7555e-02,  2.0910e-02],\n",
      "         ...,\n",
      "         [ 5.8092e-02,  7.5468e-03, -8.1070e-03],\n",
      "         [ 5.5341e-02,  3.8413e-02,  2.3637e-03],\n",
      "         [-4.3630e-02,  5.1105e-02,  4.2470e-03]],\n",
      "\n",
      "        [[-1.9909e-02, -2.6181e-02, -1.9533e-02],\n",
      "         [ 2.1112e-02, -1.0544e-02,  4.4863e-02],\n",
      "         [-6.9522e-02, -4.6329e-02, -3.7042e-02],\n",
      "         ...,\n",
      "         [-2.4779e-02, -5.8645e-02, -2.3399e-02],\n",
      "         [ 3.5895e-02,  3.3074e-02, -3.7108e-02],\n",
      "         [-1.6735e-02,  1.2708e-02,  1.5621e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.4543e-02, -8.9595e-03,  1.9398e-02],\n",
      "         [ 2.4305e-02,  3.2445e-02,  3.5969e-02],\n",
      "         [ 3.6019e-02, -5.5473e-02,  1.4259e-03],\n",
      "         ...,\n",
      "         [-6.2461e-03,  5.6558e-03, -6.4772e-02],\n",
      "         [-5.2757e-02, -5.8549e-02, -1.1906e-02],\n",
      "         [ 3.3424e-03, -3.9721e-02, -5.9075e-05]],\n",
      "\n",
      "        [[ 2.1412e-02, -4.0994e-02, -7.2164e-02],\n",
      "         [-2.1045e-02, -2.5416e-02,  1.8360e-02],\n",
      "         [-6.3877e-02, -6.0011e-02, -2.1278e-02],\n",
      "         ...,\n",
      "         [-2.3210e-02, -4.0710e-02,  2.0083e-02],\n",
      "         [ 5.6867e-02, -3.1353e-02, -5.8162e-02],\n",
      "         [ 6.0495e-02,  1.5427e-02, -2.6119e-02]],\n",
      "\n",
      "        [[ 2.4579e-02, -7.3182e-02, -2.2353e-03],\n",
      "         [-1.9783e-02,  5.9000e-03,  4.8822e-02],\n",
      "         [ 3.6703e-02,  4.7584e-02, -2.1273e-02],\n",
      "         ...,\n",
      "         [-1.6231e-02,  2.2686e-02,  3.7649e-02],\n",
      "         [-3.6530e-02, -2.4955e-03,  4.2188e-02],\n",
      "         [ 2.9921e-02, -1.7315e-02, -3.3380e-02]]], requires_grad=True))\n",
      "('convs.1.0.bias', Parameter containing:\n",
      "tensor([ 0.0394,  0.0541, -0.0995, -0.0898, -0.0372,  0.0002, -0.0156, -0.0667,\n",
      "        -0.0042,  0.0182,  0.0278, -0.0395, -0.0085, -0.0488,  0.0102, -0.0590,\n",
      "        -0.0742, -0.0359, -0.0468, -0.0293, -0.0122, -0.0363, -0.0611,  0.0202,\n",
      "        -0.0384, -0.0161,  0.0189, -0.0302, -0.0364, -0.0134,  0.0306,  0.0040,\n",
      "        -0.0639,  0.0191,  0.0138,  0.0249, -0.0480,  0.0256,  0.0192, -0.0425,\n",
      "        -0.0074, -0.0335, -0.0196, -0.0137, -0.0489, -0.0040, -0.0668, -0.0249,\n",
      "         0.0078, -0.0462,  0.0345,  0.0259, -0.0010, -0.0859, -0.0668, -0.0507,\n",
      "        -0.0037, -0.0078, -0.0567,  0.0179, -0.0145,  0.0177, -0.0616, -0.0293,\n",
      "        -0.0704, -0.0446, -0.0877,  0.0246, -0.0334, -0.0509],\n",
      "       requires_grad=True))\n",
      "('convs.2.0.weight', Parameter containing:\n",
      "tensor([[[ 9.4447e-03,  2.8034e-02, -4.0426e-02, -1.4562e-02],\n",
      "         [-1.4184e-02,  5.2791e-02,  6.1229e-02, -1.9207e-02],\n",
      "         [ 1.0496e-02, -2.3770e-02,  2.6606e-02, -3.5290e-02],\n",
      "         ...,\n",
      "         [ 7.7837e-02, -3.2927e-02,  9.2001e-03,  5.2507e-02],\n",
      "         [-2.7689e-02,  1.9734e-02, -5.1882e-02,  2.5850e-02],\n",
      "         [-6.1930e-03, -1.0730e-02, -6.1517e-02, -5.0075e-02]],\n",
      "\n",
      "        [[-7.0647e-02, -1.8365e-02,  4.8753e-03, -2.1148e-02],\n",
      "         [ 3.5220e-02, -1.6439e-04, -3.6472e-02, -7.6028e-03],\n",
      "         [-3.1323e-03,  2.1922e-02, -1.1243e-02,  1.1031e-02],\n",
      "         ...,\n",
      "         [-6.5921e-02, -3.2346e-02,  7.5760e-03, -1.8214e-02],\n",
      "         [ 1.9266e-02, -1.6818e-02,  2.0307e-02,  4.4299e-02],\n",
      "         [-1.1440e-02, -1.6778e-02,  2.0522e-02, -3.9475e-02]],\n",
      "\n",
      "        [[ 4.5840e-03,  2.8542e-02, -8.1011e-03, -2.0305e-02],\n",
      "         [ 3.4525e-02,  5.7026e-02,  4.3218e-03, -1.3365e-02],\n",
      "         [ 5.8432e-02,  3.1462e-02, -4.8520e-02,  1.4182e-02],\n",
      "         ...,\n",
      "         [ 3.3536e-02, -2.0496e-05, -3.4877e-02, -3.4976e-02],\n",
      "         [ 3.1997e-02, -3.0784e-02,  6.3557e-02,  3.8649e-02],\n",
      "         [-1.3717e-02, -2.1099e-02,  1.6948e-02, -6.2455e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.3818e-03,  1.0484e-02, -2.9029e-02,  2.5040e-02],\n",
      "         [-4.0714e-03,  5.6564e-03, -1.6184e-02, -2.4350e-02],\n",
      "         [ 2.3659e-02, -2.4422e-02, -4.1277e-02, -1.4456e-02],\n",
      "         ...,\n",
      "         [-2.0778e-02,  2.0809e-02, -1.5644e-02, -3.9552e-02],\n",
      "         [ 3.4994e-02, -1.3130e-02, -3.7376e-02, -4.5884e-02],\n",
      "         [ 4.7450e-02,  3.6864e-03,  3.7156e-02,  2.9417e-02]],\n",
      "\n",
      "        [[ 1.6298e-02,  1.3713e-02,  8.9293e-03, -1.8306e-02],\n",
      "         [-2.0713e-02,  1.9047e-03, -3.0108e-02, -4.4023e-02],\n",
      "         [ 1.1287e-02,  2.8456e-02,  2.5503e-02, -3.5368e-02],\n",
      "         ...,\n",
      "         [-3.6494e-02, -2.4886e-03,  6.3209e-02, -6.1298e-02],\n",
      "         [ 3.6356e-03,  1.6752e-02,  3.1343e-02, -2.2474e-02],\n",
      "         [-3.7825e-02,  5.5044e-03, -2.8297e-02,  2.6427e-02]],\n",
      "\n",
      "        [[-4.1744e-02, -9.8308e-03,  1.0132e-02,  2.2877e-02],\n",
      "         [-2.2266e-02, -5.4038e-02,  1.5237e-02, -2.3218e-02],\n",
      "         [-2.5099e-02,  7.1622e-03,  2.0923e-02,  5.4768e-02],\n",
      "         ...,\n",
      "         [-1.8931e-02,  4.8017e-03, -1.7505e-03, -2.3403e-02],\n",
      "         [ 6.0806e-03,  4.0914e-02, -2.8872e-02, -4.2891e-02],\n",
      "         [-1.4267e-02, -2.9519e-02, -9.3742e-03, -7.7701e-03]]],\n",
      "       requires_grad=True))\n",
      "('convs.2.0.bias', Parameter containing:\n",
      "tensor([-0.0082, -0.0022, -0.0400, -0.0185,  0.0114, -0.0271, -0.0390,  0.0229,\n",
      "        -0.0182, -0.0245,  0.0144,  0.0241, -0.0470, -0.0303, -0.0310, -0.0288,\n",
      "        -0.0404, -0.0345, -0.0118,  0.0010, -0.0057,  0.0295, -0.0356,  0.0124,\n",
      "        -0.0522, -0.0208, -0.0163, -0.0137, -0.0371,  0.0307, -0.0162, -0.0199,\n",
      "        -0.0239,  0.0215, -0.0647, -0.0321, -0.0110, -0.0011,  0.0120, -0.0008,\n",
      "        -0.0012, -0.0349,  0.0118, -0.0509,  0.0050, -0.0414,  0.0114, -0.0120,\n",
      "        -0.0039, -0.0229, -0.0422, -0.0366, -0.0243,  0.0023, -0.0368, -0.0321,\n",
      "        -0.0526, -0.0696,  0.0062, -0.0192, -0.0041, -0.0072, -0.0571, -0.0209,\n",
      "        -0.0249,  0.0363,  0.0317, -0.0479,  0.0205, -0.0063],\n",
      "       requires_grad=True))\n",
      "('convs.3.0.weight', Parameter containing:\n",
      "tensor([[[ 2.0831e-02, -2.8794e-02,  5.9843e-02, -4.6938e-02, -6.0959e-03],\n",
      "         [-5.6528e-03,  6.3493e-03,  1.4857e-02,  1.7953e-02, -3.8757e-02],\n",
      "         [ 2.0210e-03,  3.8619e-02,  6.2912e-02,  5.1461e-02,  1.6180e-02],\n",
      "         ...,\n",
      "         [-2.4957e-02, -2.9921e-02,  2.7545e-03,  1.8421e-02, -3.7114e-02],\n",
      "         [-4.1542e-02, -3.3005e-02, -3.8535e-02,  2.4950e-02,  2.8288e-02],\n",
      "         [ 5.2916e-03, -9.7885e-03,  4.2560e-02,  2.9431e-02, -1.2725e-02]],\n",
      "\n",
      "        [[-9.3761e-03, -4.8489e-02, -4.2206e-02, -1.0965e-03, -1.5283e-02],\n",
      "         [-6.6182e-02, -5.2771e-03, -3.1297e-02, -2.6820e-02, -4.5360e-02],\n",
      "         [ 1.4724e-02, -2.2429e-02, -1.9910e-02,  3.3654e-02,  1.9513e-02],\n",
      "         ...,\n",
      "         [ 5.2277e-03, -2.5281e-02, -8.0032e-03, -2.4854e-02,  3.9965e-02],\n",
      "         [ 2.8749e-02,  1.4635e-02, -1.9774e-02, -1.2071e-03,  4.6490e-03],\n",
      "         [-1.3664e-02,  2.4133e-02, -4.4011e-02,  7.2388e-05, -9.3585e-03]],\n",
      "\n",
      "        [[-1.3972e-02, -5.7144e-03,  1.8097e-02,  4.2427e-02, -3.2304e-02],\n",
      "         [ 1.1523e-02, -4.1821e-02, -3.3754e-02,  1.2931e-02, -1.6716e-02],\n",
      "         [ 2.8565e-02, -2.7136e-02,  8.7103e-03, -4.7218e-02, -1.3648e-02],\n",
      "         ...,\n",
      "         [ 3.1597e-02, -2.5856e-02,  2.9112e-02, -3.8072e-02,  1.4567e-02],\n",
      "         [-3.3798e-02, -1.4825e-02,  3.4843e-02, -5.1690e-03,  4.1640e-02],\n",
      "         [ 2.5068e-02, -1.5822e-02,  3.8649e-03, -2.1643e-04, -1.4187e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.3788e-02, -2.8076e-02, -1.3642e-02,  4.5549e-04, -4.0287e-02],\n",
      "         [ 6.8764e-03, -2.2500e-02,  4.3381e-02, -4.3574e-03, -3.2401e-02],\n",
      "         [-1.2952e-02, -2.1507e-02, -1.2629e-02,  1.1894e-02, -5.5116e-02],\n",
      "         ...,\n",
      "         [ 3.0496e-02,  2.6798e-02, -4.4854e-03,  1.4862e-03,  4.0416e-02],\n",
      "         [ 3.2368e-02, -5.5998e-02, -1.4054e-02,  1.1602e-02,  3.1880e-02],\n",
      "         [-4.4128e-02, -1.3107e-03, -1.5131e-02,  2.4241e-02, -8.7730e-04]],\n",
      "\n",
      "        [[-1.8403e-02,  2.3280e-02,  8.0001e-03, -6.0746e-03, -2.2983e-02],\n",
      "         [ 1.2702e-02, -8.7288e-03, -3.0370e-02,  5.7940e-03,  8.8666e-03],\n",
      "         [-7.3979e-03, -2.7306e-02, -1.4677e-02,  2.0534e-02, -3.1870e-02],\n",
      "         ...,\n",
      "         [ 1.9170e-02, -1.2861e-02, -4.3981e-02, -2.6632e-02,  1.5276e-02],\n",
      "         [ 2.2048e-02, -3.1942e-02, -4.8133e-02, -3.6963e-02, -2.7242e-02],\n",
      "         [-4.6857e-03, -6.2626e-02, -2.7513e-02,  2.4264e-02,  5.0518e-03]],\n",
      "\n",
      "        [[-2.4573e-03, -2.6102e-02,  5.0433e-02, -3.6796e-02,  1.4137e-02],\n",
      "         [-1.5379e-02, -3.4649e-02,  3.2566e-02, -7.2282e-03, -1.4740e-02],\n",
      "         [-7.7385e-03, -2.2809e-02,  1.8740e-02,  1.3785e-02, -2.0896e-02],\n",
      "         ...,\n",
      "         [-3.2969e-02,  2.6952e-02, -2.6426e-02, -2.6398e-02, -2.5998e-02],\n",
      "         [-2.7410e-03, -3.0532e-02, -2.1801e-02,  1.3040e-02, -5.0090e-02],\n",
      "         [-3.3950e-02,  3.3272e-02, -1.5972e-02, -9.8374e-03, -1.5254e-02]]],\n",
      "       requires_grad=True))\n",
      "('convs.3.0.bias', Parameter containing:\n",
      "tensor([-0.0339,  0.0011, -0.0580,  0.0062, -0.0591, -0.0238, -0.0522, -0.0151,\n",
      "        -0.0322,  0.0049, -0.0370, -0.0090, -0.0272, -0.0410, -0.0607,  0.0189,\n",
      "        -0.0008, -0.0062,  0.0162, -0.0183, -0.0875, -0.0376, -0.0426, -0.0361,\n",
      "        -0.0211, -0.0359, -0.0104,  0.0231,  0.0108, -0.0132, -0.0676, -0.0046,\n",
      "        -0.0034, -0.0599, -0.0071, -0.0083,  0.0028,  0.0207, -0.0508, -0.0237,\n",
      "         0.0237, -0.0072, -0.0761, -0.0084, -0.0148, -0.0128, -0.0413, -0.0294,\n",
      "        -0.0033,  0.0096, -0.0043, -0.0140, -0.0286, -0.0621, -0.0537, -0.0468,\n",
      "        -0.0261,  0.0321, -0.0228, -0.0478, -0.0232,  0.0418,  0.0216, -0.0142,\n",
      "         0.0265, -0.0320,  0.0303,  0.0117, -0.0284, -0.0559],\n",
      "       requires_grad=True))\n",
      "('fc.weight', Parameter containing:\n",
      "tensor([[-0.0070, -0.1149,  0.0136,  ...,  0.0295,  0.0115, -0.1368],\n",
      "        [-0.0411, -0.0393,  0.0062,  ..., -0.0545, -0.0121,  0.0026],\n",
      "        [ 0.0314, -0.0376, -0.0362,  ..., -0.0264,  0.0065, -0.0111],\n",
      "        ...,\n",
      "        [ 0.0220,  0.0114,  0.0262,  ..., -0.0246,  0.0155, -0.0274],\n",
      "        [ 0.0004,  0.0422,  0.0575,  ..., -0.0117,  0.0041, -0.0077],\n",
      "        [-0.0040, -0.0391, -0.0332,  ...,  0.0024, -0.0746, -0.0910]],\n",
      "       requires_grad=True))\n",
      "('fc.bias', Parameter containing:\n",
      "tensor([-0.1462, -0.1331, -0.1426, -0.0575, -0.0158, -0.1040, -0.0493,  0.0078,\n",
      "         0.0046,  0.0203, -0.0711,  0.0633, -0.0843, -0.0208,  0.0552,  0.0577,\n",
      "         0.0461,  0.1065, -0.0052, -0.1935], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for i in model.named_parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## K-Fold Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T13:31:28.634820Z",
     "iopub.status.busy": "2022-08-05T13:31:28.634312Z",
     "iopub.status.idle": "2022-08-05T13:31:41.406697Z",
     "shell.execute_reply": "2022-08-05T13:31:41.405987Z",
     "shell.execute_reply.started": "2022-08-05T13:31:28.634794Z"
    }
   },
   "outputs": [],
   "source": [
    "test = test_process()\n",
    "test_dataset = MixDataset(tp.max_seq_len, w2v, c2v, phraser, label2idx,  test['name'].values, test['description'].values)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_loader = DataLoader(test_dataset, sampler=test_sampler, batch_size=int(tp.batch_size*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result = kfold_inference(test_loader, tp, Textcnn, './checkpoint/textcnn_mean_teacher', 5, device)\n",
    "result['pred'] = result['pred_avg']\n",
    "result_process(result, label2idx, './submit/textcnn_mean_teacher_5fold_avg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_tf23",
   "language": "python",
   "name": "py36_tf23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
