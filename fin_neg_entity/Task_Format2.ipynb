{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T05:19:44.608757Z",
     "iopub.status.busy": "2022-09-09T05:19:44.608003Z",
     "iopub.status.idle": "2022-09-09T05:19:53.548869Z",
     "shell.execute_reply": "2022-09-09T05:19:53.547789Z",
     "shell.execute_reply.started": "2022-09-09T05:19:44.608711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import collections \n",
    "from itertools import chain\n",
    "import torch \n",
    "import time \n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from src.train_utils import set_seed, ModelSave, get_torch_device, EarlyStop, TrainParams\n",
    "from src.evaluation import binary_cls_report, classification_inference\n",
    "from src.metric import  binary_cls_metrics, binary_cls_log\n",
    "\n",
    "from models import BertClassifier\n",
    "from dataset import SeqPairDataset, data_loader\n",
    "from evaluation import overall_f1\n",
    "import transformers \n",
    "transformers.logging.set_verbosity_error()\n",
    "from transformers import BertTokenizer,AdamW, get_linear_schedule_with_warmup\n",
    "device = get_torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T05:19:53.551564Z",
     "iopub.status.busy": "2022-09-09T05:19:53.550346Z",
     "iopub.status.idle": "2022-09-09T05:19:53.559288Z",
     "shell.execute_reply": "2022-09-09T05:19:53.557991Z",
     "shell.execute_reply.started": "2022-09-09T05:19:53.551521Z"
    }
   },
   "outputs": [],
   "source": [
    "tp = TrainParams(\n",
    "    log_steps = 10,\n",
    "    save_steps = 100,\n",
    "    epoch_size=20,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    max_seq_len=512,\n",
    "    batch_size=20,\n",
    "    lr=5e-6,\n",
    "    weight_decay=0.0,\n",
    "    epsilon=1e-6,\n",
    "    warmup_steps=100,\n",
    "    dropout_rate=0.5,\n",
    "    label_size=2,\n",
    "    gradient_clip=1.0,\n",
    "    early_stop_params = {\n",
    "        'monitor':'f1',\n",
    "        'mode':'max',\n",
    "        'min_delta': 0,\n",
    "        'patience':3,\n",
    "        'verbose':False\n",
    "    },\n",
    "    pretrain_model = 'hfl/chinese-roberta-wwm-ext',\n",
    "    continue_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(tp.pretrain_model, do_lower_case=True)\n",
    "special_tokens_dict = {'additional_special_tokens':['[t]','[c]','[o]','[e]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print('We have added', num_added_toks, 'tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T05:19:57.386191Z",
     "iopub.status.busy": "2022-09-09T05:19:57.385463Z",
     "iopub.status.idle": "2022-09-09T05:20:23.699463Z",
     "shell.execute_reply": "2022-09-09T05:20:23.698275Z",
     "shell.execute_reply.started": "2022-09-09T05:19:57.386130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'log_steps': 10, 'save_steps': 100, 'epoch_size': 20, 'loss_fn': CrossEntropyLoss(), 'max_seq_len': 512, 'batch_size': 20, 'lr': 5e-06, 'weight_decay': 0.0, 'epsilon': 1e-06, 'warmup_steps': 100, 'dropout_rate': 0.5, 'label_size': 2, 'gradient_clip': 1.0, 'early_stop_params': {'monitor': 'f1', 'mode': 'max', 'min_delta': 0, 'patience': 3, 'verbose': False}, 'pretrain_model': 'hfl/chinese-roberta-wwm-ext', 'continue_train': False, 'num_train_steps': 7460}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SeqPairDataset(data_loader('/kaggle/input/finneg/trainsample/trainsample/train2.txt'), tp.max_seq_len, tokenizer)\n",
    "valid_dataset = SeqPairDataset(data_loader('/kaggle/input/finneg/trainsample/trainsample/valid2.txt'), tp.max_seq_len, tokenizer)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=tp.batch_size)\n",
    "valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=tp.batch_size)\n",
    "\n",
    "tp.update({'num_train_steps': int(len(train_loader)*tp.epoch_size)})\n",
    "print(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T05:20:23.701490Z",
     "iopub.status.busy": "2022-09-09T05:20:23.700770Z",
     "iopub.status.idle": "2022-09-09T05:20:48.775964Z",
     "shell.execute_reply": "2022-09-09T05:20:48.774944Z",
     "shell.execute_reply.started": "2022-09-09T05:20:23.701451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: [Errno 2] No such file or directory: './checkpoint/single_task_bert2' not exists\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a72f271c5f4a118242256a2677f557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/393M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CKPT = './checkpoint/single_task_bert2'\n",
    "saver = ModelSave(CKPT, continue_train=False)\n",
    "saver.init()\n",
    "es = EarlyStop(**tp.early_stop_params)\n",
    "global_step = 0\n",
    "tb = SummaryWriter(CKPT)\n",
    "\n",
    "model = BertClassifier(tp)\n",
    "model.bert.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(device)\n",
    "optimizer, scheduler = model.get_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T05:20:48.777746Z",
     "iopub.status.busy": "2022-09-09T05:20:48.777386Z",
     "iopub.status.idle": "2022-09-09T06:59:30.768918Z",
     "shell.execute_reply": "2022-09-09T06:59:30.767846Z",
     "shell.execute_reply.started": "2022-09-09T05:20:48.777711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   10    |   0.722304   |     -     |   12.19  \n",
      "   1    |   20    |   0.813473   |     -     |   10.14  \n",
      "   1    |   30    |   0.749007   |     -     |   10.14  \n",
      "   1    |   40    |   0.727100   |     -     |   10.22  \n",
      "   1    |   50    |   0.738717   |     -     |   10.13  \n",
      "   1    |   60    |   0.693971   |     -     |   10.14  \n",
      "   1    |   70    |   0.675772   |     -     |   10.12  \n",
      "   1    |   80    |   0.661169   |     -     |   10.16  \n",
      "   1    |   90    |   0.585707   |     -     |   10.14  \n",
      "   1    |   100   |   0.589076   |     -     |   10.17  \n",
      "   1    |   110   |   0.417993   |     -     |   46.10  \n",
      "   1    |   120   |   0.436615   |     -     |   9.79   \n",
      "   1    |   130   |   0.321123   |     -     |   9.80   \n",
      "   1    |   140   |   0.331407   |     -     |   9.83   \n",
      "   1    |   150   |   0.359760   |     -     |   9.77   \n",
      "   1    |   160   |   0.308194   |     -     |   9.80   \n",
      "   1    |   170   |   0.301190   |     -     |   9.76   \n",
      "   1    |   180   |   0.289500   |     -     |   9.79   \n",
      "   1    |   190   |   0.307799   |     -     |   9.81   \n",
      "   1    |   200   |   0.325813   |     -     |   9.76   \n",
      "   1    |   210   |   0.254298   |     -     |   52.60  \n",
      "   1    |   220   |   0.325168   |     -     |   9.81   \n",
      "   1    |   230   |   0.288511   |     -     |   9.77   \n",
      "   1    |   240   |   0.228827   |     -     |   9.82   \n",
      "   1    |   250   |   0.292078   |     -     |   9.78   \n",
      "   1    |   260   |   0.275965   |     -     |   9.79   \n",
      "   1    |   270   |   0.296742   |     -     |   9.84   \n",
      "   1    |   280   |   0.237626   |     -     |   9.80   \n",
      "   1    |   290   |   0.238919   |     -     |   9.76   \n",
      "   1    |   300   |   0.188448   |     -     |   9.80   \n",
      "   1    |   310   |   0.296604   |     -     |   52.16  \n",
      "   1    |   320   |   0.259260   |     -     |   9.80   \n",
      "   1    |   330   |   0.211564   |     -     |   9.79   \n",
      "   1    |   340   |   0.201408   |     -     |   9.86   \n",
      "   1    |   350   |   0.174012   |     -     |   9.81   \n",
      "   1    |   360   |   0.258654   |     -     |   9.79   \n",
      "   1    |   370   |   0.155865   |     -     |   9.79   \n",
      "   1    |   372   |   0.199047   |     -     |   1.96   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.393862   |  0.220629  |  534.14  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   1    |  90.402%  |  97.388%  |  97.397%  |  90.402%  |  90.402%  |  90.402%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   2    |   10    |   0.241597   |     -     |   11.14  \n",
      "   2    |   20    |   0.166513   |     -     |   10.13  \n",
      "   2    |   30    |   0.233416   |     -     |   10.14  \n",
      "   2    |   40    |   0.134706   |     -     |   10.16  \n",
      "   2    |   50    |   0.178317   |     -     |   10.13  \n",
      "   2    |   60    |   0.203798   |     -     |   10.13  \n",
      "   2    |   70    |   0.184697   |     -     |   10.15  \n",
      "   2    |   80    |   0.232467   |     -     |   10.14  \n",
      "   2    |   90    |   0.258914   |     -     |   10.14  \n",
      "   2    |   100   |   0.191956   |     -     |   10.14  \n",
      "   2    |   110   |   0.178051   |     -     |   53.39  \n",
      "   2    |   120   |   0.108456   |     -     |   9.84   \n",
      "   2    |   130   |   0.156744   |     -     |   9.80   \n",
      "   2    |   140   |   0.104732   |     -     |   9.81   \n",
      "   2    |   150   |   0.121044   |     -     |   9.80   \n",
      "   2    |   160   |   0.123072   |     -     |   9.78   \n",
      "   2    |   170   |   0.186543   |     -     |   9.76   \n",
      "   2    |   180   |   0.197921   |     -     |   9.77   \n",
      "   2    |   190   |   0.231674   |     -     |   9.81   \n",
      "   2    |   200   |   0.185168   |     -     |   9.78   \n",
      "   2    |   210   |   0.176668   |     -     |   43.03  \n",
      "   2    |   220   |   0.136057   |     -     |   9.91   \n",
      "   2    |   230   |   0.128080   |     -     |   9.79   \n",
      "   2    |   240   |   0.116767   |     -     |   9.82   \n",
      "   2    |   250   |   0.200930   |     -     |   9.80   \n",
      "   2    |   260   |   0.138525   |     -     |   9.78   \n",
      "   2    |   270   |   0.127320   |     -     |   9.80   \n",
      "   2    |   280   |   0.150766   |     -     |   9.87   \n",
      "   2    |   290   |   0.087639   |     -     |   9.75   \n",
      "   2    |   300   |   0.176163   |     -     |   9.82   \n",
      "   2    |   310   |   0.154246   |     -     |   43.03  \n",
      "   2    |   320   |   0.111644   |     -     |   9.83   \n",
      "   2    |   330   |   0.233752   |     -     |   9.82   \n",
      "   2    |   340   |   0.169582   |     -     |   9.79   \n",
      "   2    |   350   |   0.168948   |     -     |   9.80   \n",
      "   2    |   360   |   0.139529   |     -     |   9.78   \n",
      "   2    |   370   |   0.147961   |     -     |   9.81   \n",
      "   2    |   372   |   0.234140   |     -     |   1.95   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.168155   |  0.229903  |  512.34  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   2    |  91.903%  |  98.061%  |  98.047%  |  91.903%  |  91.903%  |  91.903%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   3    |   10    |   0.212714   |     -     |   11.16  \n",
      "   3    |   20    |   0.120880   |     -     |   10.15  \n",
      "   3    |   30    |   0.109774   |     -     |   10.20  \n",
      "   3    |   40    |   0.096921   |     -     |   10.13  \n",
      "   3    |   50    |   0.050115   |     -     |   10.16  \n",
      "   3    |   60    |   0.082893   |     -     |   10.16  \n",
      "   3    |   70    |   0.224842   |     -     |   10.15  \n",
      "   3    |   80    |   0.094431   |     -     |   10.13  \n",
      "   3    |   90    |   0.138691   |     -     |   10.20  \n",
      "   3    |   100   |   0.074187   |     -     |   10.13  \n",
      "   3    |   110   |   0.049676   |     -     |   43.51  \n",
      "   3    |   120   |   0.207811   |     -     |   9.88   \n",
      "   3    |   130   |   0.082453   |     -     |   9.79   \n",
      "   3    |   140   |   0.170607   |     -     |   9.81   \n",
      "   3    |   150   |   0.127818   |     -     |   9.82   \n",
      "   3    |   160   |   0.072605   |     -     |   9.79   \n",
      "   3    |   170   |   0.067722   |     -     |   9.79   \n",
      "   3    |   180   |   0.066579   |     -     |   9.82   \n",
      "   3    |   190   |   0.041811   |     -     |   9.80   \n",
      "   3    |   200   |   0.020402   |     -     |   9.81   \n",
      "   3    |   210   |   0.156261   |     -     |   43.58  \n",
      "   3    |   220   |   0.143362   |     -     |   9.82   \n",
      "   3    |   230   |   0.109063   |     -     |   9.81   \n",
      "   3    |   240   |   0.096605   |     -     |   9.86   \n",
      "   3    |   250   |   0.090065   |     -     |   9.82   \n",
      "   3    |   260   |   0.201499   |     -     |   9.79   \n",
      "   3    |   270   |   0.159854   |     -     |   9.83   \n",
      "   3    |   280   |   0.101834   |     -     |   9.81   \n",
      "   3    |   290   |   0.036453   |     -     |   9.81   \n",
      "   3    |   300   |   0.052175   |     -     |   9.82   \n",
      "   3    |   310   |   0.150311   |     -     |   44.02  \n",
      "   3    |   320   |   0.082044   |     -     |   9.82   \n",
      "   3    |   330   |   0.157850   |     -     |   9.78   \n",
      "   3    |   340   |   0.036313   |     -     |   9.80   \n",
      "   3    |   350   |   0.108486   |     -     |   9.81   \n",
      "   3    |   360   |   0.106984   |     -     |   9.81   \n",
      "   3    |   370   |   0.098787   |     -     |   9.86   \n",
      "   3    |   372   |   0.423231   |     -     |   1.96   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.110398   |  0.229486  |  504.49  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   3    |  93.405%  |  98.250%  |  98.242%  |  93.405%  |  93.405%  |  93.405%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   4    |   10    |   0.056596   |     -     |   11.16  \n",
      "   4    |   20    |   0.075539   |     -     |   10.18  \n",
      "   4    |   30    |   0.069293   |     -     |   10.16  \n",
      "   4    |   40    |   0.084238   |     -     |   10.15  \n",
      "   4    |   50    |   0.094302   |     -     |   10.17  \n",
      "   4    |   60    |   0.054475   |     -     |   10.16  \n",
      "   4    |   70    |   0.079212   |     -     |   10.15  \n",
      "   4    |   80    |   0.047360   |     -     |   10.15  \n",
      "   4    |   90    |   0.078674   |     -     |   10.19  \n",
      "   4    |   100   |   0.153275   |     -     |   10.14  \n",
      "   4    |   110   |   0.034803   |     -     |   43.18  \n",
      "   4    |   120   |   0.057455   |     -     |   9.83   \n",
      "   4    |   130   |   0.029921   |     -     |   9.78   \n",
      "   4    |   140   |   0.068465   |     -     |   9.80   \n",
      "   4    |   150   |   0.006285   |     -     |   9.84   \n",
      "   4    |   160   |   0.171139   |     -     |   9.81   \n",
      "   4    |   170   |   0.116940   |     -     |   9.80   \n",
      "   4    |   180   |   0.066191   |     -     |   9.81   \n",
      "   4    |   190   |   0.028855   |     -     |   9.79   \n",
      "   4    |   200   |   0.068675   |     -     |   9.78   \n",
      "   4    |   210   |   0.058753   |     -     |   43.27  \n",
      "   4    |   220   |   0.128706   |     -     |   9.90   \n",
      "   4    |   230   |   0.021025   |     -     |   9.81   \n",
      "   4    |   240   |   0.082355   |     -     |   9.87   \n",
      "   4    |   250   |   0.013141   |     -     |   9.80   \n",
      "   4    |   260   |   0.008195   |     -     |   9.80   \n",
      "   4    |   270   |   0.009654   |     -     |   9.81   \n",
      "   4    |   280   |   0.098461   |     -     |   9.80   \n",
      "   4    |   290   |   0.104614   |     -     |   9.80   \n",
      "   4    |   300   |   0.110603   |     -     |   9.87   \n",
      "   4    |   310   |   0.163337   |     -     |   43.36  \n",
      "   4    |   320   |   0.040709   |     -     |   9.82   \n",
      "   4    |   330   |   0.043700   |     -     |   9.84   \n",
      "   4    |   340   |   0.106184   |     -     |   9.80   \n",
      "   4    |   350   |   0.068748   |     -     |   9.80   \n",
      "   4    |   360   |   0.055389   |     -     |   9.82   \n",
      "   4    |   370   |   0.055944   |     -     |   9.82   \n",
      "   4    |   372   |   0.011314   |     -     |   1.96   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.070407   |  0.272687  |  503.64  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   4    |  93.298%  |  98.276%  |  98.297%  |  93.298%  |  93.298%  |  93.298%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   5    |   10    |   0.005126   |     -     |   11.18  \n",
      "   5    |   20    |   0.017839   |     -     |   10.17  \n",
      "   5    |   30    |   0.081211   |     -     |   10.13  \n",
      "   5    |   40    |   0.070577   |     -     |   10.14  \n",
      "   5    |   50    |   0.098073   |     -     |   10.14  \n",
      "   5    |   60    |   0.033779   |     -     |   10.15  \n",
      "   5    |   70    |   0.078002   |     -     |   10.15  \n",
      "   5    |   80    |   0.044821   |     -     |   10.18  \n",
      "   5    |   90    |   0.079475   |     -     |   10.15  \n",
      "   5    |   100   |   0.065188   |     -     |   10.15  \n",
      "   5    |   110   |   0.035253   |     -     |   43.18  \n",
      "   5    |   120   |   0.025060   |     -     |   9.81   \n",
      "   5    |   130   |   0.029604   |     -     |   9.82   \n",
      "   5    |   140   |   0.045946   |     -     |   9.84   \n",
      "   5    |   150   |   0.026029   |     -     |   9.82   \n",
      "   5    |   160   |   0.090486   |     -     |   9.79   \n",
      "   5    |   170   |   0.022486   |     -     |   9.80   \n",
      "   5    |   180   |   0.012713   |     -     |   9.83   \n",
      "   5    |   190   |   0.021960   |     -     |   9.82   \n",
      "   5    |   200   |   0.070165   |     -     |   9.81   \n",
      "   5    |   210   |   0.058231   |     -     |   43.19  \n",
      "   5    |   220   |   0.036695   |     -     |   9.83   \n",
      "   5    |   230   |   0.049221   |     -     |   9.80   \n",
      "   5    |   240   |   0.006092   |     -     |   9.81   \n",
      "   5    |   250   |   0.032418   |     -     |   9.83   \n",
      "   5    |   260   |   0.048995   |     -     |   9.79   \n",
      "   5    |   270   |   0.011501   |     -     |   9.81   \n",
      "   5    |   280   |   0.009009   |     -     |   9.84   \n",
      "   5    |   290   |   0.112168   |     -     |   9.79   \n",
      "   5    |   300   |   0.093272   |     -     |   9.80   \n",
      "   5    |   310   |   0.025897   |     -     |   43.13  \n",
      "   5    |   320   |   0.001846   |     -     |   9.89   \n",
      "   5    |   330   |   0.073883   |     -     |   9.83   \n",
      "   5    |   340   |   0.090040   |     -     |   9.80   \n",
      "   5    |   350   |   0.002332   |     -     |   9.81   \n",
      "   5    |   360   |   0.019512   |     -     |   9.83   \n",
      "   5    |   370   |   0.005408   |     -     |   9.79   \n",
      "   5    |   372   |   0.058526   |     -     |   1.96   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.044154   |  0.300875  |  502.93  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   5    |  93.458%  |  98.168%  |  98.185%  |  93.458%  |  93.458%  |  93.458%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   6    |   10    |   0.053038   |     -     |   11.16  \n",
      "   6    |   20    |   0.051961   |     -     |   10.17  \n",
      "   6    |   30    |   0.105041   |     -     |   10.16  \n",
      "   6    |   40    |   0.034389   |     -     |   10.13  \n",
      "   6    |   50    |   0.040040   |     -     |   10.15  \n",
      "   6    |   60    |   0.013953   |     -     |   10.15  \n",
      "   6    |   70    |   0.042397   |     -     |   10.16  \n",
      "   6    |   80    |   0.101836   |     -     |   10.15  \n",
      "   6    |   90    |   0.042156   |     -     |   10.15  \n",
      "   6    |   100   |   0.029714   |     -     |   10.13  \n",
      "   6    |   110   |   0.045289   |     -     |   43.14  \n",
      "   6    |   120   |   0.037157   |     -     |   9.80   \n",
      "   6    |   130   |   0.060077   |     -     |   9.82   \n",
      "   6    |   140   |   0.003932   |     -     |   9.84   \n",
      "   6    |   150   |   0.027211   |     -     |   9.83   \n",
      "   6    |   160   |   0.007056   |     -     |   9.80   \n",
      "   6    |   170   |   0.042315   |     -     |   9.82   \n",
      "   6    |   180   |   0.037241   |     -     |   9.81   \n",
      "   6    |   190   |   0.005841   |     -     |   9.81   \n",
      "   6    |   200   |   0.033498   |     -     |   9.81   \n",
      "   6    |   210   |   0.048835   |     -     |   43.40  \n",
      "   6    |   220   |   0.004403   |     -     |   9.83   \n",
      "   6    |   230   |   0.020742   |     -     |   9.80   \n",
      "   6    |   240   |   0.020944   |     -     |   9.84   \n",
      "   6    |   250   |   0.001136   |     -     |   9.79   \n",
      "   6    |   260   |   0.045392   |     -     |   9.81   \n",
      "   6    |   270   |   0.029312   |     -     |   9.82   \n",
      "   6    |   280   |   0.031990   |     -     |   9.83   \n",
      "   6    |   290   |   0.003055   |     -     |   9.82   \n",
      "   6    |   300   |   0.001691   |     -     |   9.84   \n",
      "   6    |   310   |   0.031922   |     -     |   43.22  \n",
      "   6    |   320   |   0.070552   |     -     |   9.84   \n",
      "   6    |   330   |   0.007293   |     -     |   9.83   \n",
      "   6    |   340   |   0.084479   |     -     |   9.80   \n",
      "   6    |   350   |   0.001137   |     -     |   9.81   \n",
      "   6    |   360   |   0.003123   |     -     |   9.84   \n",
      "   6    |   370   |   0.020571   |     -     |   9.83   \n",
      "   6    |   372   |   0.005457   |     -     |   1.95   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   0.033525   |  0.321173  |  503.29  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   6    |  93.405%  |  98.007%  |  98.015%  |  93.405%  |  93.405%  |  93.405%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   7    |   10    |   0.002344   |     -     |   11.15  \n",
      "   7    |   20    |   0.026859   |     -     |   10.17  \n",
      "   7    |   30    |   0.084371   |     -     |   10.15  \n",
      "   7    |   40    |   0.006777   |     -     |   10.16  \n",
      "   7    |   50    |   0.035007   |     -     |   10.16  \n",
      "   7    |   60    |   0.051875   |     -     |   10.15  \n",
      "   7    |   70    |   0.047788   |     -     |   10.16  \n",
      "   7    |   80    |   0.028243   |     -     |   10.19  \n",
      "   7    |   90    |   0.032731   |     -     |   10.16  \n",
      "   7    |   100   |   0.001162   |     -     |   10.19  \n",
      "   7    |   110   |   0.020580   |     -     |   43.23  \n",
      "   7    |   120   |   0.039006   |     -     |   9.82   \n",
      "   7    |   130   |   0.021955   |     -     |   9.83   \n",
      "   7    |   140   |   0.049786   |     -     |   9.83   \n",
      "   7    |   150   |   0.001276   |     -     |   9.83   \n",
      "   7    |   160   |   0.026513   |     -     |   9.81   \n",
      "   7    |   170   |   0.002636   |     -     |   9.85   \n",
      "   7    |   180   |   0.033913   |     -     |   9.82   \n",
      "   7    |   190   |   0.031480   |     -     |   9.84   \n",
      "   7    |   200   |   0.001629   |     -     |   9.79   \n",
      "   7    |   210   |   0.000679   |     -     |   43.76  \n",
      "   7    |   220   |   0.030494   |     -     |   9.83   \n",
      "   7    |   230   |   0.001499   |     -     |   9.83   \n",
      "   7    |   240   |   0.069405   |     -     |   9.85   \n",
      "   7    |   250   |   0.000691   |     -     |   9.79   \n",
      "   7    |   260   |   0.000638   |     -     |   9.79   \n",
      "   7    |   270   |   0.013994   |     -     |   9.83   \n",
      "   7    |   280   |   0.012718   |     -     |   9.81   \n",
      "   7    |   290   |   0.032231   |     -     |   9.80   \n",
      "   7    |   300   |   0.061150   |     -     |   9.87   \n",
      "   7    |   310   |   0.005421   |     -     |   43.26  \n",
      "   7    |   320   |   0.001731   |     -     |   9.86   \n",
      "   7    |   330   |   0.060048   |     -     |   9.82   \n",
      "   7    |   340   |   0.034178   |     -     |   9.79   \n",
      "   7    |   350   |   0.001071   |     -     |   9.81   \n",
      "   7    |   360   |   0.059514   |     -     |   9.83   \n",
      "   7    |   370   |   0.001134   |     -     |   9.80   \n",
      "   7    |   372   |   0.000709   |     -     |   1.98   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   0.025078   |  0.355552  |  504.36  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   7    |  93.244%  |  97.853%  |  97.817%  |  93.244%  |  93.244%  |  93.244%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   8    |   10    |   0.025096   |     -     |   11.18  \n",
      "   8    |   20    |   0.049565   |     -     |   10.16  \n",
      "   8    |   30    |   0.042226   |     -     |   10.16  \n",
      "   8    |   40    |   0.015770   |     -     |   10.16  \n",
      "   8    |   50    |   0.009749   |     -     |   10.17  \n",
      "   8    |   60    |   0.052751   |     -     |   10.21  \n",
      "   8    |   70    |   0.025619   |     -     |   10.15  \n",
      "   8    |   80    |   0.031593   |     -     |   10.18  \n",
      "   8    |   90    |   0.030385   |     -     |   10.16  \n",
      "   8    |   100   |   0.029858   |     -     |   10.14  \n",
      "   8    |   110   |   0.000492   |     -     |   43.20  \n",
      "   8    |   120   |   0.000405   |     -     |   9.90   \n",
      "   8    |   130   |   0.000887   |     -     |   9.81   \n",
      "   8    |   140   |   0.000413   |     -     |   9.83   \n",
      "   8    |   150   |   0.000348   |     -     |   9.81   \n",
      "   8    |   160   |   0.024234   |     -     |   9.81   \n",
      "   8    |   170   |   0.053261   |     -     |   9.85   \n",
      "   8    |   180   |   0.018786   |     -     |   9.80   \n",
      "   8    |   190   |   0.000579   |     -     |   9.80   \n",
      "   8    |   200   |   0.007832   |     -     |   9.87   \n",
      "   8    |   210   |   0.033907   |     -     |   43.18  \n",
      "   8    |   220   |   0.035633   |     -     |   9.84   \n",
      "   8    |   230   |   0.000399   |     -     |   9.84   \n",
      "   8    |   240   |   0.046218   |     -     |   9.82   \n",
      "   8    |   250   |   0.000556   |     -     |   9.81   \n",
      "   8    |   260   |   0.001194   |     -     |   9.82   \n",
      "   8    |   270   |   0.038768   |     -     |   9.82   \n",
      "   8    |   280   |   0.004204   |     -     |   9.82   \n",
      "   8    |   290   |   0.015971   |     -     |   9.82   \n",
      "   8    |   300   |   0.000382   |     -     |   9.82   \n",
      "   8    |   310   |   0.000455   |     -     |   43.34  \n",
      "   8    |   320   |   0.000336   |     -     |   9.81   \n",
      "   8    |   330   |   0.000762   |     -     |   9.82   \n",
      "   8    |   340   |   0.055473   |     -     |   9.84   \n",
      "   8    |   350   |   0.003411   |     -     |   9.79   \n",
      "   8    |   360   |   0.020656   |     -     |   9.85   \n",
      "   8    |   370   |   0.041925   |     -     |   9.82   \n",
      "   8    |   372   |   0.000345   |     -     |   1.96   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   0.019427   |  0.383105  |  503.49  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   8    |  93.566%  |  97.897%  |  97.884%  |  93.566%  |  93.566%  |  93.566%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   9    |   10    |   0.013840   |     -     |   11.19  \n",
      "   9    |   20    |   0.027827   |     -     |   10.18  \n",
      "   9    |   30    |   0.055034   |     -     |   10.17  \n",
      "   9    |   40    |   0.062632   |     -     |   10.17  \n",
      "   9    |   50    |   0.043021   |     -     |   10.19  \n",
      "   9    |   60    |   0.003634   |     -     |   10.15  \n",
      "   9    |   70    |   0.004581   |     -     |   10.13  \n",
      "   9    |   80    |   0.002532   |     -     |   10.19  \n",
      "   9    |   90    |   0.037103   |     -     |   10.16  \n",
      "   9    |   100   |   0.017176   |     -     |   10.19  \n",
      "   9    |   110   |   0.041145   |     -     |   43.11  \n",
      "   9    |   120   |   0.016210   |     -     |   9.82   \n",
      "   9    |   130   |   0.000645   |     -     |   9.85   \n",
      "   9    |   140   |   0.023986   |     -     |   9.83   \n",
      "   9    |   150   |   0.024579   |     -     |   9.79   \n",
      "   9    |   160   |   0.014622   |     -     |   9.80   \n",
      "   9    |   170   |   0.023857   |     -     |   9.89   \n",
      "   9    |   180   |   0.000844   |     -     |   9.82   \n",
      "   9    |   190   |   0.040373   |     -     |   9.81   \n",
      "   9    |   200   |   0.001132   |     -     |   9.86   \n",
      "   9    |   210   |   0.000480   |     -     |   43.28  \n",
      "   9    |   220   |   0.000421   |     -     |   9.81   \n",
      "   9    |   230   |   0.000572   |     -     |   9.82   \n",
      "   9    |   240   |   0.000424   |     -     |   9.79   \n",
      "   9    |   250   |   0.005301   |     -     |   9.81   \n",
      "   9    |   260   |   0.000560   |     -     |   9.82   \n",
      "   9    |   270   |   0.000602   |     -     |   9.81   \n",
      "   9    |   280   |   0.000307   |     -     |   9.83   \n",
      "   9    |   290   |   0.000304   |     -     |   9.87   \n",
      "   9    |   300   |   0.036833   |     -     |   9.83   \n",
      "   9    |   310   |   0.023374   |     -     |   43.38  \n",
      "   9    |   320   |   0.000425   |     -     |   9.82   \n",
      "   9    |   330   |   0.000281   |     -     |   9.81   \n",
      "   9    |   340   |   0.000351   |     -     |   9.82   \n",
      "   9    |   350   |   0.021841   |     -     |   9.81   \n",
      "   9    |   360   |   0.000305   |     -     |   9.82   \n",
      "   9    |   370   |   0.000260   |     -     |   9.83   \n",
      "   9    |   372   |   0.000237   |     -     |   1.96   \n",
      "----------------------------------------------------------------------\n",
      "   9    |    -    |   0.014754   |  0.378062  |  503.78  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   9    |  93.512%  |  98.068%  |  98.079%  |  93.512%  |  93.512%  |  93.512%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "  10    |   10    |   0.001623   |     -     |   11.19  \n",
      "  10    |   20    |   0.006507   |     -     |   10.17  \n",
      "  10    |   30    |   0.036148   |     -     |   10.16  \n",
      "  10    |   40    |   0.053266   |     -     |   10.15  \n",
      "  10    |   50    |   0.001704   |     -     |   10.14  \n",
      "  10    |   60    |   0.015970   |     -     |   10.16  \n",
      "  10    |   70    |   0.003984   |     -     |   10.18  \n",
      "  10    |   80    |   0.046808   |     -     |   10.13  \n",
      "  10    |   90    |   0.035803   |     -     |   10.16  \n",
      "  10    |   100   |   0.020729   |     -     |   10.15  \n",
      "  10    |   110   |   0.000274   |     -     |   43.62  \n",
      "  10    |   120   |   0.039412   |     -     |   9.83   \n",
      "  10    |   130   |   0.028338   |     -     |   9.77   \n",
      "  10    |   140   |   0.013809   |     -     |   9.82   \n",
      "  10    |   150   |   0.004256   |     -     |   9.81   \n",
      "  10    |   160   |   0.000392   |     -     |   9.81   \n",
      "  10    |   170   |   0.000298   |     -     |   9.80   \n",
      "  10    |   180   |   0.000436   |     -     |   9.79   \n",
      "  10    |   190   |   0.000266   |     -     |   9.80   \n",
      "  10    |   200   |   0.000238   |     -     |   9.86   \n",
      "  10    |   210   |   0.004833   |     -     |   43.12  \n",
      "  10    |   220   |   0.000222   |     -     |   9.81   \n",
      "  10    |   230   |   0.000229   |     -     |   9.83   \n",
      "  10    |   240   |   0.000223   |     -     |   9.79   \n",
      "  10    |   250   |   0.037182   |     -     |   9.80   \n",
      "  10    |   260   |   0.000632   |     -     |   9.83   \n",
      "  10    |   270   |   0.000270   |     -     |   9.80   \n",
      "  10    |   280   |   0.000900   |     -     |   9.82   \n",
      "  10    |   290   |   0.010340   |     -     |   9.81   \n",
      "  10    |   300   |   0.050083   |     -     |   9.79   \n",
      "  10    |   310   |   0.016783   |     -     |   43.37  \n",
      "  10    |   320   |   0.000253   |     -     |   9.89   \n",
      "  10    |   330   |   0.000293   |     -     |   9.81   \n",
      "  10    |   340   |   0.000220   |     -     |   9.81   \n",
      "  10    |   350   |   0.000279   |     -     |   9.84   \n",
      "  10    |   360   |   0.000218   |     -     |   9.83   \n",
      "  10    |   370   |   0.000621   |     -     |   9.81   \n",
      "  10    |   372   |   0.000285   |     -     |   1.96   \n",
      "----------------------------------------------------------------------\n",
      "  10    |    -    |   0.011668   |  0.410636  |  503.60  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "  10    |  93.351%  |  98.107%  |  98.104%  |  93.351%  |  93.351%  |  93.351%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "  11    |   10    |   0.007424   |     -     |   11.17  \n",
      "  11    |   20    |   0.006927   |     -     |   10.15  \n",
      "  11    |   30    |   0.018430   |     -     |   10.14  \n",
      "  11    |   40    |   0.004179   |     -     |   10.16  \n",
      "  11    |   50    |   0.004117   |     -     |   10.17  \n",
      "  11    |   60    |   0.048643   |     -     |   10.17  \n",
      "  11    |   70    |   0.016448   |     -     |   10.17  \n",
      "  11    |   80    |   0.006336   |     -     |   10.15  \n",
      "  11    |   90    |   0.000728   |     -     |   10.13  \n",
      "  11    |   100   |   0.013215   |     -     |   10.18  \n",
      "  11    |   110   |   0.000397   |     -     |   43.36  \n",
      "  11    |   120   |   0.000342   |     -     |   9.82   \n",
      "  11    |   130   |   0.049537   |     -     |   9.85   \n",
      "  11    |   140   |   0.019986   |     -     |   9.81   \n",
      "  11    |   150   |   0.000224   |     -     |   9.82   \n",
      "  11    |   160   |   0.000233   |     -     |   9.83   \n",
      "  11    |   170   |   0.000169   |     -     |   9.83   \n",
      "  11    |   180   |   0.000166   |     -     |   9.81   \n",
      "  11    |   190   |   0.000176   |     -     |   9.80   \n",
      "  11    |   200   |   0.000169   |     -     |   9.82   \n",
      "  11    |   210   |   0.051680   |     -     |   43.12  \n",
      "  11    |   220   |   0.000286   |     -     |   9.82   \n",
      "  11    |   230   |   0.014492   |     -     |   9.82   \n",
      "  11    |   240   |   0.024772   |     -     |   9.81   \n",
      "  11    |   250   |   0.000377   |     -     |   9.80   \n",
      "  11    |   260   |   0.000657   |     -     |   9.85   \n",
      "  11    |   270   |   0.019882   |     -     |   9.81   \n",
      "  11    |   280   |   0.003820   |     -     |   9.81   \n",
      "  11    |   290   |   0.000204   |     -     |   9.84   \n",
      "  11    |   300   |   0.008014   |     -     |   9.83   \n",
      "  11    |   310   |   0.008221   |     -     |   43.15  \n",
      "  11    |   320   |   0.000224   |     -     |   9.91   \n",
      "  11    |   330   |   0.005883   |     -     |   9.81   \n",
      "  11    |   340   |   0.000446   |     -     |   9.79   \n",
      "  11    |   350   |   0.000269   |     -     |   9.82   \n",
      "  11    |   360   |   0.000197   |     -     |   9.80   \n",
      "  11    |   370   |   0.001419   |     -     |   9.80   \n",
      "  11    |   372   |   0.000225   |     -     |   1.95   \n",
      "----------------------------------------------------------------------\n",
      "  11    |    -    |   0.009126   |  0.414964  |  503.21  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "  11    |  93.405%  |  98.072%  |  98.068%  |  93.405%  |  93.405%  |  93.405%  \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(tp['epoch_size']):\n",
    "    print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10}  | {'Elapsed':^9}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    # Measure the elapsed time of each epoch\n",
    "    t0_epoch, t0_batch = time.time(), time.time()\n",
    "    total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        global_step +=1\n",
    "        batch_counts +=1\n",
    "\n",
    "        #Forward propogate\n",
    "        model.zero_grad()\n",
    "        feature = {k:v.to(device) for k, v in batch.items()}\n",
    "        logits = model(feature)\n",
    "        loss = model.compute_loss(feature, logits)\n",
    "        batch_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), tp.gradient_clip)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # Log steps for train loss logging\n",
    "        if (step % tp.log_steps == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "            time_elapsed = time.time() - t0_batch\n",
    "            print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "            tb.add_scalar('loss/batch_train', batch_loss / batch_counts, global_step=global_step)\n",
    "            batch_loss, batch_counts = 0, 0\n",
    "            t0_batch = time.time()\n",
    "\n",
    "        # Save steps for ckpt saving and dev evaluation\n",
    "        if (step % tp.save_steps == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "            val_metrics = binary_cls_metrics(model, valid_loader, device)\n",
    "            for key, val in val_metrics.items():\n",
    "                tb.add_scalar(f'metric/{key}', val, global_step=global_step)\n",
    "            avg_train_loss = total_loss / step\n",
    "            tb.add_scalars('loss/train_valid',{'train': avg_train_loss,\n",
    "                                                'valid': val_metrics['val_loss']}, global_step=global_step)\n",
    "            saver(total_loss / step, val_metrics['val_loss'], epoch_i, global_step, model, optimizer, scheduler)\n",
    "\n",
    "    # On Epoch End: calcualte train & valid loss and log overall metrics\n",
    "    time_elapsed = time.time() - t0_epoch\n",
    "    val_metrics = binary_cls_metrics(model, valid_loader, device)\n",
    "    avg_train_loss = total_loss / step\n",
    "\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_metrics['val_loss']:^10.6f} | {time_elapsed:^9.2f}\")\n",
    "    binary_cls_log(epoch_i, val_metrics)\n",
    "    print(\"\\n\")\n",
    "    if es.check(val_metrics):\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T06:59:30.771098Z",
     "iopub.status.busy": "2022-09-09T06:59:30.770374Z",
     "iopub.status.idle": "2022-09-09T07:00:01.695853Z",
     "shell.execute_reply": "2022-09-09T07:00:01.694969Z",
     "shell.execute_reply.started": "2022-09-09T06:59:30.771058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_s': 0.9461077844311376,\n",
       " 'f1_e': 0.9355007865757735,\n",
       " 'f1': 0.9397435857179192}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = classification_inference(model, valid_loader, device)\n",
    "valid = pd.read_csv('/kaggle/input/finneg/trainsample/trainsample/valid.csv')\n",
    "valid['pred'] = result['pred']\n",
    "valid['prob'] = result['prob']\n",
    "valid.to_csv('valid2.csv')\n",
    "overall_f1(valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
