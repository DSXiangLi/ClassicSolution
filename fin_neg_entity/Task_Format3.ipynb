{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T07:09:51.323038Z",
     "iopub.status.busy": "2022-09-09T07:09:51.322096Z",
     "iopub.status.idle": "2022-09-09T07:09:59.706901Z",
     "shell.execute_reply": "2022-09-09T07:09:59.705799Z",
     "shell.execute_reply.started": "2022-09-09T07:09:51.322994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import collections \n",
    "from itertools import chain\n",
    "import torch \n",
    "import time \n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from src.train_utils import set_seed, ModelSave, get_torch_device, EarlyStop, TrainParams\n",
    "from src.evaluation import binary_cls_report, classification_inference\n",
    "from src.metric import  binary_cls_metrics, binary_cls_log\n",
    "\n",
    "from models import BertClassifier\n",
    "from dataset import SeqPairDataset, data_loader\n",
    "from evaluation import overall_f1\n",
    "import transformers \n",
    "transformers.logging.set_verbosity_error()\n",
    "from transformers import BertTokenizer,AdamW, get_linear_schedule_with_warmup\n",
    "device = get_torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T07:09:59.709880Z",
     "iopub.status.busy": "2022-09-09T07:09:59.708309Z",
     "iopub.status.idle": "2022-09-09T07:09:59.717792Z",
     "shell.execute_reply": "2022-09-09T07:09:59.716866Z",
     "shell.execute_reply.started": "2022-09-09T07:09:59.709839Z"
    }
   },
   "outputs": [],
   "source": [
    "tp = TrainParams(\n",
    "    log_steps = 10,\n",
    "    save_steps = 10000,\n",
    "    epoch_size=20,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    max_seq_len=512,\n",
    "    batch_size=20,\n",
    "    lr=5e-6,\n",
    "    weight_decay=0.0,\n",
    "    epsilon=1e-6,\n",
    "    warmup_steps=100,\n",
    "    dropout_rate=0.5,\n",
    "    label_size=2,\n",
    "    gradient_clip=1.0,\n",
    "    early_stop_params = {\n",
    "        'monitor':'f1',\n",
    "        'mode':'max',\n",
    "        'min_delta': 0,\n",
    "        'patience':3,\n",
    "        'verbose':False\n",
    "    },\n",
    "    pretrain_model = 'hfl/chinese-roberta-wwm-ext',\n",
    "    continue_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(tp.pretrain_model, do_lower_case=True)\n",
    "special_tokens_dict = {'additional_special_tokens':['[t]','[c]','[o]','[e]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    "\n",
    "train_dataset = SeqPairDataset(data_loader('./trainsample/train3.txt'), tp.max_seq_len, tokenizer)\n",
    "valid_dataset = SeqPairDataset(data_loader('./trainsample/valid3.txt'), tp.max_seq_len, tokenizer)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=tp.batch_size)\n",
    "valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=tp.batch_size)\n",
    "\n",
    "tp.update({'num_train_steps': int(len(train_loader)*tp.epoch_size)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT = './checkpoint/single_task_bert3'\n",
    "saver = ModelSave(CKPT, continue_train=False)\n",
    "saver.init()\n",
    "es = EarlyStop(**tp.early_stop_params)\n",
    "global_step = 0\n",
    "tb = SummaryWriter(CKPT)\n",
    "\n",
    "model = BertClassifier(tp)\n",
    "model.bert.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(device)\n",
    "optimizer, scheduler = model.get_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T07:11:10.314298Z",
     "iopub.status.busy": "2022-09-09T07:11:10.313935Z",
     "iopub.status.idle": "2022-09-09T08:25:25.080771Z",
     "shell.execute_reply": "2022-09-09T08:25:25.079697Z",
     "shell.execute_reply.started": "2022-09-09T07:11:10.314259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   10    |   0.781429   |     -     |   12.26  \n",
      "   1    |   20    |   0.756371   |     -     |   10.13  \n",
      "   1    |   30    |   0.779599   |     -     |   10.14  \n",
      "   1    |   40    |   0.760590   |     -     |   10.15  \n",
      "   1    |   50    |   0.728303   |     -     |   10.18  \n",
      "   1    |   60    |   0.641215   |     -     |   10.12  \n",
      "   1    |   70    |   0.643211   |     -     |   10.14  \n",
      "   1    |   80    |   0.623227   |     -     |   10.12  \n",
      "   1    |   90    |   0.558558   |     -     |   10.15  \n",
      "   1    |   100   |   0.482720   |     -     |   10.17  \n",
      "   1    |   110   |   0.473972   |     -     |   10.13  \n",
      "   1    |   120   |   0.429527   |     -     |   10.16  \n",
      "   1    |   130   |   0.383886   |     -     |   10.18  \n",
      "   1    |   140   |   0.409829   |     -     |   10.19  \n",
      "   1    |   150   |   0.366958   |     -     |   10.14  \n",
      "   1    |   160   |   0.382302   |     -     |   10.15  \n",
      "   1    |   170   |   0.315529   |     -     |   10.11  \n",
      "   1    |   180   |   0.276330   |     -     |   10.13  \n",
      "   1    |   190   |   0.459911   |     -     |   10.18  \n",
      "   1    |   200   |   0.290701   |     -     |   10.14  \n",
      "   1    |   210   |   0.351403   |     -     |   10.14  \n",
      "   1    |   220   |   0.287638   |     -     |   10.14  \n",
      "   1    |   230   |   0.321560   |     -     |   10.12  \n",
      "   1    |   240   |   0.327417   |     -     |   10.13  \n",
      "   1    |   250   |   0.317054   |     -     |   10.18  \n",
      "   1    |   260   |   0.293171   |     -     |   10.18  \n",
      "   1    |   270   |   0.346174   |     -     |   10.13  \n",
      "   1    |   280   |   0.278321   |     -     |   10.14  \n",
      "   1    |   290   |   0.327619   |     -     |   10.14  \n",
      "   1    |   300   |   0.265213   |     -     |   10.13  \n",
      "   1    |   310   |   0.330399   |     -     |   10.14  \n",
      "   1    |   320   |   0.422045   |     -     |   10.19  \n",
      "   1    |   330   |   0.282628   |     -     |   10.13  \n",
      "   1    |   340   |   0.283034   |     -     |   10.15  \n",
      "   1    |   350   |   0.254384   |     -     |   10.17  \n",
      "   1    |   360   |   0.320029   |     -     |   10.12  \n",
      "   1    |   370   |   0.243496   |     -     |   10.13  \n",
      "   1    |   372   |   0.346760   |     -     |   2.03   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.428582   |  0.259031  |  416.50  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   1    |  88.901%  |  96.025%  |  96.133%  |  88.901%  |  88.901%  |  88.901%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   2    |   10    |   0.268962   |     -     |   11.17  \n",
      "   2    |   20    |   0.240744   |     -     |   10.14  \n",
      "   2    |   30    |   0.201965   |     -     |   10.20  \n",
      "   2    |   40    |   0.200979   |     -     |   10.14  \n",
      "   2    |   50    |   0.243602   |     -     |   10.14  \n",
      "   2    |   60    |   0.159699   |     -     |   10.20  \n",
      "   2    |   70    |   0.217872   |     -     |   10.16  \n",
      "   2    |   80    |   0.253879   |     -     |   10.14  \n",
      "   2    |   90    |   0.276438   |     -     |   10.15  \n",
      "   2    |   100   |   0.272638   |     -     |   10.17  \n",
      "   2    |   110   |   0.187520   |     -     |   10.16  \n",
      "   2    |   120   |   0.146137   |     -     |   10.16  \n",
      "   2    |   130   |   0.191414   |     -     |   10.16  \n",
      "   2    |   140   |   0.224056   |     -     |   10.18  \n",
      "   2    |   150   |   0.172595   |     -     |   10.16  \n",
      "   2    |   160   |   0.255659   |     -     |   10.18  \n",
      "   2    |   170   |   0.128646   |     -     |   10.14  \n",
      "   2    |   180   |   0.227521   |     -     |   10.14  \n",
      "   2    |   190   |   0.155217   |     -     |   10.20  \n",
      "   2    |   200   |   0.219001   |     -     |   10.13  \n",
      "   2    |   210   |   0.155829   |     -     |   10.16  \n",
      "   2    |   220   |   0.196496   |     -     |   10.20  \n",
      "   2    |   230   |   0.200112   |     -     |   10.16  \n",
      "   2    |   240   |   0.337529   |     -     |   10.14  \n",
      "   2    |   250   |   0.282316   |     -     |   10.20  \n",
      "   2    |   260   |   0.132753   |     -     |   10.16  \n",
      "   2    |   270   |   0.169182   |     -     |   10.15  \n",
      "   2    |   280   |   0.202226   |     -     |   10.20  \n",
      "   2    |   290   |   0.198429   |     -     |   10.15  \n",
      "   2    |   300   |   0.157231   |     -     |   10.16  \n",
      "   2    |   310   |   0.133291   |     -     |   10.20  \n",
      "   2    |   320   |   0.194059   |     -     |   10.16  \n",
      "   2    |   330   |   0.211458   |     -     |   10.14  \n",
      "   2    |   340   |   0.164241   |     -     |   10.19  \n",
      "   2    |   350   |   0.196352   |     -     |   10.13  \n",
      "   2    |   360   |   0.291550   |     -     |   10.13  \n",
      "   2    |   370   |   0.181889   |     -     |   10.16  \n",
      "   2    |   372   |   0.259860   |     -     |   2.06   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.207752   |  0.209649  |  421.89  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   2    |  92.386%  |  97.790%  |  97.773%  |  92.386%  |  92.386%  |  92.386%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   3    |   10    |   0.107692   |     -     |   11.20  \n",
      "   3    |   20    |   0.183660   |     -     |   10.21  \n",
      "   3    |   30    |   0.132962   |     -     |   10.16  \n",
      "   3    |   40    |   0.178430   |     -     |   10.13  \n",
      "   3    |   50    |   0.159660   |     -     |   10.14  \n",
      "   3    |   60    |   0.143528   |     -     |   10.15  \n",
      "   3    |   70    |   0.209135   |     -     |   10.18  \n",
      "   3    |   80    |   0.155468   |     -     |   10.18  \n",
      "   3    |   90    |   0.137869   |     -     |   10.18  \n",
      "   3    |   100   |   0.130959   |     -     |   10.15  \n",
      "   3    |   110   |   0.190618   |     -     |   10.16  \n",
      "   3    |   120   |   0.103743   |     -     |   10.14  \n",
      "   3    |   130   |   0.100165   |     -     |   10.18  \n",
      "   3    |   140   |   0.153178   |     -     |   10.16  \n",
      "   3    |   150   |   0.235205   |     -     |   10.17  \n",
      "   3    |   160   |   0.105452   |     -     |   10.17  \n",
      "   3    |   170   |   0.193225   |     -     |   10.16  \n",
      "   3    |   180   |   0.226034   |     -     |   10.16  \n",
      "   3    |   190   |   0.119825   |     -     |   10.19  \n",
      "   3    |   200   |   0.172906   |     -     |   10.17  \n",
      "   3    |   210   |   0.168144   |     -     |   10.16  \n",
      "   3    |   220   |   0.146974   |     -     |   10.15  \n",
      "   3    |   230   |   0.121166   |     -     |   10.17  \n",
      "   3    |   240   |   0.120624   |     -     |   10.16  \n",
      "   3    |   250   |   0.155213   |     -     |   10.17  \n",
      "   3    |   260   |   0.140376   |     -     |   10.22  \n",
      "   3    |   270   |   0.198271   |     -     |   10.16  \n",
      "   3    |   280   |   0.143966   |     -     |   10.16  \n",
      "   3    |   290   |   0.117302   |     -     |   10.15  \n",
      "   3    |   300   |   0.206242   |     -     |   10.17  \n",
      "   3    |   310   |   0.091665   |     -     |   10.18  \n",
      "   3    |   320   |   0.191389   |     -     |   10.15  \n",
      "   3    |   330   |   0.175069   |     -     |   10.16  \n",
      "   3    |   340   |   0.131653   |     -     |   10.15  \n",
      "   3    |   350   |   0.185224   |     -     |   10.14  \n",
      "   3    |   360   |   0.150814   |     -     |   10.17  \n",
      "   3    |   370   |   0.105581   |     -     |   10.17  \n",
      "   3    |   372   |   0.324518   |     -     |   2.04   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.154975   |  0.211432  |  412.69  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   3    |  93.780%  |  98.241%  |  98.233%  |  93.780%  |  93.780%  |  93.780%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   4    |   10    |   0.129301   |     -     |   11.24  \n",
      "   4    |   20    |   0.103132   |     -     |   10.14  \n",
      "   4    |   30    |   0.112135   |     -     |   10.14  \n",
      "   4    |   40    |   0.089490   |     -     |   10.17  \n",
      "   4    |   50    |   0.123324   |     -     |   10.13  \n",
      "   4    |   60    |   0.097126   |     -     |   10.16  \n",
      "   4    |   70    |   0.101522   |     -     |   10.23  \n",
      "   4    |   80    |   0.038438   |     -     |   10.16  \n",
      "   4    |   90    |   0.100407   |     -     |   10.13  \n",
      "   4    |   100   |   0.106479   |     -     |   10.14  \n",
      "   4    |   110   |   0.024271   |     -     |   10.15  \n",
      "   4    |   120   |   0.119102   |     -     |   10.13  \n",
      "   4    |   130   |   0.106489   |     -     |   10.18  \n",
      "   4    |   140   |   0.168036   |     -     |   10.17  \n",
      "   4    |   150   |   0.148384   |     -     |   10.16  \n",
      "   4    |   160   |   0.198222   |     -     |   10.16  \n",
      "   4    |   170   |   0.098400   |     -     |   10.16  \n",
      "   4    |   180   |   0.079191   |     -     |   10.16  \n",
      "   4    |   190   |   0.151063   |     -     |   10.18  \n",
      "   4    |   200   |   0.162377   |     -     |   10.16  \n",
      "   4    |   210   |   0.076080   |     -     |   10.16  \n",
      "   4    |   220   |   0.087498   |     -     |   10.17  \n",
      "   4    |   230   |   0.130219   |     -     |   10.17  \n",
      "   4    |   240   |   0.207395   |     -     |   10.14  \n",
      "   4    |   250   |   0.153869   |     -     |   10.19  \n",
      "   4    |   260   |   0.232201   |     -     |   10.16  \n",
      "   4    |   270   |   0.119608   |     -     |   10.17  \n",
      "   4    |   280   |   0.154931   |     -     |   10.15  \n",
      "   4    |   290   |   0.113022   |     -     |   10.15  \n",
      "   4    |   300   |   0.186247   |     -     |   10.12  \n",
      "   4    |   310   |   0.130893   |     -     |   10.13  \n",
      "   4    |   320   |   0.148634   |     -     |   10.22  \n",
      "   4    |   330   |   0.099562   |     -     |   10.14  \n",
      "   4    |   340   |   0.220209   |     -     |   10.17  \n",
      "   4    |   350   |   0.155116   |     -     |   10.16  \n",
      "   4    |   360   |   0.065518   |     -     |   10.14  \n",
      "   4    |   370   |   0.089692   |     -     |   10.14  \n",
      "   4    |   372   |   0.008774   |     -     |   2.03   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.124792   |  0.214024  |  412.30  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   4    |  93.995%  |  98.225%  |  98.239%  |  93.995%  |  93.995%  |  93.995%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   5    |   10    |   0.054236   |     -     |   11.20  \n",
      "   5    |   20    |   0.052281   |     -     |   10.14  \n",
      "   5    |   30    |   0.044437   |     -     |   10.17  \n",
      "   5    |   40    |   0.120311   |     -     |   10.23  \n",
      "   5    |   50    |   0.067565   |     -     |   10.18  \n",
      "   5    |   60    |   0.095428   |     -     |   10.16  \n",
      "   5    |   70    |   0.031105   |     -     |   10.18  \n",
      "   5    |   80    |   0.126827   |     -     |   10.17  \n",
      "   5    |   90    |   0.064160   |     -     |   10.16  \n",
      "   5    |   100   |   0.148159   |     -     |   10.20  \n",
      "   5    |   110   |   0.133351   |     -     |   10.17  \n",
      "   5    |   120   |   0.072749   |     -     |   10.15  \n",
      "   5    |   130   |   0.165577   |     -     |   10.18  \n",
      "   5    |   140   |   0.083074   |     -     |   10.16  \n",
      "   5    |   150   |   0.126846   |     -     |   10.19  \n",
      "   5    |   160   |   0.137399   |     -     |   10.18  \n",
      "   5    |   170   |   0.078454   |     -     |   10.15  \n",
      "   5    |   180   |   0.127087   |     -     |   10.15  \n",
      "   5    |   190   |   0.046671   |     -     |   10.20  \n",
      "   5    |   200   |   0.115960   |     -     |   10.20  \n",
      "   5    |   210   |   0.116114   |     -     |   10.17  \n",
      "   5    |   220   |   0.055289   |     -     |   10.20  \n",
      "   5    |   230   |   0.029683   |     -     |   10.20  \n",
      "   5    |   240   |   0.110192   |     -     |   10.13  \n",
      "   5    |   250   |   0.121499   |     -     |   10.12  \n",
      "   5    |   260   |   0.038653   |     -     |   10.18  \n",
      "   5    |   270   |   0.216178   |     -     |   10.18  \n",
      "   5    |   280   |   0.149713   |     -     |   10.17  \n",
      "   5    |   290   |   0.065333   |     -     |   10.41  \n",
      "   5    |   300   |   0.189549   |     -     |   10.16  \n",
      "   5    |   310   |   0.073918   |     -     |   10.20  \n",
      "   5    |   320   |   0.123002   |     -     |   10.19  \n",
      "   5    |   330   |   0.102106   |     -     |   10.16  \n",
      "   5    |   340   |   0.161544   |     -     |   10.17  \n",
      "   5    |   350   |   0.021952   |     -     |   10.16  \n",
      "   5    |   360   |   0.093649   |     -     |   10.17  \n",
      "   5    |   370   |   0.036685   |     -     |   10.21  \n",
      "   5    |   372   |   0.053470   |     -     |   2.03   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.097120   |  0.267189  |  413.05  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   5    |  93.298%  |  98.121%  |  98.130%  |  93.298%  |  93.298%  |  93.298%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   6    |   10    |   0.010606   |     -     |   11.17  \n",
      "   6    |   20    |   0.084496   |     -     |   10.21  \n",
      "   6    |   30    |   0.047669   |     -     |   10.18  \n",
      "   6    |   40    |   0.135708   |     -     |   10.21  \n",
      "   6    |   50    |   0.061622   |     -     |   10.15  \n",
      "   6    |   60    |   0.118616   |     -     |   10.16  \n",
      "   6    |   70    |   0.091174   |     -     |   10.18  \n",
      "   6    |   80    |   0.109381   |     -     |   10.18  \n",
      "   6    |   90    |   0.053313   |     -     |   10.19  \n",
      "   6    |   100   |   0.045302   |     -     |   10.17  \n",
      "   6    |   110   |   0.117060   |     -     |   10.17  \n",
      "   6    |   120   |   0.066908   |     -     |   10.15  \n",
      "   6    |   130   |   0.067552   |     -     |   10.18  \n",
      "   6    |   140   |   0.044541   |     -     |   10.21  \n",
      "   6    |   150   |   0.133757   |     -     |   10.18  \n",
      "   6    |   160   |   0.170115   |     -     |   10.20  \n",
      "   6    |   170   |   0.063360   |     -     |   10.14  \n",
      "   6    |   180   |   0.055978   |     -     |   10.18  \n",
      "   6    |   190   |   0.061493   |     -     |   10.19  \n",
      "   6    |   200   |   0.075641   |     -     |   10.18  \n",
      "   6    |   210   |   0.046038   |     -     |   10.19  \n",
      "   6    |   220   |   0.004904   |     -     |   10.18  \n",
      "   6    |   230   |   0.060056   |     -     |   10.16  \n",
      "   6    |   240   |   0.037607   |     -     |   10.18  \n",
      "   6    |   250   |   0.105577   |     -     |   10.17  \n",
      "   6    |   260   |   0.122408   |     -     |   10.16  \n",
      "   6    |   270   |   0.018145   |     -     |   10.19  \n",
      "   6    |   280   |   0.073354   |     -     |   10.19  \n",
      "   6    |   290   |   0.099333   |     -     |   10.18  \n",
      "   6    |   300   |   0.192449   |     -     |   10.20  \n",
      "   6    |   310   |   0.127839   |     -     |   10.19  \n",
      "   6    |   320   |   0.053024   |     -     |   10.15  \n",
      "   6    |   330   |   0.049081   |     -     |   10.18  \n",
      "   6    |   340   |   0.076770   |     -     |   10.19  \n",
      "   6    |   350   |   0.028067   |     -     |   10.20  \n",
      "   6    |   360   |   0.038903   |     -     |   10.17  \n",
      "   6    |   370   |   0.088718   |     -     |   10.20  \n",
      "   6    |   372   |   0.098843   |     -     |   2.04   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   0.076812   |  0.266254  |  413.21  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   6    |  94.263%  |  98.273%  |  98.267%  |  94.263%  |  94.263%  |  94.263%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   7    |   10    |   0.031549   |     -     |   11.21  \n",
      "   7    |   20    |   0.039169   |     -     |   10.18  \n",
      "   7    |   30    |   0.035152   |     -     |   10.19  \n",
      "   7    |   40    |   0.080478   |     -     |   10.17  \n",
      "   7    |   50    |   0.065966   |     -     |   10.17  \n",
      "   7    |   60    |   0.029516   |     -     |   10.19  \n",
      "   7    |   70    |   0.075153   |     -     |   10.17  \n",
      "   7    |   80    |   0.098065   |     -     |   10.17  \n",
      "   7    |   90    |   0.085739   |     -     |   10.18  \n",
      "   7    |   100   |   0.044946   |     -     |   10.16  \n",
      "   7    |   110   |   0.168015   |     -     |   10.20  \n",
      "   7    |   120   |   0.034556   |     -     |   10.24  \n",
      "   7    |   130   |   0.050878   |     -     |   10.18  \n",
      "   7    |   140   |   0.047010   |     -     |   10.16  \n",
      "   7    |   150   |   0.054204   |     -     |   10.17  \n",
      "   7    |   160   |   0.063500   |     -     |   10.21  \n",
      "   7    |   170   |   0.066817   |     -     |   10.18  \n",
      "   7    |   180   |   0.028441   |     -     |   10.18  \n",
      "   7    |   190   |   0.129456   |     -     |   10.17  \n",
      "   7    |   200   |   0.050633   |     -     |   10.17  \n",
      "   7    |   210   |   0.030203   |     -     |   10.16  \n",
      "   7    |   220   |   0.119041   |     -     |   10.19  \n",
      "   7    |   230   |   0.094489   |     -     |   10.18  \n",
      "   7    |   240   |   0.087117   |     -     |   10.17  \n",
      "   7    |   250   |   0.051858   |     -     |   10.18  \n",
      "   7    |   260   |   0.035555   |     -     |   10.16  \n",
      "   7    |   270   |   0.082141   |     -     |   10.16  \n",
      "   7    |   280   |   0.036339   |     -     |   10.20  \n",
      "   7    |   290   |   0.080292   |     -     |   10.18  \n",
      "   7    |   300   |   0.094945   |     -     |   10.18  \n",
      "   7    |   310   |   0.029251   |     -     |   10.23  \n",
      "   7    |   320   |   0.007741   |     -     |   10.17  \n",
      "   7    |   330   |   0.004713   |     -     |   10.20  \n",
      "   7    |   340   |   0.206697   |     -     |   10.21  \n",
      "   7    |   350   |   0.103564   |     -     |   10.18  \n",
      "   7    |   360   |   0.087002   |     -     |   10.18  \n",
      "   7    |   370   |   0.053080   |     -     |   10.17  \n",
      "   7    |   372   |   0.089954   |     -     |   2.03   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   0.067323   |  0.278123  |  413.41  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   7    |  94.638%  |  98.298%  |  98.297%  |  94.638%  |  94.638%  |  94.638%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   8    |   10    |   0.012785   |     -     |   11.19  \n",
      "   8    |   20    |   0.033321   |     -     |   10.19  \n",
      "   8    |   30    |   0.053914   |     -     |   10.18  \n",
      "   8    |   40    |   0.014436   |     -     |   10.21  \n",
      "   8    |   50    |   0.010741   |     -     |   10.17  \n",
      "   8    |   60    |   0.050276   |     -     |   10.18  \n",
      "   8    |   70    |   0.053891   |     -     |   10.15  \n",
      "   8    |   80    |   0.090353   |     -     |   10.19  \n",
      "   8    |   90    |   0.106163   |     -     |   10.22  \n",
      "   8    |   100   |   0.106686   |     -     |   10.17  \n",
      "   8    |   110   |   0.068511   |     -     |   10.16  \n",
      "   8    |   120   |   0.026538   |     -     |   10.17  \n",
      "   8    |   130   |   0.051345   |     -     |   10.16  \n",
      "   8    |   140   |   0.036031   |     -     |   10.19  \n",
      "   8    |   150   |   0.062386   |     -     |   10.19  \n",
      "   8    |   160   |   0.039518   |     -     |   10.16  \n",
      "   8    |   170   |   0.006354   |     -     |   10.16  \n",
      "   8    |   180   |   0.043679   |     -     |   10.19  \n",
      "   8    |   190   |   0.035068   |     -     |   10.22  \n",
      "   8    |   200   |   0.029049   |     -     |   10.18  \n",
      "   8    |   210   |   0.110486   |     -     |   10.19  \n",
      "   8    |   220   |   0.080943   |     -     |   10.16  \n",
      "   8    |   230   |   0.030093   |     -     |   10.17  \n",
      "   8    |   240   |   0.001608   |     -     |   10.18  \n",
      "   8    |   250   |   0.085045   |     -     |   10.20  \n",
      "   8    |   260   |   0.035700   |     -     |   10.20  \n",
      "   8    |   270   |   0.074596   |     -     |   10.21  \n",
      "   8    |   280   |   0.046488   |     -     |   10.16  \n",
      "   8    |   290   |   0.079212   |     -     |   10.18  \n",
      "   8    |   300   |   0.068886   |     -     |   10.21  \n",
      "   8    |   310   |   0.040467   |     -     |   10.18  \n",
      "   8    |   320   |   0.034128   |     -     |   10.18  \n",
      "   8    |   330   |   0.040640   |     -     |   10.18  \n",
      "   8    |   340   |   0.057533   |     -     |   10.17  \n",
      "   8    |   350   |   0.027983   |     -     |   10.19  \n",
      "   8    |   360   |   0.098910   |     -     |   10.19  \n",
      "   8    |   370   |   0.147971   |     -     |   10.18  \n",
      "   8    |   372   |   0.026895   |     -     |   2.06   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   0.053720   |  0.289722  |  413.48  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   8    |  94.477%  |  98.196%  |  98.191%  |  94.477%  |  94.477%  |  94.477%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   9    |   10    |   0.010908   |     -     |   11.18  \n",
      "   9    |   20    |   0.010349   |     -     |   10.22  \n",
      "   9    |   30    |   0.022047   |     -     |   10.18  \n",
      "   9    |   40    |   0.079215   |     -     |   10.19  \n",
      "   9    |   50    |   0.001654   |     -     |   10.20  \n",
      "   9    |   60    |   0.027272   |     -     |   10.17  \n",
      "   9    |   70    |   0.039686   |     -     |   10.18  \n",
      "   9    |   80    |   0.051802   |     -     |   10.20  \n",
      "   9    |   90    |   0.034945   |     -     |   10.19  \n",
      "   9    |   100   |   0.067678   |     -     |   10.16  \n",
      "   9    |   110   |   0.006127   |     -     |   10.18  \n",
      "   9    |   120   |   0.063257   |     -     |   10.16  \n",
      "   9    |   130   |   0.039552   |     -     |   10.18  \n",
      "   9    |   140   |   0.022966   |     -     |   10.18  \n",
      "   9    |   150   |   0.007937   |     -     |   10.16  \n",
      "   9    |   160   |   0.072606   |     -     |   10.18  \n",
      "   9    |   170   |   0.073675   |     -     |   10.18  \n",
      "   9    |   180   |   0.040391   |     -     |   10.18  \n",
      "   9    |   190   |   0.083130   |     -     |   10.16  \n",
      "   9    |   200   |   0.097470   |     -     |   10.18  \n",
      "   9    |   210   |   0.082320   |     -     |   10.20  \n",
      "   9    |   220   |   0.036270   |     -     |   10.17  \n",
      "   9    |   230   |   0.056350   |     -     |   10.16  \n",
      "   9    |   240   |   0.124270   |     -     |   10.18  \n",
      "   9    |   250   |   0.021673   |     -     |   10.15  \n",
      "   9    |   260   |   0.003824   |     -     |   10.19  \n",
      "   9    |   270   |   0.082919   |     -     |   10.19  \n",
      "   9    |   280   |   0.053343   |     -     |   10.17  \n",
      "   9    |   290   |   0.022540   |     -     |   10.18  \n",
      "   9    |   300   |   0.048104   |     -     |   10.17  \n",
      "   9    |   310   |   0.062925   |     -     |   10.16  \n",
      "   9    |   320   |   0.005205   |     -     |   10.18  \n",
      "   9    |   330   |   0.025987   |     -     |   10.21  \n",
      "   9    |   340   |   0.052596   |     -     |   10.17  \n",
      "   9    |   350   |   0.077605   |     -     |   10.16  \n",
      "   9    |   360   |   0.043855   |     -     |   10.21  \n",
      "   9    |   370   |   0.074696   |     -     |   10.20  \n",
      "   9    |   372   |   0.007504   |     -     |   2.03   \n",
      "----------------------------------------------------------------------\n",
      "   9    |    -    |   0.046498   |  0.339837  |  413.15  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   9    |  94.263%  |  98.193%  |  98.206%  |  94.263%  |  94.263%  |  94.263%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "  10    |   10    |   0.061489   |     -     |   11.17  \n",
      "  10    |   20    |   0.056504   |     -     |   10.23  \n",
      "  10    |   30    |   0.057284   |     -     |   10.16  \n",
      "  10    |   40    |   0.001629   |     -     |   10.17  \n",
      "  10    |   50    |   0.060413   |     -     |   10.19  \n",
      "  10    |   60    |   0.030367   |     -     |   10.16  \n",
      "  10    |   70    |   0.040415   |     -     |   10.18  \n",
      "  10    |   80    |   0.007159   |     -     |   10.20  \n",
      "  10    |   90    |   0.001095   |     -     |   10.17  \n",
      "  10    |   100   |   0.018974   |     -     |   10.17  \n",
      "  10    |   110   |   0.041124   |     -     |   10.21  \n",
      "  10    |   120   |   0.024487   |     -     |   10.16  \n",
      "  10    |   130   |   0.106553   |     -     |   10.21  \n",
      "  10    |   140   |   0.028379   |     -     |   10.18  \n",
      "  10    |   150   |   0.021396   |     -     |   10.16  \n",
      "  10    |   160   |   0.049177   |     -     |   10.18  \n",
      "  10    |   170   |   0.060248   |     -     |   10.24  \n",
      "  10    |   180   |   0.063380   |     -     |   10.22  \n",
      "  10    |   190   |   0.060807   |     -     |   10.18  \n",
      "  10    |   200   |   0.038159   |     -     |   10.17  \n",
      "  10    |   210   |   0.155440   |     -     |   10.17  \n",
      "  10    |   220   |   0.015880   |     -     |   10.14  \n",
      "  10    |   230   |   0.058980   |     -     |   10.19  \n",
      "  10    |   240   |   0.002828   |     -     |   10.21  \n",
      "  10    |   250   |   0.037368   |     -     |   10.18  \n",
      "  10    |   260   |   0.036639   |     -     |   10.20  \n",
      "  10    |   270   |   0.009088   |     -     |   10.16  \n",
      "  10    |   280   |   0.072885   |     -     |   10.15  \n",
      "  10    |   290   |   0.027498   |     -     |   10.18  \n",
      "  10    |   300   |   0.015859   |     -     |   10.16  \n",
      "  10    |   310   |   0.002133   |     -     |   10.15  \n",
      "  10    |   320   |   0.046902   |     -     |   10.19  \n",
      "  10    |   330   |   0.033678   |     -     |   10.15  \n",
      "  10    |   340   |   0.025185   |     -     |   10.17  \n",
      "  10    |   350   |   0.001367   |     -     |   10.15  \n",
      "  10    |   360   |   0.038085   |     -     |   10.21  \n",
      "  10    |   370   |   0.039863   |     -     |   10.17  \n",
      "  10    |   372   |   0.030239   |     -     |   2.03   \n",
      "----------------------------------------------------------------------\n",
      "  10    |    -    |   0.039272   |  0.319963  |  413.09  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "  10    |  94.477%  |  98.118%  |  98.124%  |  94.477%  |  94.477%  |  94.477%  \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(tp['epoch_size']):\n",
    "    print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10}  | {'Elapsed':^9}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    # Measure the elapsed time of each epoch\n",
    "    t0_epoch, t0_batch = time.time(), time.time()\n",
    "    total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        global_step +=1\n",
    "        batch_counts +=1\n",
    "\n",
    "        #Forward propogate\n",
    "        model.zero_grad()\n",
    "        feature = {k:v.to(device) for k, v in batch.items()}\n",
    "        logits = model(feature)\n",
    "        loss = model.compute_loss(feature, logits)\n",
    "        batch_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), tp.gradient_clip)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # Log steps for train loss logging\n",
    "        if (step % tp.log_steps == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "            time_elapsed = time.time() - t0_batch\n",
    "            print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "            tb.add_scalar('loss/batch_train', batch_loss / batch_counts, global_step=global_step)\n",
    "            batch_loss, batch_counts = 0, 0\n",
    "            t0_batch = time.time()\n",
    "\n",
    "        # Save steps for ckpt saving and dev evaluation\n",
    "        if (step % tp.save_steps == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "            val_metrics = binary_cls_metrics(model, valid_loader, device)\n",
    "            for key, val in val_metrics.items():\n",
    "                tb.add_scalar(f'metric/{key}', val, global_step=global_step)\n",
    "            avg_train_loss = total_loss / step\n",
    "            tb.add_scalars('loss/train_valid',{'train': avg_train_loss,\n",
    "                                                'valid': val_metrics['val_loss']}, global_step=global_step)\n",
    "            saver(total_loss / step, val_metrics['val_loss'], epoch_i, global_step, model, optimizer, scheduler)\n",
    "\n",
    "    # On Epoch End: calcualte train & valid loss and log overall metrics\n",
    "    time_elapsed = time.time() - t0_epoch\n",
    "    val_metrics = binary_cls_metrics(model, valid_loader, device)\n",
    "    avg_train_loss = total_loss / step\n",
    "\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_metrics['val_loss']:^10.6f} | {time_elapsed:^9.2f}\")\n",
    "    binary_cls_log(epoch_i, val_metrics)\n",
    "    print(\"\\n\")\n",
    "    if es.check(val_metrics):\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T08:40:27.116958Z",
     "iopub.status.busy": "2022-09-09T08:40:27.116597Z",
     "iopub.status.idle": "2022-09-09T08:40:27.138659Z",
     "shell.execute_reply": "2022-09-09T08:40:27.137615Z",
     "shell.execute_reply.started": "2022-09-09T08:40:27.116927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_s': 0.9501822600243014,\n",
       " 'f1_e': 0.9450666666666666,\n",
       " 'f1': 0.9471129040097206}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = classification_inference(model, valid_loader, device)\n",
    "\n",
    "valid = pd.read_csv('./trainsample/valid.csv')\n",
    "valid['pred'] = result['pred']\n",
    "valid['prob'] = result['prob']\n",
    "valid.loc[:,['id','single_entity','pred','prob']].to_csv('valid_pred_v3.csv')\n",
    "overall_f1(valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
