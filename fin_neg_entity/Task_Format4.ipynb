{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-10T00:15:26.322925Z",
     "iopub.status.busy": "2022-09-10T00:15:26.322590Z",
     "iopub.status.idle": "2022-09-10T00:15:35.192193Z",
     "shell.execute_reply": "2022-09-10T00:15:35.191041Z",
     "shell.execute_reply.started": "2022-09-10T00:15:26.322892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import collections \n",
    "from itertools import chain\n",
    "import torch \n",
    "import time \n",
    "import pandas as pd\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from src.train_utils import set_seed, ModelSave, get_torch_device, EarlyStop, TrainParams\n",
    "from src.evaluation import binary_cls_report, classification_inference\n",
    "from src.metric import  binary_cls_metrics, binary_cls_log\n",
    "\n",
    "from models import BertClassifier\n",
    "from dataset import SeqPairMtlDataset, data_loader\n",
    "from evaluation import overall_f1\n",
    "import transformers \n",
    "transformers.logging.set_verbosity_error()\n",
    "from transformers import BertTokenizer,AdamW, get_linear_schedule_with_warmup\n",
    "device = get_torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-10T00:15:35.195212Z",
     "iopub.status.busy": "2022-09-10T00:15:35.193942Z",
     "iopub.status.idle": "2022-09-10T00:15:35.203535Z",
     "shell.execute_reply": "2022-09-10T00:15:35.202428Z",
     "shell.execute_reply.started": "2022-09-10T00:15:35.195125Z"
    }
   },
   "outputs": [],
   "source": [
    "tp = TrainParams(\n",
    "    log_steps = 10,\n",
    "    save_steps = 10000,\n",
    "    epoch_size=20,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    max_seq_len=512,\n",
    "    batch_size=20,\n",
    "    lr=5e-6,\n",
    "    weight_decay=0.0,\n",
    "    epsilon=1e-6,\n",
    "    warmup_steps=100,\n",
    "    dropout_rate=0.5,\n",
    "    label_size=2,\n",
    "    gradient_clip=1.0,\n",
    "    hidden_s=200,\n",
    "    hidden_e=200,\n",
    "    early_stop_params = {\n",
    "        'monitor':'f1',\n",
    "        'mode':'max',\n",
    "        'min_delta': 0,\n",
    "        'patience':3,\n",
    "        'verbose':False\n",
    "    },\n",
    "    pretrain_model = 'hfl/chinese-roberta-wwm-ext',\n",
    "    continue_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(tp.pretrain_model, do_lower_case=True)\n",
    "special_tokens_dict = {'additional_special_tokens':['[t]','[c]','[o]','[e]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    "\n",
    "train_dataset = SeqPairMtlDataset(data_loader('./trainsample/train4.txt'), tp.max_seq_len, tokenizer)\n",
    "valid_dataset = SeqPairMtlDataset(data_loader('./trainsample/valid4.txt'), tp.max_seq_len, tokenizer)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=tp.batch_size)\n",
    "valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=tp.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.update({'num_train_steps': int(len(train_loader)*tp.epoch_size)})\n",
    "\n",
    "CKPT = './checkpoint/single_task_bert4'\n",
    "saver = ModelSave(CKPT, continue_train=False)\n",
    "saver.init()\n",
    "es = EarlyStop(**tp.early_stop_params)\n",
    "global_step = 0\n",
    "tb = SummaryWriter(CKPT)\n",
    "\n",
    "model = BertMtl(tp)\n",
    "model.bert.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(device)\n",
    "optimizer, scheduler = model.get_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-09-10T00:16:59.896193Z",
     "iopub.status.busy": "2022-09-10T00:16:59.895831Z",
     "iopub.status.idle": "2022-09-10T02:00:35.500924Z",
     "shell.execute_reply": "2022-09-10T02:00:35.499912Z",
     "shell.execute_reply.started": "2022-09-10T00:16:59.896156Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   10    |   1.468846   |     -     |   12.05  \n",
      "   1    |   20    |   1.468075   |     -     |   10.14  \n",
      "   1    |   30    |   1.393609   |     -     |   10.13  \n",
      "   1    |   40    |   1.369234   |     -     |   10.10  \n",
      "   1    |   50    |   1.361736   |     -     |   10.11  \n",
      "   1    |   60    |   1.277964   |     -     |   10.17  \n",
      "   1    |   70    |   1.310135   |     -     |   10.10  \n",
      "   1    |   80    |   1.212889   |     -     |   10.13  \n",
      "   1    |   90    |   1.216510   |     -     |   10.13  \n",
      "   1    |   100   |   1.070289   |     -     |   10.16  \n",
      "   1    |   110   |   0.996813   |     -     |   10.12  \n",
      "   1    |   120   |   0.861399   |     -     |   10.15  \n",
      "   1    |   130   |   0.790482   |     -     |   10.10  \n",
      "   1    |   140   |   0.672316   |     -     |   10.11  \n",
      "   1    |   150   |   0.656919   |     -     |   10.14  \n",
      "   1    |   160   |   0.559417   |     -     |   10.10  \n",
      "   1    |   170   |   0.508747   |     -     |   10.10  \n",
      "   1    |   180   |   0.588149   |     -     |   10.18  \n",
      "   1    |   190   |   0.570458   |     -     |   10.12  \n",
      "   1    |   200   |   0.589991   |     -     |   10.13  \n",
      "   1    |   210   |   0.503113   |     -     |   10.10  \n",
      "   1    |   220   |   0.502980   |     -     |   10.12  \n",
      "   1    |   230   |   0.509747   |     -     |   10.11  \n",
      "   1    |   240   |   0.574262   |     -     |   10.13  \n",
      "   1    |   250   |   0.449119   |     -     |   10.13  \n",
      "   1    |   260   |   0.546911   |     -     |   10.15  \n",
      "   1    |   270   |   0.441043   |     -     |   10.11  \n",
      "   1    |   280   |   0.508663   |     -     |   10.15  \n",
      "   1    |   290   |   0.408819   |     -     |   10.08  \n",
      "   1    |   300   |   0.421172   |     -     |   10.11  \n",
      "   1    |   310   |   0.467216   |     -     |   10.16  \n",
      "   1    |   320   |   0.458833   |     -     |   10.09  \n",
      "   1    |   330   |   0.381903   |     -     |   10.10  \n",
      "   1    |   340   |   0.445289   |     -     |   10.12  \n",
      "   1    |   350   |   0.380436   |     -     |   10.13  \n",
      "   1    |   360   |   0.351396   |     -     |   10.10  \n",
      "   1    |   370   |   0.407981   |     -     |   10.13  \n",
      "   1    |   372   |   0.319617   |     -     |   2.02   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.750367   |  2.303816  |  415.69  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   1    |  89.383%  |  94.928%  |  94.832%  |  89.383%  |  89.383%  |  89.383%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   2    |   10    |   0.324113   |     -     |   11.12  \n",
      "   2    |   20    |   0.465199   |     -     |   10.13  \n",
      "   2    |   30    |   0.389908   |     -     |   10.14  \n",
      "   2    |   40    |   0.429041   |     -     |   10.10  \n",
      "   2    |   50    |   0.384935   |     -     |   10.17  \n",
      "   2    |   60    |   0.340783   |     -     |   10.14  \n",
      "   2    |   70    |   0.337269   |     -     |   10.12  \n",
      "   2    |   80    |   0.338673   |     -     |   10.12  \n",
      "   2    |   90    |   0.289724   |     -     |   10.12  \n",
      "   2    |   100   |   0.354898   |     -     |   10.15  \n",
      "   2    |   110   |   0.470092   |     -     |   10.12  \n",
      "   2    |   120   |   0.348623   |     -     |   10.14  \n",
      "   2    |   130   |   0.312165   |     -     |   10.10  \n",
      "   2    |   140   |   0.328346   |     -     |   10.11  \n",
      "   2    |   150   |   0.320013   |     -     |   10.15  \n",
      "   2    |   160   |   0.319721   |     -     |   10.11  \n",
      "   2    |   170   |   0.230508   |     -     |   10.13  \n",
      "   2    |   180   |   0.193771   |     -     |   10.18  \n",
      "   2    |   190   |   0.290680   |     -     |   10.11  \n",
      "   2    |   200   |   0.369678   |     -     |   10.10  \n",
      "   2    |   210   |   0.327565   |     -     |   10.14  \n",
      "   2    |   220   |   0.235684   |     -     |   10.12  \n",
      "   2    |   320   |   0.393319   |     -     |   10.13  \n",
      "   2    |   330   |   0.232926   |     -     |   10.15  \n",
      "   2    |   340   |   0.257541   |     -     |   10.15  \n",
      "   2    |   350   |   0.317914   |     -     |   10.12  \n",
      "   2    |   360   |   0.295684   |     -     |   10.12  \n",
      "   2    |   370   |   0.236474   |     -     |   10.14  \n",
      "   2    |   372   |   0.452286   |     -     |   2.02   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.325196   |  2.527185  |  411.60  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   2    |  93.190%  |  97.780%  |  97.744%  |  93.190%  |  93.190%  |  93.190%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   3    |   10    |   0.218493   |     -     |   11.15  \n",
      "   3    |   20    |   0.237511   |     -     |   10.12  \n",
      "   3    |   30    |   0.226257   |     -     |   10.12  \n",
      "   3    |   40    |   0.356220   |     -     |   10.15  \n",
      "   3    |   50    |   0.257838   |     -     |   10.15  \n",
      "   3    |   60    |   0.261678   |     -     |   10.12  \n",
      "   3    |   70    |   0.246610   |     -     |   10.13  \n",
      "   3    |   80    |   0.299384   |     -     |   10.15  \n",
      "   3    |   90    |   0.305445   |     -     |   10.13  \n",
      "   3    |   100   |   0.278796   |     -     |   10.13  \n",
      "   3    |   110   |   0.235604   |     -     |   10.14  \n",
      "   3    |   120   |   0.300858   |     -     |   10.16  \n",
      "   3    |   130   |   0.183532   |     -     |   10.13  \n",
      "   3    |   140   |   0.212379   |     -     |   10.11  \n",
      "   3    |   150   |   0.208019   |     -     |   10.12  \n",
      "   3    |   160   |   0.295813   |     -     |   10.12  \n",
      "   3    |   170   |   0.250028   |     -     |   10.12  \n",
      "   3    |   180   |   0.202671   |     -     |   10.18  \n",
      "   3    |   190   |   0.226390   |     -     |   10.13  \n",
      "   3    |   200   |   0.195819   |     -     |   10.14  \n",
      "   3    |   210   |   0.193876   |     -     |   10.20  \n",
      "   3    |   220   |   0.208518   |     -     |   10.12  \n",
      "   3    |   230   |   0.213558   |     -     |   10.11  \n",
      "   3    |   240   |   0.181830   |     -     |   10.16  \n",
      "   3    |   250   |   0.100909   |     -     |   10.18  \n",
      "   3    |   260   |   0.127150   |     -     |   10.13  \n",
      "   3    |   270   |   0.195444   |     -     |   10.14  \n",
      "   3    |   280   |   0.162802   |     -     |   10.12  \n",
      "   3    |   290   |   0.186014   |     -     |   10.13  \n",
      "   3    |   300   |   0.198162   |     -     |   10.14  \n",
      "   3    |   310   |   0.224107   |     -     |   10.13  \n",
      "   3    |   320   |   0.283856   |     -     |   10.14  \n",
      "   3    |   330   |   0.174367   |     -     |   10.16  \n",
      "   3    |   340   |   0.125987   |     -     |   10.16  \n",
      "   3    |   350   |   0.237615   |     -     |   10.13  \n",
      "   3    |   360   |   0.127960   |     -     |   10.11  \n",
      "   3    |   370   |   0.227040   |     -     |   10.15  \n",
      "   3    |   372   |   0.229883   |     -     |   2.03   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.221408   |  2.883255  |  411.91  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   3    |  94.048%  |  98.031%  |  97.986%  |  94.048%  |  94.048%  |  94.048%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   4    |   10    |   0.231262   |     -     |   11.14  \n",
      "   4    |   20    |   0.285118   |     -     |   10.17  \n",
      "   4    |   30    |   0.179528   |     -     |   10.15  \n",
      "   4    |   40    |   0.162480   |     -     |   10.13  \n",
      "   4    |   50    |   0.151351   |     -     |   10.15  \n",
      "   4    |   60    |   0.249695   |     -     |   10.14  \n",
      "   4    |   70    |   0.132958   |     -     |   10.13  \n",
      "   4    |   80    |   0.186389   |     -     |   10.13  \n",
      "   4    |   90    |   0.086349   |     -     |   10.14  \n",
      "   4    |   100   |   0.080837   |     -     |   10.14  \n",
      "   4    |   110   |   0.120721   |     -     |   10.15  \n",
      "   4    |   120   |   0.251173   |     -     |   10.16  \n",
      "   4    |   130   |   0.081533   |     -     |   10.12  \n",
      "   4    |   140   |   0.163107   |     -     |   10.13  \n",
      "   4    |   150   |   0.120327   |     -     |   10.14  \n",
      "   4    |   160   |   0.189515   |     -     |   10.13  \n",
      "   4    |   170   |   0.172837   |     -     |   10.17  \n",
      "   4    |   180   |   0.143540   |     -     |   10.15  \n",
      "   4    |   190   |   0.186971   |     -     |   10.14  \n",
      "   4    |   200   |   0.183536   |     -     |   10.13  \n",
      "   4    |   210   |   0.157144   |     -     |   10.12  \n",
      "   4    |   220   |   0.139566   |     -     |   10.13  \n",
      "   4    |   230   |   0.182931   |     -     |   10.14  \n",
      "   4    |   240   |   0.385463   |     -     |   10.17  \n",
      "   4    |   250   |   0.111996   |     -     |   10.14  \n",
      "   4    |   260   |   0.175718   |     -     |   10.16  \n",
      "   4    |   270   |   0.178462   |     -     |   10.14  \n",
      "   4    |   280   |   0.112571   |     -     |   10.13  \n",
      "   4    |   290   |   0.130352   |     -     |   10.15  \n",
      "   4    |   300   |   0.181468   |     -     |   10.22  \n",
      "   4    |   310   |   0.191718   |     -     |   10.13  \n",
      "   4    |   320   |   0.120500   |     -     |   10.16  \n",
      "   4    |   330   |   0.169875   |     -     |   10.16  \n",
      "   4    |   340   |   0.176776   |     -     |   10.14  \n",
      "   4    |   350   |   0.175008   |     -     |   10.13  \n",
      "   4    |   360   |   0.312477   |     -     |   10.13  \n",
      "   4    |   370   |   0.278018   |     -     |   10.17  \n",
      "   4    |   372   |   0.064408   |     -     |   2.02   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.176755   |  3.240038  |  412.15  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   4    |  93.780%  |  98.315%  |  98.291%  |  93.780%  |  93.780%  |  93.780%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   5    |   10    |   0.143991   |     -     |   11.15  \n",
      "   5    |   20    |   0.189670   |     -     |   10.16  \n",
      "   5    |   30    |   0.087794   |     -     |   10.15  \n",
      "   5    |   40    |   0.111184   |     -     |   10.12  \n",
      "   5    |   50    |   0.168336   |     -     |   10.14  \n",
      "   5    |   60    |   0.140421   |     -     |   10.15  \n",
      "   5    |   70    |   0.106564   |     -     |   10.17  \n",
      "   5    |   80    |   0.123208   |     -     |   10.24  \n",
      "   5    |   90    |   0.076753   |     -     |   10.14  \n",
      "   5    |   100   |   0.099741   |     -     |   10.12  \n",
      "   5    |   110   |   0.110273   |     -     |   10.17  \n",
      "   5    |   120   |   0.054711   |     -     |   10.15  \n",
      "   5    |   130   |   0.125249   |     -     |   10.16  \n",
      "   5    |   140   |   0.157449   |     -     |   10.14  \n",
      "   5    |   150   |   0.116950   |     -     |   10.15  \n",
      "   5    |   160   |   0.118090   |     -     |   10.15  \n",
      "   5    |   170   |   0.225206   |     -     |   10.12  \n",
      "   5    |   180   |   0.136668   |     -     |   10.13  \n",
      "   5    |   190   |   0.131206   |     -     |   10.13  \n",
      "   5    |   200   |   0.149309   |     -     |   10.16  \n",
      "   5    |   210   |   0.063550   |     -     |   10.17  \n",
      "   5    |   220   |   0.057000   |     -     |   10.13  \n",
      "   5    |   230   |   0.083460   |     -     |   10.14  \n",
      "   5    |   240   |   0.128725   |     -     |   10.14  \n",
      "   5    |   250   |   0.099166   |     -     |   10.12  \n",
      "   5    |   260   |   0.140720   |     -     |   10.12  \n",
      "   5    |   270   |   0.120248   |     -     |   10.21  \n",
      "   5    |   280   |   0.112360   |     -     |   10.16  \n",
      "   5    |   290   |   0.154980   |     -     |   10.12  \n",
      "   5    |   300   |   0.089126   |     -     |   10.19  \n",
      "   5    |   310   |   0.190035   |     -     |   10.13  \n",
      "   5    |   320   |   0.245320   |     -     |   10.12  \n",
      "   5    |   330   |   0.165145   |     -     |   10.18  \n",
      "   5    |   340   |   0.085941   |     -     |   10.12  \n",
      "   5    |   350   |   0.172295   |     -     |   10.14  \n",
      "   5    |   360   |   0.094658   |     -     |   10.12  \n",
      "   5    |   370   |   0.259925   |     -     |   10.14  \n",
      "   5    |   372   |   0.188209   |     -     |   2.02   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.131384   |  3.487739  |  412.57  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   5    |  94.531%  |  98.386%  |  98.373%  |  94.531%  |  94.531%  |  94.531%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   6    |   10    |   0.115141   |     -     |   11.21  \n",
      "   6    |   20    |   0.102326   |     -     |   10.16  \n",
      "   6    |   30    |   0.138918   |     -     |   10.14  \n",
      "   6    |   40    |   0.144961   |     -     |   10.14  \n",
      "   6    |   50    |   0.151739   |     -     |   10.15  \n",
      "   6    |   60    |   0.064328   |     -     |   10.13  \n",
      "   6    |   70    |   0.060530   |     -     |   10.14  \n",
      "   6    |   80    |   0.077309   |     -     |   10.22  \n",
      "   6    |   90    |   0.107231   |     -     |   10.15  \n",
      "   6    |   100   |   0.100925   |     -     |   10.12  \n",
      "   6    |   110   |   0.140234   |     -     |   10.20  \n",
      "   6    |   120   |   0.172386   |     -     |   10.12  \n",
      "   6    |   130   |   0.067482   |     -     |   10.10  \n",
      "   6    |   140   |   0.105695   |     -     |   10.15  \n",
      "   6    |   150   |   0.096748   |     -     |   10.16  \n",
      "   6    |   160   |   0.149494   |     -     |   10.13  \n",
      "   6    |   170   |   0.102018   |     -     |   10.15  \n",
      "   6    |   180   |   0.130235   |     -     |   10.15  \n",
      "   6    |   190   |   0.161644   |     -     |   10.14  \n",
      "   6    |   200   |   0.146787   |     -     |   10.11  \n",
      "   6    |   210   |   0.076172   |     -     |   10.13  \n",
      "   6    |   220   |   0.036047   |     -     |   10.16  \n",
      "   6    |   230   |   0.085074   |     -     |   10.13  \n",
      "   6    |   240   |   0.067597   |     -     |   10.15  \n",
      "   6    |   250   |   0.102456   |     -     |   10.16  \n",
      "   6    |   260   |   0.102353   |     -     |   10.14  \n",
      "   6    |   270   |   0.068160   |     -     |   10.15  \n",
      "   6    |   280   |   0.120081   |     -     |   10.15  \n",
      "   6    |   290   |   0.058062   |     -     |   10.18  \n",
      "   6    |   300   |   0.155203   |     -     |   10.16  \n",
      "   6    |   310   |   0.087211   |     -     |   10.15  \n",
      "   6    |   320   |   0.153322   |     -     |   10.15  \n",
      "   6    |   330   |   0.069437   |     -     |   10.18  \n",
      "   6    |   340   |   0.089343   |     -     |   10.13  \n",
      "   6    |   350   |   0.170860   |     -     |   10.17  \n",
      "   6    |   360   |   0.072895   |     -     |   10.20  \n",
      "   6    |   370   |   0.137143   |     -     |   10.13  \n",
      "   6    |   372   |   0.270569   |     -     |   2.03   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   0.108956   |  3.703669  |  412.55  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   6    |  94.263%  |  98.410%  |  98.419%  |  94.263%  |  94.263%  |  94.263%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   7    |   10    |   0.115766   |     -     |   11.17  \n",
      "   7    |   20    |   0.060530   |     -     |   10.16  \n",
      "   7    |   30    |   0.044009   |     -     |   10.19  \n",
      "   7    |   40    |   0.060062   |     -     |   10.14  \n",
      "   7    |   50    |   0.069053   |     -     |   10.15  \n",
      "   7    |   60    |   0.045793   |     -     |   10.16  \n",
      "   7    |   70    |   0.043003   |     -     |   10.17  \n",
      "   7    |   80    |   0.148315   |     -     |   10.17  \n",
      "   7    |   90    |   0.046834   |     -     |   10.17  \n",
      "   7    |   100   |   0.097693   |     -     |   10.16  \n",
      "   7    |   110   |   0.083213   |     -     |   10.17  \n",
      "   7    |   120   |   0.109617   |     -     |   10.15  \n",
      "   7    |   130   |   0.133748   |     -     |   10.12  \n",
      "   7    |   140   |   0.076320   |     -     |   10.17  \n",
      "   7    |   150   |   0.063101   |     -     |   10.17  \n",
      "   7    |   160   |   0.046201   |     -     |   10.15  \n",
      "   7    |   170   |   0.122879   |     -     |   10.15  \n",
      "   7    |   180   |   0.118322   |     -     |   10.14  \n",
      "   7    |   190   |   0.085471   |     -     |   10.17  \n",
      "   7    |   200   |   0.086909   |     -     |   10.14  \n",
      "   7    |   210   |   0.134047   |     -     |   10.14  \n",
      "   7    |   220   |   0.083692   |     -     |   10.17  \n",
      "   7    |   230   |   0.238628   |     -     |   10.18  \n",
      "   7    |   240   |   0.040870   |     -     |   10.18  \n",
      "   7    |   250   |   0.082948   |     -     |   10.14  \n",
      "   7    |   260   |   0.089506   |     -     |   10.14  \n",
      "   7    |   270   |   0.067738   |     -     |   10.15  \n",
      "   7    |   280   |   0.072897   |     -     |   10.19  \n",
      "   7    |   290   |   0.171408   |     -     |   10.16  \n",
      "   7    |   300   |   0.101678   |     -     |   10.16  \n",
      "   7    |   310   |   0.078087   |     -     |   10.14  \n",
      "   7    |   320   |   0.120985   |     -     |   10.14  \n",
      "   7    |   330   |   0.101757   |     -     |   10.19  \n",
      "   7    |   340   |   0.197067   |     -     |   10.13  \n",
      "   7    |   350   |   0.049095   |     -     |   10.18  \n",
      "   7    |   360   |   0.100714   |     -     |   10.20  \n",
      "   7    |   370   |   0.037782   |     -     |   10.15  \n",
      "   7    |   372   |   0.156814   |     -     |   2.02   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   0.093244   |  3.958876  |  412.83  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   7    |  94.531%  |  98.557%  |  98.569%  |  94.531%  |  94.531%  |  94.531%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   8    |   10    |   0.010557   |     -     |   11.15  \n",
      "   8    |   20    |   0.107134   |     -     |   10.19  \n",
      "   8    |   30    |   0.086420   |     -     |   10.17  \n",
      "   8    |   40    |   0.069497   |     -     |   10.15  \n",
      "   8    |   50    |   0.033972   |     -     |   10.15  \n",
      "   8    |   60    |   0.107443   |     -     |   10.14  \n",
      "   8    |   70    |   0.035386   |     -     |   10.14  \n",
      "   8    |   80    |   0.057773   |     -     |   10.17  \n",
      "   8    |   90    |   0.035037   |     -     |   10.16  \n",
      "   8    |   100   |   0.100808   |     -     |   10.14  \n",
      "   8    |   110   |   0.057493   |     -     |   10.18  \n",
      "   8    |   120   |   0.063655   |     -     |   10.16  \n",
      "   8    |   130   |   0.052629   |     -     |   10.14  \n",
      "   8    |   140   |   0.028904   |     -     |   10.16  \n",
      "   8    |   150   |   0.047483   |     -     |   10.14  \n",
      "   8    |   160   |   0.048169   |     -     |   10.18  \n",
      "   8    |   170   |   0.107013   |     -     |   10.16  \n",
      "   8    |   180   |   0.099264   |     -     |   10.14  \n",
      "   8    |   190   |   0.065160   |     -     |   10.15  \n",
      "   8    |   200   |   0.062800   |     -     |   10.13  \n",
      "   8    |   210   |   0.093409   |     -     |   10.15  \n",
      "   8    |   220   |   0.163321   |     -     |   10.18  \n",
      "   8    |   230   |   0.116859   |     -     |   10.15  \n",
      "   8    |   240   |   0.041880   |     -     |   10.15  \n",
      "   8    |   250   |   0.167571   |     -     |   10.15  \n",
      "   8    |   260   |   0.020579   |     -     |   10.15  \n",
      "   8    |   270   |   0.062391   |     -     |   10.19  \n",
      "   8    |   280   |   0.012532   |     -     |   10.14  \n",
      "   8    |   290   |   0.011172   |     -     |   10.17  \n",
      "   8    |   300   |   0.131835   |     -     |   10.17  \n",
      "   8    |   310   |   0.151557   |     -     |   10.14  \n",
      "   8    |   320   |   0.117721   |     -     |   10.15  \n",
      "   8    |   330   |   0.039260   |     -     |   10.18  \n",
      "   8    |   340   |   0.060047   |     -     |   10.14  \n",
      "   8    |   350   |   0.036254   |     -     |   10.16  \n",
      "   8    |   360   |   0.013566   |     -     |   10.16  \n",
      "   8    |   370   |   0.079185   |     -     |   10.16  \n",
      "   8    |   372   |   0.016177   |     -     |   2.03   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   0.069893   |  4.105455  |  412.98  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   8    |  94.692%  |  98.464%  |  98.478%  |  94.692%  |  94.692%  |  94.692%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   9    |   10    |   0.040811   |     -     |   11.18  \n",
      "   9    |   20    |   0.052230   |     -     |   10.19  \n",
      "   9    |   30    |   0.031699   |     -     |   10.15  \n",
      "   9    |   40    |   0.086429   |     -     |   10.14  \n",
      "   9    |   50    |   0.063336   |     -     |   10.14  \n",
      "   9    |   60    |   0.044128   |     -     |   10.14  \n",
      "   9    |   70    |   0.146040   |     -     |   10.15  \n",
      "   9    |   80    |   0.120918   |     -     |   10.18  \n",
      "   9    |   90    |   0.065239   |     -     |   10.16  \n",
      "   9    |   100   |   0.039922   |     -     |   10.15  \n",
      "   9    |   110   |   0.048114   |     -     |   10.16  \n",
      "   9    |   120   |   0.096981   |     -     |   10.13  \n",
      "   9    |   130   |   0.140065   |     -     |   10.12  \n",
      "   9    |   140   |   0.053849   |     -     |   10.17  \n",
      "   9    |   150   |   0.071054   |     -     |   10.14  \n",
      "   9    |   160   |   0.013805   |     -     |   10.15  \n",
      "   9    |   170   |   0.086912   |     -     |   10.17  \n",
      "   9    |   180   |   0.003716   |     -     |   10.14  \n",
      "   9    |   190   |   0.053150   |     -     |   10.13  \n",
      "   9    |   200   |   0.114307   |     -     |   10.16  \n",
      "   9    |   210   |   0.033656   |     -     |   10.17  \n",
      "   9    |   220   |   0.070785   |     -     |   10.15  \n",
      "   9    |   230   |   0.046406   |     -     |   10.16  \n",
      "   9    |   240   |   0.059372   |     -     |   10.16  \n",
      "   9    |   250   |   0.073941   |     -     |   10.14  \n",
      "   9    |   260   |   0.034801   |     -     |   10.12  \n",
      "   9    |   270   |   0.026269   |     -     |   10.18  \n",
      "   9    |   280   |   0.019977   |     -     |   10.14  \n",
      "   9    |   290   |   0.068671   |     -     |   10.16  \n",
      "   9    |   300   |   0.054142   |     -     |   10.16  \n",
      "   9    |   310   |   0.076024   |     -     |   10.14  \n",
      "   9    |   320   |   0.026963   |     -     |   10.16  \n",
      "   9    |   330   |   0.003867   |     -     |   10.16  \n",
      "   9    |   340   |   0.052374   |     -     |   10.18  \n",
      "   9    |   350   |   0.030282   |     -     |   10.14  \n",
      "   9    |   360   |   0.161750   |     -     |   10.18  \n",
      "   9    |   370   |   0.112756   |     -     |   10.13  \n",
      "   9    |   372   |   0.236151   |     -     |   2.03   \n",
      "----------------------------------------------------------------------\n",
      "   9    |    -    |   0.063872   |  4.272531  |  412.70  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   9    |  94.745%  |  98.592%  |  98.583%  |  94.745%  |  94.745%  |  94.745%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "  10    |   10    |   0.066780   |     -     |   11.19  \n",
      "  10    |   20    |   0.054374   |     -     |   10.18  \n",
      "  10    |   30    |   0.045513   |     -     |   10.14  \n",
      "  10    |   40    |   0.079474   |     -     |   10.15  \n",
      "  10    |   50    |   0.095484   |     -     |   10.15  \n",
      "  10    |   60    |   0.038960   |     -     |   10.13  \n",
      "  10    |   70    |   0.032236   |     -     |   10.14  \n",
      "  10    |   80    |   0.108344   |     -     |   10.21  \n",
      "  10    |   90    |   0.035144   |     -     |   10.16  \n",
      "  10    |   100   |   0.059903   |     -     |   10.14  \n",
      "  10    |   110   |   0.129256   |     -     |   10.17  \n",
      "  10    |   120   |   0.038002   |     -     |   10.15  \n",
      "  10    |   130   |   0.007165   |     -     |   10.14  \n",
      "  10    |   140   |   0.114119   |     -     |   10.18  \n",
      "  10    |   150   |   0.061284   |     -     |   10.17  \n",
      "  10    |   160   |   0.079616   |     -     |   10.16  \n",
      "  10    |   170   |   0.058013   |     -     |   10.14  \n",
      "  10    |   180   |   0.051668   |     -     |   10.14  \n",
      "  10    |   190   |   0.080179   |     -     |   10.14  \n",
      "  10    |   200   |   0.093433   |     -     |   10.19  \n",
      "  10    |   210   |   0.038636   |     -     |   10.17  \n",
      "  10    |   220   |   0.135681   |     -     |   10.14  \n",
      "  10    |   230   |   0.121692   |     -     |   10.15  \n",
      "  10    |   240   |   0.099165   |     -     |   10.13  \n",
      "  10    |   250   |   0.008319   |     -     |   10.15  \n",
      "  10    |   260   |   0.097323   |     -     |   10.16  \n",
      "  10    |   270   |   0.006401   |     -     |   10.18  \n",
      "  10    |   280   |   0.014819   |     -     |   10.12  \n",
      "  10    |   290   |   0.064493   |     -     |   10.14  \n",
      "  10    |   300   |   0.056621   |     -     |   10.18  \n",
      "  10    |   310   |   0.009278   |     -     |   10.13  \n",
      "  10    |   320   |   0.078586   |     -     |   10.14  \n",
      "  10    |   330   |   0.014202   |     -     |   10.17  \n",
      "  10    |   340   |   0.054291   |     -     |   10.17  \n",
      "  10    |   350   |   0.078064   |     -     |   10.15  \n",
      "  10    |   360   |   0.070975   |     -     |   10.16  \n",
      "  10    |   370   |   0.017380   |     -     |   10.14  \n",
      "  10    |   372   |   0.001582   |     -     |   2.03   \n",
      "----------------------------------------------------------------------\n",
      "  10    |    -    |   0.061878   |  4.351832  |  412.80  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "  10    |  94.906%  |  98.571%  |  98.547%  |  94.906%  |  94.906%  |  94.906%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "  11    |   10    |   0.036541   |     -     |   11.19  \n",
      "  11    |   20    |   0.099962   |     -     |   10.18  \n",
      "  11    |   30    |   0.102537   |     -     |   10.16  \n",
      "  11    |   40    |   0.093734   |     -     |   10.12  \n",
      "  11    |   50    |   0.034784   |     -     |   10.15  \n",
      "  11    |   60    |   0.043566   |     -     |   10.14  \n",
      "  11    |   70    |   0.006283   |     -     |   10.16  \n",
      "  11    |   80    |   0.049642   |     -     |   10.19  \n",
      "  11    |   90    |   0.058818   |     -     |   10.15  \n",
      "  11    |   100   |   0.056620   |     -     |   10.13  \n",
      "  11    |   110   |   0.046822   |     -     |   10.17  \n",
      "  11    |   120   |   0.007163   |     -     |   10.13  \n",
      "  11    |   130   |   0.057899   |     -     |   10.13  \n",
      "  11    |   140   |   0.043900   |     -     |   10.14  \n",
      "  11    |   150   |   0.019742   |     -     |   10.18  \n",
      "  11    |   160   |   0.038768   |     -     |   10.14  \n",
      "  11    |   170   |   0.019986   |     -     |   10.16  \n",
      "  11    |   180   |   0.052406   |     -     |   10.13  \n",
      "  11    |   190   |   0.052173   |     -     |   10.12  \n",
      "  11    |   200   |   0.048174   |     -     |   10.16  \n",
      "  11    |   210   |   0.017749   |     -     |   10.18  \n",
      "  11    |   220   |   0.028205   |     -     |   10.15  \n",
      "  11    |   230   |   0.006899   |     -     |   10.16  \n",
      "  11    |   240   |   0.022121   |     -     |   10.13  \n",
      "  11    |   250   |   0.135952   |     -     |   10.13  \n",
      "  11    |   260   |   0.064778   |     -     |   10.16  \n",
      "  11    |   270   |   0.020428   |     -     |   10.13  \n",
      "  11    |   280   |   0.028487   |     -     |   10.19  \n",
      "  11    |   290   |   0.044244   |     -     |   10.16  \n",
      "  11    |   300   |   0.005875   |     -     |   10.16  \n",
      "  11    |   310   |   0.028564   |     -     |   10.14  \n",
      "  11    |   320   |   0.035642   |     -     |   10.12  \n",
      "  11    |   330   |   0.127689   |     -     |   10.19  \n",
      "  11    |   340   |   0.070961   |     -     |   10.19  \n",
      "  11    |   350   |   0.039911   |     -     |   10.15  \n",
      "  11    |   360   |   0.034484   |     -     |   10.17  \n",
      "  11    |   370   |   0.052913   |     -     |   10.13  \n",
      "  11    |   372   |   0.001455   |     -     |   2.03   \n",
      "----------------------------------------------------------------------\n",
      "  11    |    -    |   0.046730   |  4.471324  |  412.84  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "  11    |  94.960%  |  98.495%  |  98.464%  |  94.960%  |  94.960%  |  94.960%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "  12    |   10    |   0.101781   |     -     |   11.19  \n",
      "  12    |   20    |   0.011593   |     -     |   10.14  \n",
      "  12    |   30    |   0.001937   |     -     |   10.16  \n",
      "  12    |   40    |   0.073156   |     -     |   10.18  \n",
      "  12    |   50    |   0.031421   |     -     |   10.16  \n",
      "  12    |   60    |   0.074276   |     -     |   10.12  \n",
      "  12    |   70    |   0.002738   |     -     |   10.17  \n",
      "  12    |   80    |   0.004444   |     -     |   10.17  \n",
      "  12    |   90    |   0.093131   |     -     |   10.15  \n",
      "  12    |   100   |   0.010169   |     -     |   10.15  \n",
      "  12    |   110   |   0.004121   |     -     |   10.14  \n",
      "  12    |   120   |   0.016084   |     -     |   10.15  \n",
      "  12    |   130   |   0.026083   |     -     |   10.14  \n",
      "  12    |   140   |   0.025217   |     -     |   10.18  \n",
      "  12    |   150   |   0.066802   |     -     |   10.15  \n",
      "  12    |   160   |   0.044022   |     -     |   10.13  \n",
      "  12    |   170   |   0.124912   |     -     |   10.15  \n",
      "  12    |   180   |   0.002160   |     -     |   10.12  \n",
      "  12    |   190   |   0.030060   |     -     |   10.13  \n",
      "  12    |   200   |   0.006567   |     -     |   10.16  \n",
      "  12    |   210   |   0.032195   |     -     |   10.17  \n",
      "  12    |   220   |   0.053521   |     -     |   10.14  \n",
      "  12    |   230   |   0.007538   |     -     |   10.17  \n",
      "  12    |   240   |   0.031507   |     -     |   10.14  \n",
      "  12    |   250   |   0.053760   |     -     |   10.14  \n",
      "  12    |   260   |   0.069291   |     -     |   10.24  \n",
      "  12    |   270   |   0.059647   |     -     |   10.17  \n",
      "  12    |   280   |   0.033360   |     -     |   10.15  \n",
      "  12    |   290   |   0.049439   |     -     |   10.14  \n",
      "  12    |   300   |   0.018138   |     -     |   10.14  \n",
      "  12    |   310   |   0.019726   |     -     |   10.12  \n",
      "  12    |   320   |   0.130838   |     -     |   10.13  \n",
      "  12    |   330   |   0.003868   |     -     |   10.15  \n",
      "  12    |   340   |   0.071738   |     -     |   10.16  \n",
      "  12    |   350   |   0.009253   |     -     |   10.14  \n",
      "  12    |   360   |   0.006200   |     -     |   10.16  \n",
      "  12    |   370   |   0.030424   |     -     |   10.14  \n",
      "  12    |   372   |   0.007721   |     -     |   2.03   \n",
      "----------------------------------------------------------------------\n",
      "  12    |    -    |   0.038786   |  4.632493  |  412.62  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "  12    |  94.102%  |  98.502%  |  98.502%  |  94.102%  |  94.102%  |  94.102%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "  13    |   10    |   0.056486   |     -     |   11.21  \n",
      "  13    |   20    |   0.019280   |     -     |   10.14  \n",
      "  13    |   30    |   0.021550   |     -     |   10.13  \n",
      "  13    |   40    |   0.030168   |     -     |   10.15  \n",
      "  13    |   50    |   0.027590   |     -     |   10.12  \n",
      "  13    |   60    |   0.027103   |     -     |   10.13  \n",
      "  13    |   70    |   0.067573   |     -     |   10.14  \n",
      "  13    |   80    |   0.061509   |     -     |   10.18  \n",
      "  13    |   90    |   0.028552   |     -     |   10.15  \n",
      "  13    |   100   |   0.030588   |     -     |   10.14  \n",
      "  13    |   110   |   0.006672   |     -     |   10.14  \n",
      "  13    |   120   |   0.040894   |     -     |   10.14  \n",
      "  13    |   130   |   0.074593   |     -     |   10.16  \n",
      "  13    |   140   |   0.018612   |     -     |   10.15  \n",
      "  13    |   150   |   0.072422   |     -     |   10.19  \n",
      "  13    |   160   |   0.025107   |     -     |   10.15  \n",
      "  13    |   170   |   0.046238   |     -     |   10.17  \n",
      "  13    |   180   |   0.008464   |     -     |   10.13  \n",
      "  13    |   190   |   0.043907   |     -     |   10.12  \n",
      "  13    |   200   |   0.084739   |     -     |   10.16  \n",
      "  13    |   210   |   0.002722   |     -     |   10.11  \n",
      "  13    |   220   |   0.001463   |     -     |   10.15  \n",
      "  13    |   230   |   0.012340   |     -     |   10.17  \n",
      "  13    |   240   |   0.072763   |     -     |   10.13  \n",
      "  13    |   250   |   0.019814   |     -     |   10.13  \n",
      "  13    |   260   |   0.045947   |     -     |   10.13  \n",
      "  13    |   270   |   0.082601   |     -     |   10.13  \n",
      "  13    |   280   |   0.103896   |     -     |   10.14  \n",
      "  13    |   290   |   0.033227   |     -     |   10.17  \n",
      "  13    |   300   |   0.006708   |     -     |   10.14  \n",
      "  13    |   310   |   0.031309   |     -     |   10.14  \n",
      "  13    |   320   |   0.018544   |     -     |   10.20  \n",
      "  13    |   330   |   0.002467   |     -     |   10.17  \n",
      "  13    |   340   |   0.033623   |     -     |   10.12  \n",
      "  13    |   350   |   0.027324   |     -     |   10.14  \n",
      "  13    |   360   |   0.023964   |     -     |   10.21  \n",
      "  13    |   370   |   0.059935   |     -     |   10.15  \n",
      "  13    |   372   |   0.001182   |     -     |   2.03   \n",
      "----------------------------------------------------------------------\n",
      "  13    |    -    |   0.037005   |  4.694770  |  412.51  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "  13    |  94.745%  |  98.424%  |  98.363%  |  94.745%  |  94.745%  |  94.745%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "  14    |   10    |   0.003699   |     -     |   11.15  \n",
      "  14    |   20    |   0.049742   |     -     |   10.13  \n",
      "  14    |   30    |   0.038634   |     -     |   10.15  \n",
      "  14    |   40    |   0.002398   |     -     |   10.17  \n",
      "  14    |   50    |   0.018634   |     -     |   10.15  \n",
      "  14    |   60    |   0.056893   |     -     |   10.15  \n",
      "  14    |   70    |   0.078944   |     -     |   10.15  \n",
      "  14    |   80    |   0.048654   |     -     |   10.14  \n",
      "  14    |   90    |   0.000937   |     -     |   10.11  \n",
      "  14    |   100   |   0.026990   |     -     |   10.15  \n",
      "  14    |   110   |   0.003050   |     -     |   10.14  \n",
      "  14    |   120   |   0.038942   |     -     |   10.14  \n",
      "  14    |   130   |   0.020239   |     -     |   10.15  \n",
      "  14    |   140   |   0.003907   |     -     |   10.15  \n",
      "  14    |   150   |   0.050859   |     -     |   10.11  \n",
      "  14    |   160   |   0.007169   |     -     |   10.13  \n",
      "  14    |   170   |   0.041633   |     -     |   10.22  \n",
      "  14    |   180   |   0.026908   |     -     |   10.13  \n",
      "  14    |   190   |   0.069708   |     -     |   10.15  \n",
      "  14    |   200   |   0.066869   |     -     |   10.15  \n",
      "  14    |   210   |   0.012138   |     -     |   10.13  \n",
      "  14    |   220   |   0.003649   |     -     |   10.14  \n",
      "  14    |   230   |   0.096656   |     -     |   10.20  \n",
      "  14    |   240   |   0.055570   |     -     |   10.13  \n",
      "  14    |   250   |   0.002446   |     -     |   10.12  \n",
      "  14    |   260   |   0.019311   |     -     |   10.14  \n",
      "  14    |   270   |   0.072859   |     -     |   10.16  \n",
      "  14    |   280   |   0.019665   |     -     |   10.13  \n",
      "  14    |   290   |   0.022643   |     -     |   10.17  \n",
      "  14    |   300   |   0.005201   |     -     |   10.16  \n",
      "  14    |   310   |   0.009847   |     -     |   10.15  \n",
      "  14    |   320   |   0.001044   |     -     |   10.16  \n",
      "  14    |   330   |   0.008845   |     -     |   10.14  \n",
      "  14    |   340   |   0.022968   |     -     |   10.13  \n",
      "  14    |   350   |   0.091820   |     -     |   10.13  \n",
      "  14    |   360   |   0.010962   |     -     |   10.14  \n",
      "  14    |   370   |   0.001864   |     -     |   10.17  \n",
      "  14    |   372   |   0.188719   |     -     |   2.03   \n",
      "----------------------------------------------------------------------\n",
      "  14    |    -    |   0.030925   |  4.747760  |  412.53  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "  14    |  94.745%  |  98.415%  |  98.377%  |  94.745%  |  94.745%  |  94.745%  \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(tp['epoch_size']):\n",
    "    print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10}  | {'Elapsed':^9}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    # Measure the elapsed time of each epoch\n",
    "    t0_epoch, t0_batch = time.time(), time.time()\n",
    "    total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        global_step +=1\n",
    "        batch_counts +=1\n",
    "\n",
    "        #Forward propogate\n",
    "        model.zero_grad()\n",
    "        feature = {k:v.to(device) for k, v in batch.items()}\n",
    "        logits = model(feature)\n",
    "        loss = model.compute_loss(feature, logits)\n",
    "        batch_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), tp.gradient_clip)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # Log steps for train loss logging\n",
    "        if (step % tp.log_steps == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "            time_elapsed = time.time() - t0_batch\n",
    "            print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "            tb.add_scalar('loss/batch_train', batch_loss / batch_counts, global_step=global_step)\n",
    "            batch_loss, batch_counts = 0, 0\n",
    "            t0_batch = time.time()\n",
    "\n",
    "        # Save steps for ckpt saving and dev evaluation\n",
    "        if (step % tp.save_steps == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "            val_metrics = binary_cls_metrics(model, valid_loader, device, label_name='label1')\n",
    "            for key, val in val_metrics.items():\n",
    "                tb.add_scalar(f'metric/{key}', val, global_step=global_step)\n",
    "            avg_train_loss = total_loss / step\n",
    "            tb.add_scalars('loss/train_valid',{'train': avg_train_loss,\n",
    "                                                'valid': val_metrics['val_loss']}, global_step=global_step)\n",
    "            saver(total_loss / step, val_metrics['val_loss'], epoch_i, global_step, model, optimizer, scheduler)\n",
    "\n",
    "    # On Epoch End: calcualte train & valid loss and log overall metrics\n",
    "    time_elapsed = time.time() - t0_epoch\n",
    "    val_metrics = binary_cls_metrics(model, valid_loader, device, label_name='label1')\n",
    "    avg_train_loss = total_loss / step\n",
    "\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_metrics['val_loss']:^10.6f} | {time_elapsed:^9.2f}\")\n",
    "    binary_cls_log(epoch_i, val_metrics)\n",
    "    print(\"\\n\")\n",
    "    if es.check(val_metrics):\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-10T02:00:35.505036Z",
     "iopub.status.busy": "2022-09-10T02:00:35.504316Z",
     "iopub.status.idle": "2022-09-10T02:01:06.407652Z",
     "shell.execute_reply": "2022-09-10T02:01:06.406753Z",
     "shell.execute_reply.started": "2022-09-10T02:00:35.505006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_s': 0.9552058111380144,\n",
       " 'f1_e': 0.9480381760339343,\n",
       " 'f1': 0.9509052300755663}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = classification_inference(model, valid_loader, device)\n",
    "valid = pd.read_csv('./trainsample/valid.csv')\n",
    "valid['pred'] = result['pred']\n",
    "valid['prob'] = result['prob']\n",
    "valid.loc[:,['id','single_entity','pred','prob']].to_csv('valid4.csv')\n",
    "overall_f1(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benifit from multitask: compare version 3 & 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3 = pd.read_csv('./submit/valid3.csv')\n",
    "v4 = pd.read_csv('./submit/valid4.csv')\n",
    "valid = pd.read_csv('./trainsample/valid.csv')\n",
    "valid['pred_v3'] = v3['pred']\n",
    "valid['pred_v4'] = v4['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_v4</th>\n",
       "      <th>pred_v3</th>\n",
       "      <th>label</th>\n",
       "      <th>negative</th>\n",
       "      <th>entity</th>\n",
       "      <th>single_entity</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['钱牛牛']</td>\n",
       "      <td>钱牛牛</td>\n",
       "      <td>NaN</td>\n",
       "      <td>钱牛牛从成立之初就拥抱合规,不搞自融、不设立资金池、不做期限错配,根据监管要求做好合规备案</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['宜信', '小额贷']</td>\n",
       "      <td>宜信</td>\n",
       "      <td>NaN</td>\n",
       "      <td>但营业收入增长的比例远远大于净利润,说明营业成本也在不断攀升,宜信还在扩张阶段;2、宜信的模...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['融和贷', '上海银行', '合肥安易贷投资管理有限公司']</td>\n",
       "      <td>上海银行</td>\n",
       "      <td>上海银行,存管融和贷失联,原股东涉骗局;网传270亿理财产品爆仓为假</td>\n",
       "      <td>上海银行:网传我行270亿理财产品爆仓系谣言。公告:近日,网络传言上海银行浦东分行(张...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['微交易', '坚固环球']</td>\n",
       "      <td>微交易</td>\n",
       "      <td>NaN</td>\n",
       "      <td>【图】坚固环球微交易违法吗?为什么很多人都说是违法的?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['网贷天眼', '生菜金融', '弘坤资产管理(上海)有限公司']</td>\n",
       "      <td>弘坤资产管理(上海)有限公司</td>\n",
       "      <td>网贷天眼早报:P2P爆雷风险或已基本释出 生菜金融被立案</td>\n",
       "      <td>弘坤资产17.4亿基金兑付延期,法人涉嫌行贿被留置 ? 近日,有投资人反映,本应于4月12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['钱爸爸', '财迷之家']</td>\n",
       "      <td>财迷之家</td>\n",
       "      <td>NaN</td>\n",
       "      <td>钱爸爸老板跑路。有通过财迷之家渠道的吗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['正聚源', '小资钱包', '资易贷(北京)金融信息服务有限公司']</td>\n",
       "      <td>正聚源</td>\n",
       "      <td>经侦立案抓人没有控钱 正聚源资产端逍遥法外 219年4月26日,北京市公安局海淀分局对资易贷...</td>\n",
       "      <td>经侦立案抓人没有控钱 正聚源资产端逍遥法外 2019年4月26日,北京市公安局海淀分局对资易...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['海象理财']</td>\n",
       "      <td>海象理财</td>\n",
       "      <td>海象理财最新消息 海象理财公布详细的兑付方案</td>\n",
       "      <td>现在网上出现了很多投资理财平台,海象理财就是其中之一,有很多人将钱投到这个平台了。但是,有...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['九鼎投资', '君安湘合', '北京银行']</td>\n",
       "      <td>九鼎投资</td>\n",
       "      <td>公司新闻:  1、一汽集团发布“龙腾行动”:投54亿元支持自主,红旗3年达成40万辆。  2...</td>\n",
       "      <td>公司新闻:  1、一汽集团发布龙腾行动:投54亿元支持自主,红旗3年达成40万辆。  2、*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['宜贷网']</td>\n",
       "      <td>宜贷网</td>\n",
       "      <td>网贷天眼早报:银河集团违规被立案调查 宜贷网退出新进展</td>\n",
       "      <td>新闻每天都在发生,行业日新月异。每天清晨,网贷天眼读早报,为您带来最新鲜、劲爆的行业新...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['有没有', '壹佰金融', '银河天成集团']</td>\n",
       "      <td>银河天成集团</td>\n",
       "      <td>NaN</td>\n",
       "      <td>投友们赶紧看看自己的账户有没有回款~前情回顾:日前,投友称壹佰金融提现困难,事件不断发酵,7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['淘淘金']</td>\n",
       "      <td>淘淘金</td>\n",
       "      <td>NaN</td>\n",
       "      <td>不容忽视的是,一名淘淘金的高管近日接受南都记者采访时还强调了投资者金融知识储备和对虚假标的辨...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['资易贷', '小资钱包', '北京正聚源', '联合小资钱包']</td>\n",
       "      <td>小资钱包</td>\n",
       "      <td>洗黑钱北京正聚源 高利贷孙正和张赛 黑恶势力齐聚海淀 诈骗分子尽在小资 罪魁祸首~李兆民 丧...</td>\n",
       "      <td>洗黑钱北京正聚源 高利贷孙正和张赛 黑恶势力齐聚海淀 诈骗分子尽在小资 罪魁祸首~李兆民 丧...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['首家', '陕西金辉投资管理有限公司']</td>\n",
       "      <td>陕西金辉投资管理有限公司</td>\n",
       "      <td>NaN</td>\n",
       "      <td>在此次拟失联名单的私募中,陕西金辉投资管理有限公司特别显眼,它是三秦股王阮杰开设的私募投资公...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['冠e通', '冠群驰聘', '冠群驰骋']</td>\n",
       "      <td>冠群驰聘</td>\n",
       "      <td>发表了博文《曝光=-冠群驰聘因涉嫌诈骗已在天津被立案调查!线上平台冠e通能否“独善其身”?》...</td>\n",
       "      <td>发表了博文《曝光=-冠群驰聘因涉嫌诈骗已在天津被立案调查!线上平台冠e通能否独善其身?》金融...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['返利', 'vpay钱包']</td>\n",
       "      <td>vpay钱包</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vpay钱包消费返利系统vpay钱包app开发、vpay钱包系统vpay钱包系统开发、vpa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['中弘', '如意岛', '中弘股份', '海南如意岛']</td>\n",
       "      <td>中弘</td>\n",
       "      <td>当债务危机遇上退市危机:中弘股份债务升至77亿,海南如意岛项目股权被冻结...</td>\n",
       "      <td>新京报新媒体  ·作者:林子 赵毅波 原创版权禁止商业转载授权 当债务危机遇上退市危机:中...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['武汉市江夏区铁投小额贷款有限责任公司']</td>\n",
       "      <td>武汉市江夏区铁投小额贷款有限责任公司</td>\n",
       "      <td>NaN</td>\n",
       "      <td>原告:武汉市江夏区铁投小额贷款有限责任公司</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['互金公社', '度小满金融', '百度有钱花']</td>\n",
       "      <td>百度有钱花</td>\n",
       "      <td>@度小满金融 @今日头条 @微博热点 @互金公社 @今日武汉 @经济观察报 @焦刚观点 @头...</td>\n",
       "      <td>@度小满金融 @今日头条 @微博热点 @互金公社 @今日武汉 @经济观察报 @焦刚观点 @头...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['爱达财富', '重庆市南岸区']</td>\n",
       "      <td>重庆市南岸区</td>\n",
       "      <td>重庆爱达投资有限公司—爱达财富已被重庆市南岸区**机关立案调查!!!</td>\n",
       "      <td>您现在的位置是:主页 &gt; 打传动态 &gt; 重庆爱达投资有限公司—爱达财富已被重庆市南岸区**...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['360金融', '和耕传承基金', '和耕传承基金销售有限公司']</td>\n",
       "      <td>和耕传承基金销售有限公司</td>\n",
       "      <td>【亿欧快讯】360金融旗下和耕传承基金销售被责令整改</td>\n",
       "      <td>扫一扫分享微信 360金融旗下和耕传承基金销售被责令整改 亿欧 金融+ 2018-12-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['投资宝', '红岭创投', '红岭控股']</td>\n",
       "      <td>红岭创投</td>\n",
       "      <td>中国基金报记者 王元也“虽然是清盘,但不是说再见”。3月23日,在红岭创投上线运营满十年之际...</td>\n",
       "      <td>中国基金报记者 王元也虽然是清盘,但不是说再见。3月23日,在红岭创投上线运营满十年之际,红...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['冠e通', '冠群驰骋']</td>\n",
       "      <td>冠群驰骋</td>\n",
       "      <td>//@Gorgeous-Moment:注意了!!冠群驰骋 逾期,可通过此官方平台投诉 :冠e...</td>\n",
       "      <td>//@Gorgeous-Moment:注意了!!冠群驰骋 逾期,可通过此官方平台投诉 : /...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['小额贷', '51信用卡']</td>\n",
       "      <td>51信用卡</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51信用卡在招股说明书中没说清楚其小额贷款公司的来龙去脉,小编也帮你加上了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['唐小僧', '资金链', '联璧金融']</td>\n",
       "      <td>联璧金融</td>\n",
       "      <td>消金圈每周动态:学信网将关停学历查询接口 800亿网贷平台唐小僧爆雷、联璧金融疑资金链断裂</td>\n",
       "      <td>监管动态 1、学信网:将关停学历查询接口,消金机构仅剩两周时间应对危机 日前获悉,学信...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['正聚源', '小资钱包', '恒丰银行', '北京正聚源']</td>\n",
       "      <td>北京正聚源</td>\n",
       "      <td>扫黑除恶  [cp] 银监会  恒丰银行和小资钱包合作愉快!      洗黑钱北京正聚源  ...</td>\n",
       "      <td>扫黑除恶  [cp] 银监会  恒丰银行和小资钱包合作愉快!      洗黑钱北京正聚源  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pred_v4  pred_v3  label  negative                                entity  \\\n",
       "176         0        1      0         0                               ['钱牛牛']   \n",
       "226         0        1      0         0                         ['宜信', '小额贷']   \n",
       "339         1        0      1         1      ['融和贷', '上海银行', '合肥安易贷投资管理有限公司']   \n",
       "407         0        1      0         1                       ['微交易', '坚固环球']   \n",
       "426         1        0      1         1    ['网贷天眼', '生菜金融', '弘坤资产管理(上海)有限公司']   \n",
       "501         0        1      0         1                       ['钱爸爸', '财迷之家']   \n",
       "503         1        0      1         1  ['正聚源', '小资钱包', '资易贷(北京)金融信息服务有限公司']   \n",
       "763         1        0      1         1                              ['海象理财']   \n",
       "825         1        0      1         1              ['九鼎投资', '君安湘合', '北京银行']   \n",
       "846         1        0      1         1                               ['宜贷网']   \n",
       "875         0        1      0         1             ['有没有', '壹佰金融', '银河天成集团']   \n",
       "876         0        1      0         0                               ['淘淘金']   \n",
       "885         1        0      1         1    ['资易贷', '小资钱包', '北京正聚源', '联合小资钱包']   \n",
       "1068        1        0      1         1                ['首家', '陕西金辉投资管理有限公司']   \n",
       "1248        1        0      1         1               ['冠e通', '冠群驰聘', '冠群驰骋']   \n",
       "1408        0        1      0         0                      ['返利', 'vpay钱包']   \n",
       "1413        1        0      1         1        ['中弘', '如意岛', '中弘股份', '海南如意岛']   \n",
       "1537        0        1      0         0                ['武汉市江夏区铁投小额贷款有限责任公司']   \n",
       "1650        1        0      1         1            ['互金公社', '度小满金融', '百度有钱花']   \n",
       "1663        0        1      0         1                    ['爱达财富', '重庆市南岸区']   \n",
       "1691        1        0      1         1   ['360金融', '和耕传承基金', '和耕传承基金销售有限公司']   \n",
       "1717        1        0      1         1               ['投资宝', '红岭创投', '红岭控股']   \n",
       "1743        1        0      1         1                       ['冠e通', '冠群驰骋']   \n",
       "1781        0        1      0         0                      ['小额贷', '51信用卡']   \n",
       "1799        1        0      1         1                ['唐小僧', '资金链', '联璧金融']   \n",
       "1847        1        0      1         1      ['正聚源', '小资钱包', '恒丰银行', '北京正聚源']   \n",
       "\n",
       "           single_entity                                              title  \\\n",
       "176                  钱牛牛                                                NaN   \n",
       "226                   宜信                                                NaN   \n",
       "339                 上海银行                 上海银行,存管融和贷失联,原股东涉骗局;网传270亿理财产品爆仓为假   \n",
       "407                  微交易                                                NaN   \n",
       "426       弘坤资产管理(上海)有限公司                       网贷天眼早报:P2P爆雷风险或已基本释出 生菜金融被立案   \n",
       "501                 财迷之家                                                NaN   \n",
       "503                  正聚源  经侦立案抓人没有控钱 正聚源资产端逍遥法外 219年4月26日,北京市公安局海淀分局对资易贷...   \n",
       "763                 海象理财                             海象理财最新消息 海象理财公布详细的兑付方案   \n",
       "825                 九鼎投资  公司新闻:  1、一汽集团发布“龙腾行动”:投54亿元支持自主,红旗3年达成40万辆。  2...   \n",
       "846                  宜贷网                        网贷天眼早报:银河集团违规被立案调查 宜贷网退出新进展   \n",
       "875               银河天成集团                                                NaN   \n",
       "876                  淘淘金                                                NaN   \n",
       "885                 小资钱包  洗黑钱北京正聚源 高利贷孙正和张赛 黑恶势力齐聚海淀 诈骗分子尽在小资 罪魁祸首~李兆民 丧...   \n",
       "1068        陕西金辉投资管理有限公司                                                NaN   \n",
       "1248                冠群驰聘  发表了博文《曝光=-冠群驰聘因涉嫌诈骗已在天津被立案调查!线上平台冠e通能否“独善其身”?》...   \n",
       "1408              vpay钱包                                                NaN   \n",
       "1413                  中弘            当债务危机遇上退市危机:中弘股份债务升至77亿,海南如意岛项目股权被冻结...   \n",
       "1537  武汉市江夏区铁投小额贷款有限责任公司                                                NaN   \n",
       "1650               百度有钱花  @度小满金融 @今日头条 @微博热点 @互金公社 @今日武汉 @经济观察报 @焦刚观点 @头...   \n",
       "1663              重庆市南岸区                 重庆爱达投资有限公司—爱达财富已被重庆市南岸区**机关立案调查!!!   \n",
       "1691        和耕传承基金销售有限公司                         【亿欧快讯】360金融旗下和耕传承基金销售被责令整改   \n",
       "1717                红岭创投  中国基金报记者 王元也“虽然是清盘,但不是说再见”。3月23日,在红岭创投上线运营满十年之际...   \n",
       "1743                冠群驰骋  //@Gorgeous-Moment:注意了!!冠群驰骋 逾期,可通过此官方平台投诉 :冠e...   \n",
       "1781               51信用卡                                                NaN   \n",
       "1799                联璧金融      消金圈每周动态:学信网将关停学历查询接口 800亿网贷平台唐小僧爆雷、联璧金融疑资金链断裂   \n",
       "1847               北京正聚源  扫黑除恶  [cp] 银监会  恒丰银行和小资钱包合作愉快!      洗黑钱北京正聚源  ...   \n",
       "\n",
       "                                                   text  \n",
       "176       钱牛牛从成立之初就拥抱合规,不搞自融、不设立资金池、不做期限错配,根据监管要求做好合规备案  \n",
       "226   但营业收入增长的比例远远大于净利润,说明营业成本也在不断攀升,宜信还在扩张阶段;2、宜信的模...  \n",
       "339      上海银行:网传我行270亿理财产品爆仓系谣言。公告:近日,网络传言上海银行浦东分行(张...  \n",
       "407                         【图】坚固环球微交易违法吗?为什么很多人都说是违法的?  \n",
       "426    弘坤资产17.4亿基金兑付延期,法人涉嫌行贿被留置 ? 近日,有投资人反映,本应于4月12...  \n",
       "501                                 钱爸爸老板跑路。有通过财迷之家渠道的吗  \n",
       "503   经侦立案抓人没有控钱 正聚源资产端逍遥法外 2019年4月26日,北京市公安局海淀分局对资易...  \n",
       "763    现在网上出现了很多投资理财平台,海象理财就是其中之一,有很多人将钱投到这个平台了。但是,有...  \n",
       "825   公司新闻:  1、一汽集团发布龙腾行动:投54亿元支持自主,红旗3年达成40万辆。  2、*...  \n",
       "846      新闻每天都在发生,行业日新月异。每天清晨,网贷天眼读早报,为您带来最新鲜、劲爆的行业新...  \n",
       "875   投友们赶紧看看自己的账户有没有回款~前情回顾:日前,投友称壹佰金融提现困难,事件不断发酵,7...  \n",
       "876   不容忽视的是,一名淘淘金的高管近日接受南都记者采访时还强调了投资者金融知识储备和对虚假标的辨...  \n",
       "885   洗黑钱北京正聚源 高利贷孙正和张赛 黑恶势力齐聚海淀 诈骗分子尽在小资 罪魁祸首~李兆民 丧...  \n",
       "1068  在此次拟失联名单的私募中,陕西金辉投资管理有限公司特别显眼,它是三秦股王阮杰开设的私募投资公...  \n",
       "1248  发表了博文《曝光=-冠群驰聘因涉嫌诈骗已在天津被立案调查!线上平台冠e通能否独善其身?》金融...  \n",
       "1408  vpay钱包消费返利系统vpay钱包app开发、vpay钱包系统vpay钱包系统开发、vpa...  \n",
       "1413   新京报新媒体  ·作者:林子 赵毅波 原创版权禁止商业转载授权 当债务危机遇上退市危机:中...  \n",
       "1537                              原告:武汉市江夏区铁投小额贷款有限责任公司  \n",
       "1650  @度小满金融 @今日头条 @微博热点 @互金公社 @今日武汉 @经济观察报 @焦刚观点 @头...  \n",
       "1663   您现在的位置是:主页 > 打传动态 > 重庆爱达投资有限公司—爱达财富已被重庆市南岸区**...  \n",
       "1691   扫一扫分享微信 360金融旗下和耕传承基金销售被责令整改 亿欧 金融+ 2018-12-2...  \n",
       "1717  中国基金报记者 王元也虽然是清盘,但不是说再见。3月23日,在红岭创投上线运营满十年之际,红...  \n",
       "1743  //@Gorgeous-Moment:注意了!!冠群驰骋 逾期,可通过此官方平台投诉 : /...  \n",
       "1781              51信用卡在招股说明书中没说清楚其小额贷款公司的来龙去脉,小编也帮你加上了  \n",
       "1799     监管动态 1、学信网:将关停学历查询接口,消金机构仅剩两周时间应对危机 日前获悉,学信...  \n",
       "1847  扫黑除恶  [cp] 银监会  恒丰银行和小资钱包合作愉快!      洗黑钱北京正聚源  ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.loc[(valid['pred_v4']!=valid['pred_v3']) &(valid['pred_v4']==valid['label']),['pred_v4','pred_v3','label','negative','entity','single_entity','title','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
