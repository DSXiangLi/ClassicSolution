{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-08T07:48:45.378588Z",
     "iopub.status.busy": "2022-09-08T07:48:45.378175Z",
     "iopub.status.idle": "2022-09-08T07:48:45.439678Z",
     "shell.execute_reply": "2022-09-08T07:48:45.438566Z",
     "shell.execute_reply.started": "2022-09-08T07:48:45.378554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import pandas as pd\n",
    "import torch \n",
    "import time\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from src.train_utils import ModelSave, get_torch_device, EarlyStop, TrainParams\n",
    "from src.evaluation import classification_inference\n",
    "from src.metric import  binary_cls_metrics, binary_cls_log\n",
    "\n",
    "from models import BertClassifier\n",
    "from dataset import SeqPairDataset, data_loader\n",
    "from evaluation import overall_f1\n",
    "import transformers \n",
    "transformers.logging.set_verbosity_error()\n",
    "from transformers import BertTokenizer\n",
    "device = get_torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-08T07:48:47.600325Z",
     "iopub.status.busy": "2022-09-08T07:48:47.599623Z",
     "iopub.status.idle": "2022-09-08T07:48:47.606732Z",
     "shell.execute_reply": "2022-09-08T07:48:47.605648Z",
     "shell.execute_reply.started": "2022-09-08T07:48:47.600287Z"
    }
   },
   "outputs": [],
   "source": [
    "tp = TrainParams(\n",
    "    log_steps = 10,\n",
    "    save_steps = 100,\n",
    "    epoch_size=20,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    max_seq_len=512,\n",
    "    batch_size=20,\n",
    "    lr=5e-6,\n",
    "    weight_decay=0.0,\n",
    "    epsilon=1e-6,\n",
    "    warmup_steps=100,\n",
    "    dropout_rate=0.5,\n",
    "    label_size=2,\n",
    "    gradient_clip=1.0,\n",
    "    early_stop_params = {\n",
    "        'monitor':'f1',\n",
    "        'mode':'max',\n",
    "        'min_delta': 0,\n",
    "        'patience':3,\n",
    "        'verbose':False\n",
    "    },\n",
    "    pretrain_model = 'hfl/chinese-roberta-wwm-ext',\n",
    "    continue_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 4 tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(tp.pretrain_model, do_lower_case=True)\n",
    "special_tokens_dict = {'additional_special_tokens':['[t]','[c]','[o]','[e]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print('We have added', num_added_toks, 'tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'log_steps': 10, 'save_steps': 100, 'epoch_size': 20, 'loss_fn': CrossEntropyLoss(), 'max_seq_len': 512, 'batch_size': 20, 'lr': 5e-06, 'weight_decay': 0.0, 'epsilon': 1e-06, 'warmup_steps': 100, 'dropout_rate': 0.5, 'label_size': 2, 'gradient_clip': 1.0, 'early_stop_params': {'monitor': 'f1', 'mode': 'max', 'min_delta': 0, 'patience': 3, 'verbose': False}, 'pretrain_model': 'hfl/chinese-roberta-wwm-ext', 'continue_train': False, 'num_train_steps': 7460}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SeqPairDataset(data_loader('./trainsample/train1.txt'), tp.max_seq_len, tokenizer)\n",
    "valid_dataset = SeqPairDataset(data_loader('./trainsample/valid1.txt'), tp.max_seq_len, tokenizer)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=tp.batch_size)\n",
    "valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=tp.batch_size)\n",
    "\n",
    "tp.update({'num_train_steps': int(len(train_loader)*tp.epoch_size)})\n",
    "print(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-08T07:48:49.839650Z",
     "iopub.status.busy": "2022-09-08T07:48:49.838927Z",
     "iopub.status.idle": "2022-09-08T07:49:31.482330Z",
     "shell.execute_reply": "2022-09-08T07:49:31.481256Z",
     "shell.execute_reply.started": "2022-09-08T07:48:49.839612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 4 tokens\n",
      "./checkpoint/single_task_bert1 model cleaned\n"
     ]
    }
   ],
   "source": [
    "CKPT = './checkpoint/single_task_bert1'\n",
    "saver = ModelSave(CKPT, continue_train=False)\n",
    "saver.init()\n",
    "es = EarlyStop(**tp.early_stop_params)\n",
    "global_step = 0\n",
    "tb = SummaryWriter(CKPT)\n",
    "\n",
    "model = BertClassifier(tp)\n",
    "model.bert.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(device)\n",
    "optimizer, scheduler = model.get_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-08T07:49:58.522507Z",
     "iopub.status.busy": "2022-09-08T07:49:58.521914Z",
     "iopub.status.idle": "2022-09-08T09:01:53.152345Z",
     "shell.execute_reply": "2022-09-08T09:01:53.151164Z",
     "shell.execute_reply.started": "2022-09-08T07:49:58.522472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   10    |   0.919962   |     -     |   11.31  \n",
      "   1    |   20    |   0.811892   |     -     |   10.13  \n",
      "   1    |   30    |   0.845449   |     -     |   10.12  \n",
      "   1    |   40    |   0.803620   |     -     |   10.08  \n",
      "   1    |   50    |   0.666076   |     -     |   10.10  \n",
      "   1    |   60    |   0.662530   |     -     |   10.09  \n",
      "   1    |   70    |   0.695655   |     -     |   10.09  \n",
      "   1    |   80    |   0.658488   |     -     |   10.11  \n",
      "   1    |   90    |   0.601209   |     -     |   10.13  \n",
      "   1    |   100   |   0.544213   |     -     |   10.09  \n",
      "   1    |   110   |   0.436710   |     -     |   46.98  \n",
      "   1    |   120   |   0.402404   |     -     |   9.74   \n",
      "   1    |   130   |   0.334195   |     -     |   9.75   \n",
      "   1    |   140   |   0.327430   |     -     |   9.75   \n",
      "   1    |   150   |   0.370337   |     -     |   9.73   \n",
      "   1    |   160   |   0.277910   |     -     |   9.75   \n",
      "   1    |   170   |   0.370185   |     -     |   9.76   \n",
      "   1    |   180   |   0.299994   |     -     |   9.74   \n",
      "   1    |   190   |   0.333394   |     -     |   9.74   \n",
      "   1    |   200   |   0.298746   |     -     |   9.84   \n",
      "   1    |   210   |   0.230489   |     -     |   52.25  \n",
      "   1    |   220   |   0.286048   |     -     |   9.75   \n",
      "   1    |   230   |   0.270867   |     -     |   9.72   \n",
      "   1    |   240   |   0.238984   |     -     |   9.75   \n",
      "   1    |   250   |   0.241514   |     -     |   9.76   \n",
      "   1    |   260   |   0.242921   |     -     |   9.75   \n",
      "   1    |   270   |   0.198103   |     -     |   9.74   \n",
      "   1    |   280   |   0.263033   |     -     |   9.76   \n",
      "   1    |   290   |   0.191611   |     -     |   9.78   \n",
      "   1    |   300   |   0.272785   |     -     |   9.72   \n",
      "   1    |   310   |   0.211905   |     -     |   52.59  \n",
      "   1    |   320   |   0.265708   |     -     |   9.75   \n",
      "   1    |   330   |   0.217413   |     -     |   9.73   \n",
      "   1    |   340   |   0.207935   |     -     |   9.74   \n",
      "   1    |   350   |   0.197358   |     -     |   9.77   \n",
      "   1    |   360   |   0.239089   |     -     |   9.78   \n",
      "   1    |   370   |   0.244517   |     -     |   9.77   \n",
      "   1    |   372   |   0.182291   |     -     |   1.94   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.398095   |  0.207069  |  532.71  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   1    |  92.601%  |  97.534%  |  97.552%  |  92.601%  |  92.601%  |  92.601%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   2    |   10    |   0.164332   |     -     |   11.11  \n",
      "   2    |   20    |   0.166026   |     -     |   10.10  \n",
      "   2    |   30    |   0.198164   |     -     |   10.08  \n",
      "   2    |   40    |   0.191127   |     -     |   10.10  \n",
      "   2    |   50    |   0.173624   |     -     |   10.16  \n",
      "   2    |   60    |   0.159376   |     -     |   10.12  \n",
      "   2    |   70    |   0.208138   |     -     |   10.11  \n",
      "   2    |   80    |   0.163449   |     -     |   10.11  \n",
      "   2    |   90    |   0.131540   |     -     |   10.09  \n",
      "   2    |   100   |   0.184238   |     -     |   10.08  \n",
      "   2    |   110   |   0.174916   |     -     |   43.03  \n",
      "   2    |   120   |   0.132364   |     -     |   9.77   \n",
      "   2    |   130   |   0.129387   |     -     |   9.76   \n",
      "   2    |   140   |   0.215154   |     -     |   9.75   \n",
      "   2    |   150   |   0.167731   |     -     |   9.79   \n",
      "   2    |   160   |   0.124665   |     -     |   9.73   \n",
      "   2    |   170   |   0.191045   |     -     |   9.75   \n",
      "   2    |   180   |   0.163899   |     -     |   9.82   \n",
      "   2    |   190   |   0.112605   |     -     |   9.74   \n",
      "   2    |   200   |   0.088972   |     -     |   9.75   \n",
      "   2    |   210   |   0.138066   |     -     |   43.17  \n",
      "   2    |   220   |   0.141777   |     -     |   9.75   \n",
      "   2    |   230   |   0.123720   |     -     |   9.74   \n",
      "   2    |   240   |   0.102372   |     -     |   9.76   \n",
      "   2    |   250   |   0.135670   |     -     |   9.75   \n",
      "   2    |   260   |   0.115969   |     -     |   9.74   \n",
      "   2    |   270   |   0.221877   |     -     |   9.76   \n",
      "   2    |   280   |   0.140923   |     -     |   9.76   \n",
      "   2    |   290   |   0.112716   |     -     |   9.74   \n",
      "   2    |   300   |   0.163733   |     -     |   9.75   \n",
      "   2    |   310   |   0.231673   |     -     |   52.01  \n",
      "   2    |   320   |   0.160133   |     -     |   9.76   \n",
      "   2    |   330   |   0.197749   |     -     |   9.76   \n",
      "   2    |   340   |   0.131261   |     -     |   9.74   \n",
      "   2    |   350   |   0.119913   |     -     |   9.75   \n",
      "   2    |   360   |   0.205989   |     -     |   9.79   \n",
      "   2    |   370   |   0.106052   |     -     |   9.76   \n",
      "   2    |   372   |   0.264047   |     -     |   1.95   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.157516   |  0.195749  |  518.94  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   2    |  93.244%  |  98.071%  |  98.068%  |  93.244%  |  93.244%  |  93.244%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   3    |   10    |   0.118717   |     -     |   11.11  \n",
      "   3    |   20    |   0.091604   |     -     |   10.09  \n",
      "   3    |   30    |   0.097137   |     -     |   10.09  \n",
      "   3    |   40    |   0.104364   |     -     |   10.14  \n",
      "   3    |   50    |   0.092250   |     -     |   10.12  \n",
      "   3    |   60    |   0.090637   |     -     |   10.12  \n",
      "   3    |   70    |   0.124225   |     -     |   10.12  \n",
      "   3    |   80    |   0.191117   |     -     |   10.09  \n",
      "   3    |   90    |   0.025914   |     -     |   10.10  \n",
      "   3    |   100   |   0.104209   |     -     |   10.10  \n",
      "   3    |   110   |   0.081945   |     -     |   42.94  \n",
      "   3    |   120   |   0.167122   |     -     |   9.88   \n",
      "   3    |   130   |   0.124749   |     -     |   9.77   \n",
      "   3    |   140   |   0.078258   |     -     |   9.75   \n",
      "   3    |   150   |   0.035711   |     -     |   9.73   \n",
      "   3    |   160   |   0.159769   |     -     |   9.76   \n",
      "   3    |   170   |   0.112638   |     -     |   9.80   \n",
      "   3    |   180   |   0.100603   |     -     |   9.77   \n",
      "   3    |   190   |   0.100534   |     -     |   9.78   \n",
      "   3    |   200   |   0.106264   |     -     |   9.76   \n",
      "   3    |   210   |   0.094420   |     -     |   43.01  \n",
      "   3    |   220   |   0.100291   |     -     |   9.90   \n",
      "   3    |   230   |   0.043508   |     -     |   9.75   \n",
      "   3    |   240   |   0.089082   |     -     |   9.76   \n",
      "   3    |   250   |   0.045706   |     -     |   9.76   \n",
      "   3    |   260   |   0.112169   |     -     |   9.78   \n",
      "   3    |   270   |   0.155071   |     -     |   9.77   \n",
      "   3    |   280   |   0.115203   |     -     |   9.76   \n",
      "   3    |   290   |   0.148425   |     -     |   9.81   \n",
      "   3    |   300   |   0.137065   |     -     |   9.75   \n",
      "   3    |   310   |   0.144962   |     -     |   43.25  \n",
      "   3    |   320   |   0.051645   |     -     |   9.95   \n",
      "   3    |   330   |   0.134161   |     -     |   9.74   \n",
      "   3    |   340   |   0.115431   |     -     |   9.75   \n",
      "   3    |   350   |   0.030688   |     -     |   9.79   \n",
      "   3    |   360   |   0.300533   |     -     |   9.80   \n",
      "   3    |   370   |   0.044260   |     -     |   9.75   \n",
      "   3    |   372   |   0.136991   |     -     |   1.95   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.107787   |  0.208252  |  501.49  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   3    |  93.566%  |  98.229%  |  98.246%  |  93.566%  |  93.566%  |  93.566%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   4    |   10    |   0.049936   |     -     |   11.16  \n",
      "   4    |   20    |   0.074443   |     -     |   10.13  \n",
      "   4    |   30    |   0.109947   |     -     |   10.09  \n",
      "   4    |   40    |   0.070174   |     -     |   10.13  \n",
      "   4    |   50    |   0.065191   |     -     |   10.11  \n",
      "   4    |   60    |   0.073693   |     -     |   10.16  \n",
      "   4    |   70    |   0.103629   |     -     |   10.13  \n",
      "   4    |   80    |   0.038046   |     -     |   10.12  \n",
      "   4    |   90    |   0.087798   |     -     |   10.12  \n",
      "   4    |   100   |   0.201817   |     -     |   10.12  \n",
      "   4    |   110   |   0.053141   |     -     |   43.48  \n",
      "   4    |   120   |   0.062191   |     -     |   9.85   \n",
      "   4    |   130   |   0.028166   |     -     |   9.76   \n",
      "   4    |   140   |   0.088698   |     -     |   9.78   \n",
      "   4    |   150   |   0.043801   |     -     |   9.77   \n",
      "   4    |   160   |   0.064559   |     -     |   9.79   \n",
      "   4    |   170   |   0.038192   |     -     |   9.78   \n",
      "   4    |   180   |   0.032139   |     -     |   9.75   \n",
      "   4    |   190   |   0.113676   |     -     |   9.77   \n",
      "   4    |   200   |   0.005285   |     -     |   9.79   \n",
      "   4    |   210   |   0.036610   |     -     |   43.22  \n",
      "   4    |   220   |   0.082004   |     -     |   9.78   \n",
      "   4    |   230   |   0.078026   |     -     |   9.79   \n",
      "   4    |   240   |   0.007654   |     -     |   9.74   \n",
      "   4    |   250   |   0.035651   |     -     |   9.77   \n",
      "   4    |   260   |   0.035726   |     -     |   9.79   \n",
      "   4    |   270   |   0.086295   |     -     |   9.78   \n",
      "   4    |   280   |   0.112094   |     -     |   9.76   \n",
      "   4    |   290   |   0.108076   |     -     |   9.78   \n",
      "   4    |   300   |   0.019742   |     -     |   9.79   \n",
      "   4    |   310   |   0.088079   |     -     |   43.13  \n",
      "   4    |   320   |   0.121597   |     -     |   9.82   \n",
      "   4    |   330   |   0.047281   |     -     |   9.80   \n",
      "   4    |   340   |   0.048361   |     -     |   9.76   \n",
      "   4    |   350   |   0.118333   |     -     |   9.75   \n",
      "   4    |   360   |   0.028349   |     -     |   9.77   \n",
      "   4    |   370   |   0.097063   |     -     |   9.76   \n",
      "   4    |   372   |   0.177281   |     -     |   1.96   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.069783   |  0.252796  |  502.37  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   4    |  93.780%  |  98.280%  |  98.302%  |  93.780%  |  93.780%  |  93.780%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   5    |   10    |   0.023890   |     -     |   11.13  \n",
      "   5    |   20    |   0.017497   |     -     |   10.11  \n",
      "   5    |   30    |   0.076356   |     -     |   10.15  \n",
      "   5    |   40    |   0.143455   |     -     |   10.12  \n",
      "   5    |   50    |   0.027135   |     -     |   10.15  \n",
      "   5    |   60    |   0.084114   |     -     |   10.11  \n",
      "   5    |   70    |   0.081337   |     -     |   10.12  \n",
      "   5    |   80    |   0.108752   |     -     |   10.12  \n",
      "   5    |   90    |   0.056665   |     -     |   10.09  \n",
      "   5    |   100   |   0.049222   |     -     |   10.14  \n",
      "   5    |   110   |   0.051042   |     -     |   43.10  \n",
      "   5    |   120   |   0.012563   |     -     |   9.85   \n",
      "   5    |   130   |   0.104647   |     -     |   9.77   \n",
      "   5    |   140   |   0.003050   |     -     |   9.77   \n",
      "   5    |   150   |   0.078801   |     -     |   9.76   \n",
      "   5    |   160   |   0.075880   |     -     |   9.78   \n",
      "   5    |   170   |   0.033080   |     -     |   9.80   \n",
      "   5    |   180   |   0.054606   |     -     |   9.75   \n",
      "   5    |   190   |   0.059821   |     -     |   9.77   \n",
      "   5    |   200   |   0.005455   |     -     |   9.77   \n",
      "   5    |   210   |   0.078298   |     -     |   43.67  \n",
      "   5    |   220   |   0.024813   |     -     |   9.76   \n",
      "   5    |   230   |   0.077392   |     -     |   9.75   \n",
      "   5    |   240   |   0.038194   |     -     |   9.78   \n",
      "   5    |   250   |   0.068737   |     -     |   9.77   \n",
      "   5    |   260   |   0.114063   |     -     |   9.77   \n",
      "   5    |   270   |   0.063878   |     -     |   9.79   \n",
      "   5    |   280   |   0.014302   |     -     |   9.79   \n",
      "   5    |   290   |   0.024999   |     -     |   9.78   \n",
      "   5    |   300   |   0.149893   |     -     |   9.80   \n",
      "   5    |   310   |   0.093029   |     -     |   43.31  \n",
      "   5    |   320   |   0.005170   |     -     |   9.81   \n",
      "   5    |   330   |   0.070014   |     -     |   9.78   \n",
      "   5    |   340   |   0.027450   |     -     |   9.76   \n",
      "   5    |   350   |   0.006004   |     -     |   9.77   \n",
      "   5    |   360   |   0.021760   |     -     |   9.78   \n",
      "   5    |   370   |   0.007564   |     -     |   9.78   \n",
      "   5    |   372   |   0.130762   |     -     |   1.95   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.055416   |  0.277812  |  502.23  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   5    |  94.102%  |  98.312%  |  98.346%  |  94.102%  |  94.102%  |  94.102%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   6    |   10    |   0.078654   |     -     |   11.13  \n",
      "   6    |   20    |   0.012757   |     -     |   10.17  \n",
      "   6    |   30    |   0.046606   |     -     |   10.13  \n",
      "   6    |   40    |   0.063839   |     -     |   10.12  \n",
      "   6    |   50    |   0.027121   |     -     |   10.12  \n",
      "   6    |   60    |   0.120693   |     -     |   10.12  \n",
      "   6    |   70    |   0.039475   |     -     |   10.14  \n",
      "   6    |   80    |   0.033285   |     -     |   10.12  \n",
      "   6    |   90    |   0.040255   |     -     |   10.11  \n",
      "   6    |   100   |   0.034371   |     -     |   10.12  \n",
      "   6    |   110   |   0.086556   |     -     |   43.20  \n",
      "   6    |   120   |   0.008249   |     -     |   9.81   \n",
      "   6    |   130   |   0.006363   |     -     |   9.78   \n",
      "   6    |   140   |   0.007809   |     -     |   9.78   \n",
      "   6    |   150   |   0.021118   |     -     |   9.77   \n",
      "   6    |   160   |   0.029929   |     -     |   9.79   \n",
      "   6    |   170   |   0.001042   |     -     |   9.76   \n",
      "   6    |   180   |   0.030783   |     -     |   9.77   \n",
      "   6    |   190   |   0.045494   |     -     |   9.81   \n",
      "   6    |   200   |   0.093697   |     -     |   9.77   \n",
      "   6    |   210   |   0.003922   |     -     |   43.27  \n",
      "   6    |   220   |   0.108725   |     -     |   9.79   \n",
      "   6    |   230   |   0.001814   |     -     |   9.77   \n",
      "   6    |   240   |   0.034680   |     -     |   9.78   \n",
      "   6    |   250   |   0.069684   |     -     |   9.79   \n",
      "   6    |   260   |   0.035499   |     -     |   9.77   \n",
      "   6    |   270   |   0.001058   |     -     |   9.78   \n",
      "   6    |   280   |   0.002649   |     -     |   9.77   \n",
      "   6    |   290   |   0.064935   |     -     |   9.79   \n",
      "   6    |   300   |   0.040623   |     -     |   9.80   \n",
      "   6    |   310   |   0.011620   |     -     |   43.32  \n",
      "   6    |   320   |   0.029903   |     -     |   9.78   \n",
      "   6    |   330   |   0.036792   |     -     |   9.83   \n",
      "   6    |   340   |   0.003627   |     -     |   9.77   \n",
      "   6    |   350   |   0.001823   |     -     |   9.79   \n",
      "   6    |   360   |   0.097848   |     -     |   9.78   \n",
      "   6    |   370   |   0.027188   |     -     |   9.78   \n",
      "   6    |   372   |   0.000939   |     -     |   1.97   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   0.037864   |  0.300767  |  502.37  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   6    |  93.405%  |  98.335%  |  98.359%  |  93.405%  |  93.405%  |  93.405%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   7    |   10    |   0.027052   |     -     |   11.15  \n",
      "   7    |   20    |   0.016012   |     -     |   10.15  \n",
      "   7    |   30    |   0.027666   |     -     |   10.14  \n",
      "   7    |   40    |   0.021584   |     -     |   10.14  \n",
      "   7    |   50    |   0.018834   |     -     |   10.14  \n",
      "   7    |   60    |   0.002713   |     -     |   10.13  \n",
      "   7    |   70    |   0.035231   |     -     |   10.12  \n",
      "   7    |   80    |   0.067019   |     -     |   10.12  \n",
      "   7    |   90    |   0.011166   |     -     |   10.13  \n",
      "   7    |   100   |   0.047322   |     -     |   10.13  \n",
      "   7    |   110   |   0.001027   |     -     |   43.09  \n",
      "   7    |   120   |   0.000711   |     -     |   10.03  \n",
      "   7    |   130   |   0.019525   |     -     |   9.78   \n",
      "   7    |   140   |   0.038553   |     -     |   9.77   \n",
      "   7    |   150   |   0.000688   |     -     |   9.79   \n",
      "   7    |   160   |   0.060978   |     -     |   9.79   \n",
      "   7    |   170   |   0.002517   |     -     |   9.79   \n",
      "   7    |   180   |   0.005409   |     -     |   9.83   \n",
      "   7    |   190   |   0.045176   |     -     |   9.78   \n",
      "   7    |   200   |   0.015437   |     -     |   9.80   \n",
      "   7    |   210   |   0.000705   |     -     |   43.06  \n",
      "   7    |   220   |   0.036990   |     -     |   9.80   \n",
      "   7    |   230   |   0.001856   |     -     |   9.78   \n",
      "   7    |   240   |   0.000813   |     -     |   9.83   \n",
      "   7    |   250   |   0.000611   |     -     |   9.77   \n",
      "   7    |   260   |   0.027737   |     -     |   9.77   \n",
      "   7    |   270   |   0.033121   |     -     |   9.79   \n",
      "   7    |   280   |   0.021362   |     -     |   9.81   \n",
      "   7    |   290   |   0.000778   |     -     |   9.81   \n",
      "   7    |   300   |   0.005722   |     -     |   9.77   \n",
      "   7    |   310   |   0.111560   |     -     |   43.64  \n",
      "   7    |   320   |   0.000757   |     -     |   9.77   \n",
      "   7    |   330   |   0.014279   |     -     |   9.76   \n",
      "   7    |   340   |   0.029715   |     -     |   9.80   \n",
      "   7    |   350   |   0.017490   |     -     |   9.76   \n",
      "   7    |   360   |   0.031811   |     -     |   9.79   \n",
      "   7    |   370   |   0.006550   |     -     |   9.84   \n",
      "   7    |   372   |   0.000637   |     -     |   1.96   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   0.021756   |  0.336078  |  502.83  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   7    |  93.780%  |  98.342%  |  98.362%  |  93.780%  |  93.780%  |  93.780%  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss   |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   8    |   10    |   0.009399   |     -     |   11.12  \n",
      "   8    |   20    |   0.034964   |     -     |   10.12  \n",
      "   8    |   30    |   0.033976   |     -     |   10.18  \n",
      "   8    |   40    |   0.024639   |     -     |   10.15  \n",
      "   8    |   50    |   0.030254   |     -     |   10.14  \n",
      "   8    |   60    |   0.035305   |     -     |   10.15  \n",
      "   8    |   70    |   0.067464   |     -     |   10.12  \n",
      "   8    |   80    |   0.042869   |     -     |   10.11  \n",
      "   8    |   90    |   0.001513   |     -     |   10.18  \n",
      "   8    |   100   |   0.041006   |     -     |   10.11  \n",
      "   8    |   110   |   0.001472   |     -     |   43.12  \n",
      "   8    |   120   |   0.000568   |     -     |   9.82   \n",
      "   8    |   130   |   0.028368   |     -     |   9.77   \n",
      "   8    |   140   |   0.020818   |     -     |   9.79   \n",
      "   8    |   150   |   0.005259   |     -     |   9.86   \n",
      "   8    |   160   |   0.004000   |     -     |   9.80   \n",
      "   8    |   170   |   0.032407   |     -     |   9.79   \n",
      "   8    |   180   |   0.002855   |     -     |   9.79   \n",
      "   8    |   190   |   0.000547   |     -     |   9.78   \n",
      "   8    |   200   |   0.052335   |     -     |   9.79   \n",
      "   8    |   210   |   0.000520   |     -     |   43.36  \n",
      "   8    |   220   |   0.000425   |     -     |   9.82   \n",
      "   8    |   230   |   0.000625   |     -     |   9.80   \n",
      "   8    |   240   |   0.033431   |     -     |   9.79   \n",
      "   8    |   250   |   0.078263   |     -     |   9.79   \n",
      "   8    |   260   |   0.016393   |     -     |   9.76   \n",
      "   8    |   270   |   0.000479   |     -     |   9.79   \n",
      "   8    |   280   |   0.000478   |     -     |   9.76   \n",
      "   8    |   290   |   0.049238   |     -     |   9.79   \n",
      "   8    |   300   |   0.000555   |     -     |   9.79   \n",
      "   8    |   310   |   0.001251   |     -     |   43.22  \n",
      "   8    |   320   |   0.009365   |     -     |   9.78   \n",
      "   8    |   330   |   0.000490   |     -     |   9.77   \n",
      "   8    |   340   |   0.004283   |     -     |   9.83   \n",
      "   8    |   350   |   0.026344   |     -     |   9.79   \n",
      "   8    |   360   |   0.009592   |     -     |   9.80   \n",
      "   8    |   370   |   0.000953   |     -     |   9.80   \n",
      "   8    |   372   |   0.000403   |     -     |   1.95   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   0.018917   |  0.338809  |  502.70  \n",
      "\n",
      "\n",
      " Epoch  |  Val Acc  |  Val AUC  |  Val AP   | Precision |  Recall   |  Val F1  \n",
      "--------------------------------------------------------------------------------\n",
      "   8    |  93.887%  |  98.127%  |  98.124%  |  93.887%  |  93.887%  |  93.887%  \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(tp['epoch_size']):\n",
    "    print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10}  | {'Elapsed':^9}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    # Measure the elapsed time of each epoch\n",
    "    t0_epoch, t0_batch = time.time(), time.time()\n",
    "    total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        global_step +=1\n",
    "        batch_counts +=1\n",
    "\n",
    "        #Forward propogate\n",
    "        model.zero_grad()\n",
    "        feature = {k:v.to(device) for k, v in batch.items()}\n",
    "        logits = model(feature)\n",
    "        loss = model.compute_loss(feature, logits)\n",
    "        batch_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), tp.gradient_clip)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # Log steps for train loss logging\n",
    "        if (step % tp.log_steps == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "            time_elapsed = time.time() - t0_batch\n",
    "            print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "            tb.add_scalar('loss/batch_train', batch_loss / batch_counts, global_step=global_step)\n",
    "            batch_loss, batch_counts = 0, 0\n",
    "            t0_batch = time.time()\n",
    "\n",
    "        # Save steps for ckpt saving and dev evaluation\n",
    "        if (step % tp.save_steps == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "            val_metrics = binary_cls_metrics(model, valid_loader, device)\n",
    "            for key, val in val_metrics.items():\n",
    "                tb.add_scalar(f'metric/{key}', val, global_step=global_step)\n",
    "            avg_train_loss = total_loss / step\n",
    "            tb.add_scalars('loss/train_valid',{'train': avg_train_loss,\n",
    "                                                'valid': val_metrics['val_loss']}, global_step=global_step)\n",
    "            saver(total_loss / step, val_metrics['val_loss'], epoch_i, global_step, model, optimizer, scheduler)\n",
    "\n",
    "    # On Epoch End: calcualte train & valid loss and log overall metrics\n",
    "    time_elapsed = time.time() - t0_epoch\n",
    "    val_metrics = binary_cls_metrics(model, valid_loader, device)\n",
    "    avg_train_loss = total_loss / step\n",
    "\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_metrics['val_loss']:^10.6f} | {time_elapsed:^9.2f}\")\n",
    "    binary_cls_log(epoch_i, val_metrics)\n",
    "    print(\"\\n\")\n",
    "    if es.check(val_metrics):\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-08T09:28:17.698352Z",
     "iopub.status.busy": "2022-09-08T09:28:17.697697Z",
     "iopub.status.idle": "2022-09-08T09:28:48.482832Z",
     "shell.execute_reply": "2022-09-08T09:28:48.481635Z",
     "shell.execute_reply.started": "2022-09-08T09:28:17.698312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_s': 0.945499081445193,\n",
       " 'f1_e': 0.9385775862068965,\n",
       " 'f1': 0.9413461843022151}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = classification_inference(model, valid_loader, device)\n",
    "valid = pd.DataFrame(valid_dataset.raw_data)\n",
    "valid['pred'] = result['pred']\n",
    "valid['prob'] = result['prob']\n",
    "valid['single_entity'] = valid['text1']\n",
    "overall_f1(valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}